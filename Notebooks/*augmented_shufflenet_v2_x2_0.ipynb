{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the paper: \n",
    "\n",
    "## Efficient and Mobile Deep Learning Architectures for Fast Identification of BacterialStrains in Resource-Constrained Devices\n",
    "\n",
    "### Architecture: ShuffleNet v2x2.0\n",
    "### Data: Augmented + Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, models\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "# Archs not in Pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# External functions\n",
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "3.8.5 (default, Jul 28 2020, 12:59:40) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data paths and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters and dataset details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset details\n",
    "dataset_version = 'original' # original or augmented\n",
    "img_shape = (224,224)\n",
    "img_size = str(img_shape[0])+\"x\"+str(img_shape[1])\n",
    "\n",
    "# Root directory of dataset\n",
    "data_dir = '/home/yibbtstll/venvs/pytorch_gpu/CySDeepBacterial/Dataset/DIBaS_augmented/'\n",
    "\n",
    "train_batch_size = 64\n",
    "val_test_batch_size = 32\n",
    "feature_extract = False\n",
    "pretrained = False\n",
    "h_epochs = 15\n",
    "kfolds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining transforms and creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for input data\n",
    "training_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "total_set = datasets.ImageFolder(data_dir, transform=training_transforms)\n",
    "\n",
    "# Defining folds\n",
    "splits = KFold(n_splits = kfolds, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the target classes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "{0: 'Acinetobacter.baumanii', 1: 'Actinomyces.israeli', 2: 'Bacteroides.fragilis', 3: 'Bifidobacterium.spp', 4: 'Clostridium.perfringens', 5: 'Enterococcus.faecalis', 6: 'Enterococcus.faecium', 7: 'Escherichia.coli', 8: 'Fusobacterium', 9: 'Lactobacillus.casei', 10: 'Lactobacillus.crispatus', 11: 'Lactobacillus.delbrueckii', 12: 'Lactobacillus.gasseri', 13: 'Lactobacillus.jehnsenii', 14: 'Lactobacillus.johnsonii', 15: 'Lactobacillus.paracasei', 16: 'Lactobacillus.plantarum', 17: 'Lactobacillus.reuteri', 18: 'Lactobacillus.rhamnosus', 19: 'Lactobacillus.salivarius', 20: 'Listeria.monocytogenes', 21: 'Micrococcus.spp', 22: 'Neisseria.gonorrhoeae', 23: 'Porfyromonas.gingivalis', 24: 'Propionibacterium.acnes', 25: 'Proteus', 26: 'Pseudomonas.aeruginosa', 27: 'Staphylococcus.aureus', 28: 'Staphylococcus.epidermidis', 29: 'Staphylococcus.saprophiticus', 30: 'Streptococcus.agalactiae', 31: 'Veionella'}\n"
     ]
    }
   ],
   "source": [
    "train_labels = {value : key for (key, value) in total_set.class_to_idx.items()}\n",
    "    \n",
    "print(len(train_labels)) \n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and inicialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing pre-trained parameters, finetunning the classifier to output 32 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained model parameters to avoid backpropogating through them\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        print(\"Setting grad to false.\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_device():\n",
    "    # Model and criterion to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShuffleNetV2(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (stage2): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(24, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(122, 122, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=122, bias=False)\n",
       "        (4): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(122, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=122, bias=False)\n",
       "        (4): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(122, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=122, bias=False)\n",
       "        (4): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(122, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=122, bias=False)\n",
       "        (4): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage3): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=244, bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage4): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(488, 488, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=488, bias=False)\n",
       "        (1): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(488, 488, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=488, bias=False)\n",
       "        (4): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(488, 488, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=488, bias=False)\n",
       "        (4): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(488, 488, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=488, bias=False)\n",
       "        (4): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(488, 488, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=488, bias=False)\n",
       "        (4): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(976, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.shufflenet_v2_x2_0(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Transfer Learning\n",
    "    model = models.shufflenet_v2_x2_0(pretrained=pretrained)\n",
    "    \n",
    "    # Mode\n",
    "    model = set_parameter_requires_grad(model, feature_extract)\n",
    "    \n",
    "    # Fine tuning\n",
    "    # Build custom classifier\n",
    "    model.fc = nn.Linear(in_features=2048,\n",
    "                        out_features=32)\n",
    "    return model.to(get_device())\n",
    "\n",
    "def create_optimizer(model):\n",
    "    # Parameters to update\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "\n",
    "    else:\n",
    "        n_params = 0\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                n_params += 1\n",
    "\n",
    "\n",
    "    # Loss function and gradient descent\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(params_to_update, \n",
    "                          lr=0.0001, \n",
    "                          weight_decay=0.00004)\n",
    "    \n",
    "    print(\"Running on device:\", get_device())\n",
    "    \n",
    "    return criterion.to(get_device()), model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Samples in training: 21665\n",
      "Samples in test: 2408\n",
      "Running on device: cuda\n",
      "\t\t Training: Epoch(0) - Loss: 1.7569, Acc: 44.6804\n",
      "\t\t Validation(0) - Loss: 0.9845, Acc: 65.9884\n",
      "\t\t Training: Epoch(1) - Loss: 0.7798, Acc: 72.4071\n",
      "\t\t Validation(1) - Loss: 0.5847, Acc: 79.7342\n",
      "\t\t Training: Epoch(2) - Loss: 0.5722, Acc: 79.7877\n",
      "\t\t Validation(2) - Loss: 0.4536, Acc: 84.5515\n",
      "\t\t Training: Epoch(3) - Loss: 0.4407, Acc: 84.5142\n",
      "\t\t Validation(3) - Loss: 0.3764, Acc: 87.1678\n",
      "\t\t Training: Epoch(4) - Loss: 0.3673, Acc: 86.8913\n",
      "\t\t Validation(4) - Loss: 0.3341, Acc: 87.8322\n",
      "\t\t Training: Epoch(5) - Loss: 0.3205, Acc: 88.8391\n",
      "\t\t Validation(5) - Loss: 0.3190, Acc: 88.5797\n",
      "\t\t Training: Epoch(6) - Loss: 0.2730, Acc: 90.5839\n",
      "\t\t Validation(6) - Loss: 0.3798, Acc: 85.9635\n",
      "\t\t Training: Epoch(7) - Loss: 0.2247, Acc: 92.1717\n",
      "\t\t Validation(7) - Loss: 0.3774, Acc: 87.3754\n",
      "\t\t Training: Epoch(8) - Loss: 0.2067, Acc: 92.8133\n",
      "\t\t Validation(8) - Loss: 0.3559, Acc: 87.4169\n",
      "\t\t Training: Epoch(9) - Loss: 0.1897, Acc: 93.3349\n",
      "\t\t Validation(9) - Loss: 0.3686, Acc: 87.7492\n",
      "\t\t Training: Epoch(10) - Loss: 0.1652, Acc: 94.4980\n",
      "\t\t Validation(10) - Loss: 0.3058, Acc: 88.9535\n",
      "\t\t Training: Epoch(11) - Loss: 0.1556, Acc: 94.6781\n",
      "\t\t Validation(11) - Loss: 0.2789, Acc: 89.8256\n",
      "\t\t Training: Epoch(12) - Loss: 0.1438, Acc: 95.0935\n",
      "\t\t Validation(12) - Loss: 0.3550, Acc: 89.2857\n",
      "\t\t Training: Epoch(13) - Loss: 0.1178, Acc: 96.0212\n",
      "\t\t Validation(13) - Loss: 0.3059, Acc: 89.6179\n",
      "\t\t Training: Epoch(14) - Loss: 0.1192, Acc: 95.9058\n",
      "\t\t Validation(14) - Loss: 0.3487, Acc: 89.9502\n",
      "Finished.\n",
      "Total time per fold: 1591.2910714149475 seconds.\n",
      "Fold : 1\n",
      "Samples in training: 21665\n",
      "Samples in test: 2408\n",
      "Running on device: cuda\n",
      "\t\t Training: Epoch(0) - Loss: 1.6537, Acc: 47.9437\n",
      "\t\t Validation(0) - Loss: 0.8272, Acc: 70.1827\n",
      "\t\t Training: Epoch(1) - Loss: 0.7636, Acc: 72.8871\n",
      "\t\t Validation(1) - Loss: 0.5171, Acc: 81.3953\n",
      "\t\t Training: Epoch(2) - Loss: 0.5627, Acc: 79.9446\n",
      "\t\t Validation(2) - Loss: 0.4790, Acc: 81.8937\n",
      "\t\t Training: Epoch(3) - Loss: 0.4564, Acc: 83.6280\n",
      "\t\t Validation(3) - Loss: 0.4231, Acc: 84.1362\n",
      "\t\t Training: Epoch(4) - Loss: 0.3843, Acc: 86.3928\n",
      "\t\t Validation(4) - Loss: 0.4313, Acc: 83.7625\n",
      "\t\t Training: Epoch(5) - Loss: 0.3189, Acc: 88.8207\n",
      "\t\t Validation(5) - Loss: 0.2803, Acc: 89.6595\n",
      "\t\t Training: Epoch(6) - Loss: 0.2577, Acc: 91.0916\n",
      "\t\t Validation(6) - Loss: 0.3471, Acc: 88.1645\n",
      "\t\t Training: Epoch(7) - Loss: 0.2281, Acc: 92.0748\n",
      "\t\t Validation(7) - Loss: 0.2690, Acc: 90.2824\n",
      "\t\t Training: Epoch(8) - Loss: 0.1981, Acc: 93.3395\n",
      "\t\t Validation(8) - Loss: 0.3185, Acc: 88.2890\n",
      "\t\t Training: Epoch(9) - Loss: 0.1849, Acc: 93.6441\n",
      "\t\t Validation(9) - Loss: 0.3061, Acc: 89.1196\n",
      "\t\t Training: Epoch(10) - Loss: 0.1591, Acc: 94.6642\n",
      "\t\t Validation(10) - Loss: 0.3537, Acc: 88.6213\n",
      "\t\t Training: Epoch(11) - Loss: 0.1570, Acc: 94.6319\n",
      "\t\t Validation(11) - Loss: 0.2342, Acc: 91.7774\n",
      "\t\t Training: Epoch(12) - Loss: 0.1321, Acc: 95.4350\n",
      "\t\t Validation(12) - Loss: 0.2539, Acc: 91.3621\n",
      "\t\t Training: Epoch(13) - Loss: 0.1237, Acc: 95.7997\n",
      "\t\t Validation(13) - Loss: 0.2698, Acc: 90.9884\n",
      "\t\t Training: Epoch(14) - Loss: 0.1093, Acc: 96.2474\n",
      "\t\t Validation(14) - Loss: 0.2729, Acc: 91.0714\n",
      "Finished.\n",
      "Total time per fold: 1593.2637944221497 seconds.\n",
      "Fold : 2\n",
      "Samples in training: 21665\n",
      "Samples in test: 2408\n",
      "Running on device: cuda\n",
      "\t\t Training: Epoch(0) - Loss: 1.6751, Acc: 46.5220\n",
      "\t\t Validation(0) - Loss: 0.8621, Acc: 69.3522\n",
      "\t\t Training: Epoch(1) - Loss: 0.7862, Acc: 72.7302\n",
      "\t\t Validation(1) - Loss: 0.5783, Acc: 79.6096\n",
      "\t\t Training: Epoch(2) - Loss: 0.5538, Acc: 80.2769\n",
      "\t\t Validation(2) - Loss: 0.4617, Acc: 83.4302\n",
      "\t\t Training: Epoch(3) - Loss: 0.4340, Acc: 84.7311\n",
      "\t\t Validation(3) - Loss: 0.4087, Acc: 84.4684\n",
      "\t\t Training: Epoch(4) - Loss: 0.3598, Acc: 87.3159\n",
      "\t\t Validation(4) - Loss: 0.3535, Acc: 86.8355\n",
      "\t\t Training: Epoch(5) - Loss: 0.3038, Acc: 89.2592\n",
      "\t\t Validation(5) - Loss: 0.3206, Acc: 88.3721\n",
      "\t\t Training: Epoch(6) - Loss: 0.2748, Acc: 90.3069\n",
      "\t\t Validation(6) - Loss: 0.3258, Acc: 88.0399\n",
      "\t\t Training: Epoch(7) - Loss: 0.2291, Acc: 92.0794\n",
      "\t\t Validation(7) - Loss: 0.3232, Acc: 88.9535\n",
      "\t\t Training: Epoch(8) - Loss: 0.2155, Acc: 92.5964\n",
      "\t\t Validation(8) - Loss: 0.3151, Acc: 89.2027\n",
      "\t\t Training: Epoch(9) - Loss: 0.1879, Acc: 93.3533\n",
      "\t\t Validation(9) - Loss: 0.3743, Acc: 87.7492\n",
      "\t\t Training: Epoch(10) - Loss: 0.1622, Acc: 94.4334\n",
      "\t\t Validation(10) - Loss: 0.6333, Acc: 83.0150\n",
      "\t\t Training: Epoch(11) - Loss: 0.1568, Acc: 94.8488\n",
      "\t\t Validation(11) - Loss: 0.3519, Acc: 88.7874\n",
      "\t\t Training: Epoch(12) - Loss: 0.1368, Acc: 95.4904\n",
      "\t\t Validation(12) - Loss: 0.3345, Acc: 88.4967\n",
      "\t\t Training: Epoch(13) - Loss: 0.1236, Acc: 95.8458\n",
      "\t\t Validation(13) - Loss: 0.2664, Acc: 90.7807\n",
      "\t\t Training: Epoch(14) - Loss: 0.1235, Acc: 95.7720\n",
      "\t\t Validation(14) - Loss: 0.3180, Acc: 89.4518\n",
      "Finished.\n",
      "Total time per fold: 1594.7388408184052 seconds.\n",
      "Fold : 3\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "Running on device: cuda\n",
      "\t\t Training: Epoch(0) - Loss: 1.6920, Acc: 47.5076\n",
      "\t\t Validation(0) - Loss: 0.8629, Acc: 71.8322\n",
      "\t\t Training: Epoch(1) - Loss: 0.7576, Acc: 73.2946\n",
      "\t\t Validation(1) - Loss: 0.5254, Acc: 81.9693\n",
      "\t\t Training: Epoch(2) - Loss: 0.5511, Acc: 80.5040\n",
      "\t\t Validation(2) - Loss: 0.4364, Acc: 84.5035\n",
      "\t\t Training: Epoch(3) - Loss: 0.4237, Acc: 85.2026\n",
      "\t\t Validation(3) - Loss: 0.3601, Acc: 87.3286\n",
      "\t\t Training: Epoch(4) - Loss: 0.3555, Acc: 87.2011\n",
      "\t\t Validation(4) - Loss: 0.3476, Acc: 87.9518\n",
      "\t\t Training: Epoch(5) - Loss: 0.3062, Acc: 89.1766\n",
      "\t\t Validation(5) - Loss: 0.3237, Acc: 88.3257\n",
      "\t\t Training: Epoch(6) - Loss: 0.2596, Acc: 90.9166\n",
      "\t\t Validation(6) - Loss: 0.3011, Acc: 88.8658\n",
      "\t\t Training: Epoch(7) - Loss: 0.2242, Acc: 92.1582\n",
      "\t\t Validation(7) - Loss: 0.2829, Acc: 89.8214\n",
      "\t\t Training: Epoch(8) - Loss: 0.2040, Acc: 92.9152\n",
      "\t\t Validation(8) - Loss: 0.2996, Acc: 90.1953\n",
      "\t\t Training: Epoch(9) - Loss: 0.1730, Acc: 94.2398\n",
      "\t\t Validation(9) - Loss: 0.2564, Acc: 90.4030\n",
      "\t\t Training: Epoch(10) - Loss: 0.1591, Acc: 94.6506\n",
      "\t\t Validation(10) - Loss: 0.2530, Acc: 91.5247\n",
      "\t\t Training: Epoch(11) - Loss: 0.1482, Acc: 94.9598\n",
      "\t\t Validation(11) - Loss: 0.2787, Acc: 90.1537\n",
      "\t\t Training: Epoch(12) - Loss: 0.1327, Acc: 95.5922\n",
      "\t\t Validation(12) - Loss: 0.3124, Acc: 90.2784\n",
      "\t\t Training: Epoch(13) - Loss: 0.1275, Acc: 95.6660\n",
      "\t\t Validation(13) - Loss: 0.2228, Acc: 92.3556\n",
      "\t\t Training: Epoch(14) - Loss: 0.1218, Acc: 95.9199\n",
      "\t\t Validation(14) - Loss: 0.2860, Acc: 90.7354\n",
      "Finished.\n",
      "Total time per fold: 1596.8209569454193 seconds.\n",
      "Fold : 4\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "Running on device: cuda\n",
      "\t\t Training: Epoch(0) - Loss: 1.7095, Acc: 46.2522\n",
      "\t\t Validation(0) - Loss: 0.8454, Acc: 71.8322\n",
      "\t\t Training: Epoch(1) - Loss: 0.7594, Acc: 73.3592\n",
      "\t\t Validation(1) - Loss: 0.5147, Acc: 82.4678\n",
      "\t\t Training: Epoch(2) - Loss: 0.5637, Acc: 80.1025\n",
      "\t\t Validation(2) - Loss: 0.4464, Acc: 83.5895\n",
      "\t\t Training: Epoch(3) - Loss: 0.4411, Acc: 84.2057\n",
      "\t\t Validation(3) - Loss: 0.4334, Acc: 85.2929\n",
      "\t\t Training: Epoch(4) - Loss: 0.3751, Acc: 86.7119\n",
      "\t\t Validation(4) - Loss: 0.4008, Acc: 85.5006\n",
      "\t\t Training: Epoch(5) - Loss: 0.3132, Acc: 88.9781\n",
      "\t\t Validation(5) - Loss: 0.4320, Acc: 84.8359\n",
      "\t\t Training: Epoch(6) - Loss: 0.2791, Acc: 90.1551\n",
      "\t\t Validation(6) - Loss: 0.2796, Acc: 89.6552\n",
      "\t\t Training: Epoch(7) - Loss: 0.2402, Acc: 91.3782\n",
      "\t\t Validation(7) - Loss: 0.4769, Acc: 85.3760\n",
      "\t\t Training: Epoch(8) - Loss: 0.2101, Acc: 92.6290\n",
      "\t\t Validation(8) - Loss: 0.2907, Acc: 89.1982\n",
      "\t\t Training: Epoch(9) - Loss: 0.1880, Acc: 93.3998\n",
      "\t\t Validation(9) - Loss: 0.2903, Acc: 89.8629\n",
      "\t\t Training: Epoch(10) - Loss: 0.1663, Acc: 94.3783\n",
      "\t\t Validation(10) - Loss: 0.3338, Acc: 88.9074\n",
      "\t\t Training: Epoch(11) - Loss: 0.1544, Acc: 94.8214\n",
      "\t\t Validation(11) - Loss: 0.2630, Acc: 91.0677\n",
      "\t\t Training: Epoch(12) - Loss: 0.1349, Acc: 95.4906\n",
      "\t\t Validation(12) - Loss: 0.2923, Acc: 90.1122\n",
      "\t\t Training: Epoch(13) - Loss: 0.1292, Acc: 95.5922\n",
      "\t\t Validation(13) - Loss: 0.3221, Acc: 89.1151\n",
      "\t\t Training: Epoch(14) - Loss: 0.1220, Acc: 95.9245\n",
      "\t\t Validation(14) - Loss: 0.2625, Acc: 91.4416\n",
      "Finished.\n",
      "Total time per fold: 1599.6758980751038 seconds.\n",
      "Fold : 5\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "Running on device: cuda\n",
      "\t\t Training: Epoch(0) - Loss: 1.7297, Acc: 45.2691\n",
      "\t\t Validation(0) - Loss: 0.8102, Acc: 72.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Training: Epoch(1) - Loss: 0.7787, Acc: 72.8838\n",
      "\t\t Validation(1) - Loss: 0.5397, Acc: 81.4292\n",
      "\t\t Training: Epoch(2) - Loss: 0.5458, Acc: 80.4302\n",
      "\t\t Validation(2) - Loss: 0.3847, Acc: 86.8301\n",
      "\t\t Training: Epoch(3) - Loss: 0.4166, Acc: 85.2395\n",
      "\t\t Validation(3) - Loss: 0.3663, Acc: 86.9547\n",
      "\t\t Training: Epoch(4) - Loss: 0.3411, Acc: 88.1981\n",
      "\t\t Validation(4) - Loss: 0.3047, Acc: 89.2397\n",
      "\t\t Training: Epoch(5) - Loss: 0.2962, Acc: 89.8320\n",
      "\t\t Validation(5) - Loss: 0.3203, Acc: 88.4088\n",
      "\t\t Training: Epoch(6) - Loss: 0.2562, Acc: 91.2536\n",
      "\t\t Validation(6) - Loss: 0.2952, Acc: 89.6552\n",
      "\t\t Training: Epoch(7) - Loss: 0.2143, Acc: 92.7167\n",
      "\t\t Validation(7) - Loss: 0.3342, Acc: 89.3644\n",
      "\t\t Training: Epoch(8) - Loss: 0.1996, Acc: 93.1506\n",
      "\t\t Validation(8) - Loss: 0.2934, Acc: 89.4890\n",
      "\t\t Training: Epoch(9) - Loss: 0.1792, Acc: 93.9583\n",
      "\t\t Validation(9) - Loss: 0.2652, Acc: 91.3170\n",
      "\t\t Training: Epoch(10) - Loss: 0.1617, Acc: 94.6091\n",
      "\t\t Validation(10) - Loss: 0.2721, Acc: 90.5276\n",
      "\t\t Training: Epoch(11) - Loss: 0.1521, Acc: 94.8121\n",
      "\t\t Validation(11) - Loss: 0.4846, Acc: 86.2900\n",
      "\t\t Training: Epoch(12) - Loss: 0.1281, Acc: 95.7306\n",
      "\t\t Validation(12) - Loss: 0.3360, Acc: 89.3228\n",
      "\t\t Training: Epoch(13) - Loss: 0.1246, Acc: 95.9614\n",
      "\t\t Validation(13) - Loss: 0.3003, Acc: 89.5305\n",
      "\t\t Training: Epoch(14) - Loss: 0.1104, Acc: 96.3030\n",
      "\t\t Validation(14) - Loss: 0.3241, Acc: 89.4890\n",
      "Finished.\n",
      "Total time per fold: 1594.197561264038 seconds.\n",
      "Fold : 6\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "Running on device: cuda\n",
      "\t\t Training: Epoch(0) - Loss: 1.6363, Acc: 48.3199\n",
      "\t\t Validation(0) - Loss: 0.8217, Acc: 71.4998\n",
      "\t\t Training: Epoch(1) - Loss: 0.7246, Acc: 74.7669\n",
      "\t\t Validation(1) - Loss: 0.7128, Acc: 75.6128\n",
      "\t\t Training: Epoch(2) - Loss: 0.5355, Acc: 81.4317\n",
      "\t\t Validation(2) - Loss: 0.4515, Acc: 83.7557\n",
      "\t\t Training: Epoch(3) - Loss: 0.4305, Acc: 84.7318\n",
      "\t\t Validation(3) - Loss: 0.4056, Acc: 84.5035\n",
      "\t\t Training: Epoch(4) - Loss: 0.3559, Acc: 87.4688\n",
      "\t\t Validation(4) - Loss: 0.4083, Acc: 85.6668\n",
      "\t\t Training: Epoch(5) - Loss: 0.2993, Acc: 89.4212\n",
      "\t\t Validation(5) - Loss: 0.3790, Acc: 86.5393\n",
      "\t\t Training: Epoch(6) - Loss: 0.2524, Acc: 91.2813\n",
      "\t\t Validation(6) - Loss: 0.4226, Acc: 84.9190\n",
      "\t\t Training: Epoch(7) - Loss: 0.2341, Acc: 91.8074\n",
      "\t\t Validation(7) - Loss: 0.3185, Acc: 89.0320\n",
      "\t\t Training: Epoch(8) - Loss: 0.2191, Acc: 92.4352\n",
      "\t\t Validation(8) - Loss: 0.3299, Acc: 88.7412\n",
      "\t\t Training: Epoch(9) - Loss: 0.1824, Acc: 93.8198\n",
      "\t\t Validation(9) - Loss: 0.3482, Acc: 87.9103\n",
      "\t\t Training: Epoch(10) - Loss: 0.1658, Acc: 94.4752\n",
      "\t\t Validation(10) - Loss: 0.3559, Acc: 87.6194\n",
      "\t\t Training: Epoch(11) - Loss: 0.1475, Acc: 95.0798\n",
      "\t\t Validation(11) - Loss: 0.2840, Acc: 90.9015\n",
      "\t\t Training: Epoch(12) - Loss: 0.1401, Acc: 95.1768\n",
      "\t\t Validation(12) - Loss: 0.3502, Acc: 88.9904\n",
      "\t\t Training: Epoch(13) - Loss: 0.1273, Acc: 95.9106\n",
      "\t\t Validation(13) - Loss: 0.3079, Acc: 89.3228\n",
      "\t\t Training: Epoch(14) - Loss: 0.1185, Acc: 96.1045\n",
      "\t\t Validation(14) - Loss: 0.4664, Acc: 86.4562\n",
      "Finished.\n",
      "Total time per fold: 1594.5925974845886 seconds.\n",
      "Fold : 7\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "Running on device: cuda\n",
      "\t\t Training: Epoch(0) - Loss: 1.7065, Acc: 45.7122\n",
      "\t\t Validation(0) - Loss: 0.8002, Acc: 72.4553\n",
      "\t\t Training: Epoch(1) - Loss: 0.7814, Acc: 72.3022\n",
      "\t\t Validation(1) - Loss: 0.5363, Acc: 80.9306\n",
      "\t\t Training: Epoch(2) - Loss: 0.5707, Acc: 79.6871\n",
      "\t\t Validation(2) - Loss: 0.5009, Acc: 82.5924\n",
      "\t\t Training: Epoch(3) - Loss: 0.4416, Acc: 84.4272\n",
      "\t\t Validation(3) - Loss: 0.5130, Acc: 82.2185\n",
      "\t\t Training: Epoch(4) - Loss: 0.3696, Acc: 87.0350\n",
      "\t\t Validation(4) - Loss: 0.4763, Acc: 82.2601\n",
      "\t\t Training: Epoch(5) - Loss: 0.2923, Acc: 89.6658\n",
      "\t\t Validation(5) - Loss: 0.3492, Acc: 87.3702\n",
      "\t\t Training: Epoch(6) - Loss: 0.2594, Acc: 90.8843\n",
      "\t\t Validation(6) - Loss: 0.3968, Acc: 85.9992\n",
      "\t\t Training: Epoch(7) - Loss: 0.2192, Acc: 92.4675\n",
      "\t\t Validation(7) - Loss: 0.3244, Acc: 88.2426\n",
      "\t\t Training: Epoch(8) - Loss: 0.1999, Acc: 93.3121\n",
      "\t\t Validation(8) - Loss: 0.3702, Acc: 88.0349\n",
      "\t\t Training: Epoch(9) - Loss: 0.1773, Acc: 93.8844\n",
      "\t\t Validation(9) - Loss: 0.3386, Acc: 88.9489\n",
      "\t\t Training: Epoch(10) - Loss: 0.1615, Acc: 94.5675\n",
      "\t\t Validation(10) - Loss: 0.3986, Acc: 86.9547\n",
      "\t\t Training: Epoch(11) - Loss: 0.1441, Acc: 95.2322\n",
      "\t\t Validation(11) - Loss: 0.2861, Acc: 90.4030\n",
      "\t\t Training: Epoch(12) - Loss: 0.1265, Acc: 95.7029\n",
      "\t\t Validation(12) - Loss: 0.3322, Acc: 89.1151\n",
      "\t\t Training: Epoch(13) - Loss: 0.1258, Acc: 95.6891\n",
      "\t\t Validation(13) - Loss: 0.3326, Acc: 89.1982\n",
      "\t\t Training: Epoch(14) - Loss: 0.1102, Acc: 96.3122\n",
      "\t\t Validation(14) - Loss: 0.3003, Acc: 90.2368\n",
      "Finished.\n",
      "Total time per fold: 1599.4523513317108 seconds.\n",
      "Fold : 8\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "Running on device: cuda\n",
      "\t\t Training: Epoch(0) - Loss: 1.7433, Acc: 45.8599\n",
      "\t\t Validation(0) - Loss: 0.8489, Acc: 71.1674\n",
      "\t\t Training: Epoch(1) - Loss: 0.7748, Acc: 73.1053\n",
      "\t\t Validation(1) - Loss: 0.5638, Acc: 80.1828\n",
      "\t\t Training: Epoch(2) - Loss: 0.5589, Acc: 80.3009\n",
      "\t\t Validation(2) - Loss: 0.4913, Acc: 83.2572\n",
      "\t\t Training: Epoch(3) - Loss: 0.4279, Acc: 85.0503\n",
      "\t\t Validation(3) - Loss: 0.3911, Acc: 85.7499\n",
      "\t\t Training: Epoch(4) - Loss: 0.3645, Acc: 87.1504\n",
      "\t\t Validation(4) - Loss: 0.3520, Acc: 87.1624\n",
      "\t\t Training: Epoch(5) - Loss: 0.3008, Acc: 89.4766\n",
      "\t\t Validation(5) - Loss: 0.3250, Acc: 88.1595\n",
      "\t\t Training: Epoch(6) - Loss: 0.2604, Acc: 90.8843\n",
      "\t\t Validation(6) - Loss: 0.3093, Acc: 89.4474\n",
      "\t\t Training: Epoch(7) - Loss: 0.2351, Acc: 91.8028\n",
      "\t\t Validation(7) - Loss: 0.2655, Acc: 90.7769\n",
      "\t\t Training: Epoch(8) - Loss: 0.2050, Acc: 92.9290\n",
      "\t\t Validation(8) - Loss: 0.2744, Acc: 90.4445\n",
      "\t\t Training: Epoch(9) - Loss: 0.1839, Acc: 93.7552\n",
      "\t\t Validation(9) - Loss: 0.3234, Acc: 89.1566\n",
      "\t\t Training: Epoch(10) - Loss: 0.1564, Acc: 94.6829\n",
      "\t\t Validation(10) - Loss: 0.4675, Acc: 85.8330\n",
      "\t\t Training: Epoch(11) - Loss: 0.1517, Acc: 94.9691\n",
      "\t\t Validation(11) - Loss: 0.3985, Acc: 86.7054\n",
      "\t\t Training: Epoch(12) - Loss: 0.1375, Acc: 95.3291\n",
      "\t\t Validation(12) - Loss: 0.2856, Acc: 90.0706\n",
      "\t\t Training: Epoch(13) - Loss: 0.1228, Acc: 95.8968\n",
      "\t\t Validation(13) - Loss: 0.3052, Acc: 89.4059\n",
      "\t\t Training: Epoch(14) - Loss: 0.1241, Acc: 95.8968\n",
      "\t\t Validation(14) - Loss: 0.2929, Acc: 89.8629\n",
      "Finished.\n",
      "Total time per fold: 1592.2537605762482 seconds.\n",
      "Fold : 9\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "Running on device: cuda\n",
      "\t\t Training: Epoch(0) - Loss: 1.7918, Acc: 43.7875\n",
      "\t\t Validation(0) - Loss: 0.8498, Acc: 69.9626\n",
      "\t\t Training: Epoch(1) - Loss: 0.7913, Acc: 72.1361\n",
      "\t\t Validation(1) - Loss: 0.7153, Acc: 73.6602\n",
      "\t\t Training: Epoch(2) - Loss: 0.5721, Acc: 79.9778\n",
      "\t\t Validation(2) - Loss: 0.4185, Acc: 85.2929\n",
      "\t\t Training: Epoch(3) - Loss: 0.4449, Acc: 84.2426\n",
      "\t\t Validation(3) - Loss: 0.4598, Acc: 82.8833\n",
      "\t\t Training: Epoch(4) - Loss: 0.3648, Acc: 87.0765\n",
      "\t\t Validation(4) - Loss: 0.4729, Acc: 83.8388\n",
      "\t\t Training: Epoch(5) - Loss: 0.3010, Acc: 89.5320\n",
      "\t\t Validation(5) - Loss: 0.3629, Acc: 87.0378\n",
      "\t\t Training: Epoch(6) - Loss: 0.2548, Acc: 91.3920\n",
      "\t\t Validation(6) - Loss: 0.3323, Acc: 88.2842\n",
      "\t\t Training: Epoch(7) - Loss: 0.2273, Acc: 92.3336\n",
      "\t\t Validation(7) - Loss: 0.2797, Acc: 89.1982\n",
      "\t\t Training: Epoch(8) - Loss: 0.1975, Acc: 93.2844\n",
      "\t\t Validation(8) - Loss: 0.3566, Acc: 88.0764\n",
      "\t\t Training: Epoch(9) - Loss: 0.1767, Acc: 94.1106\n",
      "\t\t Validation(9) - Loss: 0.2702, Acc: 90.1122\n",
      "\t\t Training: Epoch(10) - Loss: 0.1518, Acc: 94.7937\n",
      "\t\t Validation(10) - Loss: 0.2488, Acc: 91.4001\n",
      "\t\t Training: Epoch(11) - Loss: 0.1489, Acc: 95.0198\n",
      "\t\t Validation(11) - Loss: 0.4549, Acc: 86.4977\n",
      "\t\t Training: Epoch(12) - Loss: 0.1301, Acc: 95.6660\n",
      "\t\t Validation(12) - Loss: 0.3664, Acc: 88.4088\n",
      "\t\t Training: Epoch(13) - Loss: 0.1140, Acc: 96.1691\n",
      "\t\t Validation(13) - Loss: 0.3034, Acc: 89.0320\n",
      "\t\t Training: Epoch(14) - Loss: 0.1074, Acc: 96.5060\n",
      "\t\t Validation(14) - Loss: 0.3101, Acc: 89.7383\n",
      "Finished.\n",
      "Total time per fold: 1596.4295735359192 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Variables to store fold scores\n",
    "train_acc = []\n",
    "test_top1_acc = []\n",
    "test_top5_acc = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "times = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(splits.split(total_set)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('Fold : {}'.format(fold))\n",
    "    \n",
    "    # Train and val samplers\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    print(\"Samples in training:\", len(train_sampler))\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    print(\"Samples in test:\", len(valid_sampler))\n",
    "    \n",
    "    # Train and val loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=train_batch_size, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=1, sampler=valid_sampler)\n",
    "    \n",
    "    device = get_device()\n",
    "    \n",
    "    criterion, model, optimizer = create_optimizer(load_model())\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(h_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        trunning_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += (preds == labels).sum()\n",
    "            trunning_corrects += preds.size(0)\n",
    "            \n",
    "\n",
    "        epoch_loss = running_loss / trunning_corrects\n",
    "        epoch_acc = (running_corrects.double()*100) / trunning_corrects\n",
    "        train_acc.append(epoch_acc.item())\n",
    "        \n",
    "        print('\\t\\t Training: Epoch({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        model.eval()  \n",
    "        \n",
    "        vrunning_loss = 0.0\n",
    "        vrunning_corrects = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for data, labels in valid_loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(data)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            vrunning_loss += loss.item() * data.size(0)\n",
    "            vrunning_corrects += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            \n",
    "        vepoch_loss = vrunning_loss/num_samples\n",
    "        vepoch_acc = (vrunning_corrects.double() * 100)/num_samples\n",
    "        \n",
    "        print('\\t\\t Validation({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, vepoch_loss, vepoch_acc))\n",
    "    \n",
    "    # Calculating and appending scores to this fold\n",
    "    model.class_to_idx = total_set.class_to_idx\n",
    "    scores = get_scores(model, valid_loader)\n",
    "    \n",
    "    test_top1_acc.append(scores[0])\n",
    "    test_top5_acc.append(scores[1])\n",
    "    test_precision.append(scores[2])\n",
    "    test_recall.append(scores[3])\n",
    "    test_f1.append(scores[4])\n",
    "    \n",
    "    time_fold = time.time() - start_time\n",
    "    times.append(time_fold)\n",
    "    print(\"Total time per fold: %s seconds.\" %(time_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy average:  0.8713459316509922\n",
      "Top-1 test accuracy average:  0.8984335382542887\n",
      "Top-5 test accuracy average:  0.9972998880618131\n",
      "Weighted Precision test accuracy average:  0.9076553083687393\n",
      "Weighted Recall test accuracy average:  0.8984335382542887\n",
      "Weighted F1 test accuracy average:  0.8977386930709619\n",
      "Average time per fold (seconds): 1595.271640586853\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy average: \", np.mean(train_acc) / 100)\n",
    "print(\"Top-1 test accuracy average: \", np.mean(test_top1_acc))\n",
    "print(\"Top-5 test accuracy average: \", np.mean(test_top5_acc))\n",
    "print(\"Weighted Precision test accuracy average: \", np.mean(test_precision))\n",
    "print(\"Weighted Recall test accuracy average: \", np.mean(test_recall))\n",
    "print(\"Weighted F1 test accuracy average: \", np.mean(test_f1))\n",
    "print(\"Average time per fold (seconds):\", np.mean(times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
