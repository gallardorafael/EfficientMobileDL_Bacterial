{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the paper: \n",
    "\n",
    "## Efficient and Mobile Deep Learning Architectures for Fast Identification of BacterialStrains in Resource-Constrained Devices\n",
    "\n",
    "### Architecture: EfficientNet B2\n",
    "### Data: Original + Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, models\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "# Archs not in Pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# External functions\n",
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "3.8.5 (default, Jul 28 2020, 12:59:40) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data paths and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters and dataset details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset details\n",
    "dataset_version = 'original' # original or augmented\n",
    "img_shape = (224,224)\n",
    "img_size = str(img_shape[0])+\"x\"+str(img_shape[1])\n",
    "\n",
    "# Root directory of dataset\n",
    "data_dir = '/home/yibbtstll/venvs/pytorch_gpu/CySDeepBacterial/Dataset/DIBaS/'\n",
    "\n",
    "train_batch_size = 32\n",
    "val_test_batch_size = 32\n",
    "feature_extract = False\n",
    "pretrained = True\n",
    "h_epochs = 10\n",
    "kfolds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining transforms and creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for input data\n",
    "training_transforms = transforms.Compose([transforms.Resize((224,224), Image.LANCZOS),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "total_set = datasets.ImageFolder(data_dir, transform=training_transforms)\n",
    "\n",
    "# Defining folds\n",
    "splits = KFold(n_splits = kfolds, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the target classes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "{0: 'Acinetobacter.baumanii', 1: 'Actinomyces.israeli', 2: 'Bacteroides.fragilis', 3: 'Bifidobacterium.spp', 4: 'Clostridium.perfringens', 5: 'Enterococcus.faecalis', 6: 'Enterococcus.faecium', 7: 'Escherichia.coli', 8: 'Fusobacterium', 9: 'Lactobacillus.casei', 10: 'Lactobacillus.crispatus', 11: 'Lactobacillus.delbrueckii', 12: 'Lactobacillus.gasseri', 13: 'Lactobacillus.jehnsenii', 14: 'Lactobacillus.johnsonii', 15: 'Lactobacillus.paracasei', 16: 'Lactobacillus.plantarum', 17: 'Lactobacillus.reuteri', 18: 'Lactobacillus.rhamnosus', 19: 'Lactobacillus.salivarius', 20: 'Listeria.monocytogenes', 21: 'Micrococcus.spp', 22: 'Neisseria.gonorrhoeae', 23: 'Porfyromonas.gingivalis', 24: 'Propionibacterium.acnes', 25: 'Proteus', 26: 'Pseudomonas.aeruginosa', 27: 'Staphylococcus.aureus', 28: 'Staphylococcus.epidermidis', 29: 'Staphylococcus.saprophiticus', 30: 'Streptococcus.agalactiae', 31: 'Veionella'}\n"
     ]
    }
   ],
   "source": [
    "train_labels = {value : key for (key, value) in total_set.class_to_idx.items()}\n",
    "    \n",
    "print(len(train_labels)) \n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and inicialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing pre-trained parameters, finetunning the classifier to output 32 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained model parameters to avoid backpropogating through them\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        print(\"Setting grad to false.\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_device():\n",
    "    # Model and criterion to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Transfer Learning\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    \n",
    "    # Mode\n",
    "    model = set_parameter_requires_grad(model, feature_extract)\n",
    "    \n",
    "    # Fine tuning\n",
    "    # Build custom classifier\n",
    "    model._fc = nn.Linear(in_features=1280,\n",
    "                        out_features=32)\n",
    "    return model\n",
    "\n",
    "def create_optimizer(model):\n",
    "    # Parameters to update\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "\n",
    "    else:\n",
    "        n_params = 0\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                n_params += 1\n",
    "\n",
    "\n",
    "    # Loss function and gradient descent\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(params_to_update, \n",
    "                          lr=0.001, \n",
    "                          weight_decay=0.000004)\n",
    "    \n",
    "    return criterion.to(get_device()), model.to(get_device()), optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.0659, Acc: 54.6512\n",
      "\t\t Validation(0) - Loss: 2.8162, Acc: 14.9254\n",
      "\t\t Training: Epoch(1) - Loss: 0.5333, Acc: 87.0432\n",
      "\t\t Validation(1) - Loss: 2.0726, Acc: 31.3433\n",
      "\t\t Training: Epoch(2) - Loss: 0.3502, Acc: 91.3621\n",
      "\t\t Validation(2) - Loss: 1.9500, Acc: 47.7612\n",
      "\t\t Training: Epoch(3) - Loss: 0.2385, Acc: 93.6877\n",
      "\t\t Validation(3) - Loss: 1.4259, Acc: 55.2239\n",
      "\t\t Training: Epoch(4) - Loss: 0.2521, Acc: 91.6944\n",
      "\t\t Validation(4) - Loss: 1.1600, Acc: 65.6716\n",
      "\t\t Training: Epoch(5) - Loss: 0.1420, Acc: 96.6777\n",
      "\t\t Validation(5) - Loss: 0.3932, Acc: 88.0597\n",
      "\t\t Training: Epoch(6) - Loss: 0.0807, Acc: 97.8405\n",
      "\t\t Validation(6) - Loss: 0.6703, Acc: 86.5672\n",
      "\t\t Training: Epoch(7) - Loss: 0.0962, Acc: 97.1761\n",
      "\t\t Validation(7) - Loss: 0.9338, Acc: 80.5970\n",
      "\t\t Training: Epoch(8) - Loss: 0.0871, Acc: 97.1761\n",
      "\t\t Validation(8) - Loss: 0.7035, Acc: 86.5672\n",
      "\t\t Training: Epoch(9) - Loss: 0.1044, Acc: 96.1794\n",
      "\t\t Validation(9) - Loss: 0.2565, Acc: 95.5224\n",
      "Finished.\n",
      "Total time per fold: 259.96586418151855 seconds.\n",
      "Fold : 1\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.2132, Acc: 50.1661\n",
      "\t\t Validation(0) - Loss: 2.2919, Acc: 31.3433\n",
      "\t\t Training: Epoch(1) - Loss: 0.5659, Acc: 85.5482\n",
      "\t\t Validation(1) - Loss: 2.1754, Acc: 38.8060\n",
      "\t\t Training: Epoch(2) - Loss: 0.2764, Acc: 91.8605\n",
      "\t\t Validation(2) - Loss: 3.0608, Acc: 31.3433\n",
      "\t\t Training: Epoch(3) - Loss: 0.2850, Acc: 91.1960\n",
      "\t\t Validation(3) - Loss: 1.3425, Acc: 62.6866\n",
      "\t\t Training: Epoch(4) - Loss: 0.2472, Acc: 93.5216\n",
      "\t\t Validation(4) - Loss: 1.0979, Acc: 62.6866\n",
      "\t\t Training: Epoch(5) - Loss: 0.1239, Acc: 97.1761\n",
      "\t\t Validation(5) - Loss: 1.0115, Acc: 70.1493\n",
      "\t\t Training: Epoch(6) - Loss: 0.0791, Acc: 98.1728\n",
      "\t\t Validation(6) - Loss: 1.3003, Acc: 61.1940\n",
      "\t\t Training: Epoch(7) - Loss: 0.0950, Acc: 97.1761\n",
      "\t\t Validation(7) - Loss: 0.7112, Acc: 80.5970\n",
      "\t\t Training: Epoch(8) - Loss: 0.1181, Acc: 97.0100\n",
      "\t\t Validation(8) - Loss: 0.7077, Acc: 77.6119\n",
      "\t\t Training: Epoch(9) - Loss: 0.1403, Acc: 96.8439\n",
      "\t\t Validation(9) - Loss: 0.4121, Acc: 91.0448\n",
      "Finished.\n",
      "Total time per fold: 265.2194423675537 seconds.\n",
      "Fold : 2\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.0509, Acc: 57.3090\n",
      "\t\t Validation(0) - Loss: 3.5133, Acc: 7.4627\n",
      "\t\t Training: Epoch(1) - Loss: 0.4192, Acc: 88.7043\n",
      "\t\t Validation(1) - Loss: 3.2904, Acc: 13.4328\n",
      "\t\t Training: Epoch(2) - Loss: 0.3456, Acc: 89.7010\n",
      "\t\t Validation(2) - Loss: 2.9445, Acc: 26.8657\n",
      "\t\t Training: Epoch(3) - Loss: 0.2554, Acc: 92.0266\n",
      "\t\t Validation(3) - Loss: 2.3633, Acc: 49.2537\n",
      "\t\t Training: Epoch(4) - Loss: 0.1610, Acc: 95.8472\n",
      "\t\t Validation(4) - Loss: 1.1982, Acc: 65.6716\n",
      "\t\t Training: Epoch(5) - Loss: 0.1425, Acc: 97.0100\n",
      "\t\t Validation(5) - Loss: 1.4885, Acc: 62.6866\n",
      "\t\t Training: Epoch(6) - Loss: 0.1752, Acc: 92.8571\n",
      "\t\t Validation(6) - Loss: 1.0607, Acc: 68.6567\n",
      "\t\t Training: Epoch(7) - Loss: 0.1024, Acc: 97.3422\n",
      "\t\t Validation(7) - Loss: 1.6786, Acc: 58.2090\n",
      "\t\t Training: Epoch(8) - Loss: 0.0892, Acc: 97.8405\n",
      "\t\t Validation(8) - Loss: 0.6090, Acc: 79.1045\n",
      "\t\t Training: Epoch(9) - Loss: 0.0825, Acc: 98.1728\n",
      "\t\t Validation(9) - Loss: 0.1468, Acc: 91.0448\n",
      "Finished.\n",
      "Total time per fold: 267.67569303512573 seconds.\n",
      "Fold : 3\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.1382, Acc: 54.9834\n",
      "\t\t Validation(0) - Loss: 2.5778, Acc: 25.3731\n",
      "\t\t Training: Epoch(1) - Loss: 0.4476, Acc: 89.5349\n",
      "\t\t Validation(1) - Loss: 3.0558, Acc: 28.3582\n",
      "\t\t Training: Epoch(2) - Loss: 0.2733, Acc: 92.1927\n",
      "\t\t Validation(2) - Loss: 2.6690, Acc: 34.3284\n",
      "\t\t Training: Epoch(3) - Loss: 0.1975, Acc: 93.1894\n",
      "\t\t Validation(3) - Loss: 3.0600, Acc: 31.3433\n",
      "\t\t Training: Epoch(4) - Loss: 0.2535, Acc: 92.8571\n",
      "\t\t Validation(4) - Loss: 2.2024, Acc: 52.2388\n",
      "\t\t Training: Epoch(5) - Loss: 0.1779, Acc: 94.8505\n",
      "\t\t Validation(5) - Loss: 2.0590, Acc: 58.2090\n",
      "\t\t Training: Epoch(6) - Loss: 0.1286, Acc: 96.0133\n",
      "\t\t Validation(6) - Loss: 2.3707, Acc: 56.7164\n",
      "\t\t Training: Epoch(7) - Loss: 0.1224, Acc: 96.6777\n",
      "\t\t Validation(7) - Loss: 1.3982, Acc: 61.1940\n",
      "\t\t Training: Epoch(8) - Loss: 0.0845, Acc: 97.5083\n",
      "\t\t Validation(8) - Loss: 0.5986, Acc: 89.5522\n",
      "\t\t Training: Epoch(9) - Loss: 0.0798, Acc: 97.1761\n",
      "\t\t Validation(9) - Loss: 0.7664, Acc: 83.5821\n",
      "Finished.\n",
      "Total time per fold: 265.645959854126 seconds.\n",
      "Fold : 4\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.0168, Acc: 57.6412\n",
      "\t\t Validation(0) - Loss: 2.6649, Acc: 20.8955\n",
      "\t\t Training: Epoch(1) - Loss: 0.5229, Acc: 86.8771\n",
      "\t\t Validation(1) - Loss: 1.4599, Acc: 50.7463\n",
      "\t\t Training: Epoch(2) - Loss: 0.3207, Acc: 90.6977\n",
      "\t\t Validation(2) - Loss: 2.3542, Acc: 37.3134\n",
      "\t\t Training: Epoch(3) - Loss: 0.1809, Acc: 94.8505\n",
      "\t\t Validation(3) - Loss: 1.5730, Acc: 53.7313\n",
      "\t\t Training: Epoch(4) - Loss: 0.1393, Acc: 96.0133\n",
      "\t\t Validation(4) - Loss: 1.6042, Acc: 65.6716\n",
      "\t\t Training: Epoch(5) - Loss: 0.0861, Acc: 97.1761\n",
      "\t\t Validation(5) - Loss: 1.1841, Acc: 71.6418\n",
      "\t\t Training: Epoch(6) - Loss: 0.1340, Acc: 96.8439\n",
      "\t\t Validation(6) - Loss: 1.0738, Acc: 68.6567\n",
      "\t\t Training: Epoch(7) - Loss: 0.1580, Acc: 96.1794\n",
      "\t\t Validation(7) - Loss: 0.8442, Acc: 76.1194\n",
      "\t\t Training: Epoch(8) - Loss: 0.1211, Acc: 96.5116\n",
      "\t\t Validation(8) - Loss: 1.2763, Acc: 68.6567\n",
      "\t\t Training: Epoch(9) - Loss: 0.0924, Acc: 97.6744\n",
      "\t\t Validation(9) - Loss: 0.7312, Acc: 76.1194\n",
      "Finished.\n",
      "Total time per fold: 271.2274708747864 seconds.\n",
      "Fold : 5\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.0533, Acc: 55.8140\n",
      "\t\t Validation(0) - Loss: 2.5881, Acc: 23.8806\n",
      "\t\t Training: Epoch(1) - Loss: 0.4503, Acc: 87.3754\n",
      "\t\t Validation(1) - Loss: 4.2514, Acc: 26.8657\n",
      "\t\t Training: Epoch(2) - Loss: 0.2441, Acc: 92.8571\n",
      "\t\t Validation(2) - Loss: 2.2472, Acc: 49.2537\n",
      "\t\t Training: Epoch(3) - Loss: 0.2017, Acc: 93.6877\n",
      "\t\t Validation(3) - Loss: 2.4931, Acc: 50.7463\n",
      "\t\t Training: Epoch(4) - Loss: 0.2019, Acc: 94.0199\n",
      "\t\t Validation(4) - Loss: 1.5545, Acc: 61.1940\n",
      "\t\t Training: Epoch(5) - Loss: 0.2321, Acc: 94.1860\n",
      "\t\t Validation(5) - Loss: 1.5269, Acc: 59.7015\n",
      "\t\t Training: Epoch(6) - Loss: 0.1713, Acc: 94.3522\n",
      "\t\t Validation(6) - Loss: 1.9260, Acc: 61.1940\n",
      "\t\t Training: Epoch(7) - Loss: 0.0903, Acc: 96.6777\n",
      "\t\t Validation(7) - Loss: 1.2575, Acc: 71.6418\n",
      "\t\t Training: Epoch(8) - Loss: 0.0745, Acc: 97.6744\n",
      "\t\t Validation(8) - Loss: 1.2858, Acc: 70.1493\n",
      "\t\t Training: Epoch(9) - Loss: 0.0476, Acc: 99.0033\n",
      "\t\t Validation(9) - Loss: 0.5880, Acc: 85.0746\n",
      "Finished.\n",
      "Total time per fold: 271.3439691066742 seconds.\n",
      "Fold : 6\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.1409, Acc: 51.6611\n",
      "\t\t Validation(0) - Loss: 2.4886, Acc: 20.8955\n",
      "\t\t Training: Epoch(1) - Loss: 0.4295, Acc: 90.3654\n",
      "\t\t Validation(1) - Loss: 2.2090, Acc: 32.8358\n",
      "\t\t Training: Epoch(2) - Loss: 0.2812, Acc: 90.6977\n",
      "\t\t Validation(2) - Loss: 2.0256, Acc: 47.7612\n",
      "\t\t Training: Epoch(3) - Loss: 0.2299, Acc: 93.6877\n",
      "\t\t Validation(3) - Loss: 1.1008, Acc: 65.6716\n",
      "\t\t Training: Epoch(4) - Loss: 0.1556, Acc: 95.6811\n",
      "\t\t Validation(4) - Loss: 1.5869, Acc: 61.1940\n",
      "\t\t Training: Epoch(5) - Loss: 0.1116, Acc: 96.8439\n",
      "\t\t Validation(5) - Loss: 1.1922, Acc: 76.1194\n",
      "\t\t Training: Epoch(6) - Loss: 0.1757, Acc: 95.5150\n",
      "\t\t Validation(6) - Loss: 1.3179, Acc: 70.1493\n",
      "\t\t Training: Epoch(7) - Loss: 0.1323, Acc: 97.1761\n",
      "\t\t Validation(7) - Loss: 1.0311, Acc: 80.5970\n",
      "\t\t Training: Epoch(8) - Loss: 0.0813, Acc: 98.1728\n",
      "\t\t Validation(8) - Loss: 1.0569, Acc: 83.5821\n",
      "\t\t Training: Epoch(9) - Loss: 0.0736, Acc: 97.8405\n",
      "\t\t Validation(9) - Loss: 0.3055, Acc: 91.0448\n",
      "Finished.\n",
      "Total time per fold: 275.55724239349365 seconds.\n",
      "Fold : 7\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.0946, Acc: 55.4817\n",
      "\t\t Validation(0) - Loss: 2.5778, Acc: 25.3731\n",
      "\t\t Training: Epoch(1) - Loss: 0.4328, Acc: 88.5382\n",
      "\t\t Validation(1) - Loss: 2.3290, Acc: 29.8507\n",
      "\t\t Training: Epoch(2) - Loss: 0.2297, Acc: 93.3555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Validation(2) - Loss: 2.3106, Acc: 44.7761\n",
      "\t\t Training: Epoch(3) - Loss: 0.2069, Acc: 93.1894\n",
      "\t\t Validation(3) - Loss: 3.2814, Acc: 38.8060\n",
      "\t\t Training: Epoch(4) - Loss: 0.1821, Acc: 94.5183\n",
      "\t\t Validation(4) - Loss: 1.2449, Acc: 67.1642\n",
      "\t\t Training: Epoch(5) - Loss: 0.1606, Acc: 95.8472\n",
      "\t\t Validation(5) - Loss: 1.3256, Acc: 74.6269\n",
      "\t\t Training: Epoch(6) - Loss: 0.2046, Acc: 94.3522\n",
      "\t\t Validation(6) - Loss: 0.5253, Acc: 83.5821\n",
      "\t\t Training: Epoch(7) - Loss: 0.1061, Acc: 97.5083\n",
      "\t\t Validation(7) - Loss: 0.9302, Acc: 76.1194\n",
      "\t\t Training: Epoch(8) - Loss: 0.1079, Acc: 96.5116\n",
      "\t\t Validation(8) - Loss: 1.7104, Acc: 62.6866\n",
      "\t\t Training: Epoch(9) - Loss: 0.1388, Acc: 96.1794\n",
      "\t\t Validation(9) - Loss: 0.3053, Acc: 91.0448\n",
      "Finished.\n",
      "Total time per fold: 270.3141601085663 seconds.\n",
      "Fold : 8\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.0797, Acc: 51.3289\n",
      "\t\t Validation(0) - Loss: 3.0111, Acc: 16.4179\n",
      "\t\t Training: Epoch(1) - Loss: 0.4397, Acc: 88.3721\n",
      "\t\t Validation(1) - Loss: 3.0062, Acc: 16.4179\n",
      "\t\t Training: Epoch(2) - Loss: 0.2136, Acc: 93.5216\n",
      "\t\t Validation(2) - Loss: 2.7381, Acc: 32.8358\n",
      "\t\t Training: Epoch(3) - Loss: 0.2207, Acc: 93.8538\n",
      "\t\t Validation(3) - Loss: 2.4614, Acc: 37.3134\n",
      "\t\t Training: Epoch(4) - Loss: 0.1486, Acc: 95.3488\n",
      "\t\t Validation(4) - Loss: 1.1111, Acc: 64.1791\n",
      "\t\t Training: Epoch(5) - Loss: 0.1831, Acc: 93.6877\n",
      "\t\t Validation(5) - Loss: 1.4898, Acc: 62.6866\n",
      "\t\t Training: Epoch(6) - Loss: 0.1416, Acc: 96.3455\n",
      "\t\t Validation(6) - Loss: 1.9836, Acc: 67.1642\n",
      "\t\t Training: Epoch(7) - Loss: 0.0924, Acc: 97.3422\n",
      "\t\t Validation(7) - Loss: 1.1033, Acc: 74.6269\n",
      "\t\t Training: Epoch(8) - Loss: 0.0988, Acc: 97.3422\n",
      "\t\t Validation(8) - Loss: 1.0959, Acc: 73.1343\n",
      "\t\t Training: Epoch(9) - Loss: 0.0921, Acc: 97.5083\n",
      "\t\t Validation(9) - Loss: 0.7057, Acc: 80.5970\n",
      "Finished.\n",
      "Total time per fold: 265.6273126602173 seconds.\n",
      "Fold : 9\n",
      "Samples in training: 603\n",
      "Samples in test: 66\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.0484, Acc: 54.5605\n",
      "\t\t Validation(0) - Loss: 2.8006, Acc: 18.1818\n",
      "\t\t Training: Epoch(1) - Loss: 0.5061, Acc: 85.4063\n",
      "\t\t Validation(1) - Loss: 2.1305, Acc: 37.8788\n",
      "\t\t Training: Epoch(2) - Loss: 0.3777, Acc: 87.7280\n",
      "\t\t Validation(2) - Loss: 1.5477, Acc: 54.5455\n",
      "\t\t Training: Epoch(3) - Loss: 0.2410, Acc: 92.7032\n",
      "\t\t Validation(3) - Loss: 1.6915, Acc: 60.6061\n",
      "\t\t Training: Epoch(4) - Loss: 0.1590, Acc: 96.6833\n",
      "\t\t Validation(4) - Loss: 1.0644, Acc: 74.2424\n",
      "\t\t Training: Epoch(5) - Loss: 0.1364, Acc: 95.3566\n",
      "\t\t Validation(5) - Loss: 0.7183, Acc: 75.7576\n",
      "\t\t Training: Epoch(6) - Loss: 0.1167, Acc: 96.6833\n",
      "\t\t Validation(6) - Loss: 0.5168, Acc: 83.3333\n",
      "\t\t Training: Epoch(7) - Loss: 0.1129, Acc: 96.6833\n",
      "\t\t Validation(7) - Loss: 1.2827, Acc: 69.6970\n",
      "\t\t Training: Epoch(8) - Loss: 0.0652, Acc: 98.3416\n",
      "\t\t Validation(8) - Loss: 1.4713, Acc: 71.2121\n",
      "\t\t Training: Epoch(9) - Loss: 0.0379, Acc: 98.8391\n",
      "\t\t Validation(9) - Loss: 0.9269, Acc: 81.8182\n",
      "Finished.\n",
      "Total time per fold: 268.0453133583069 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Variables to store fold scores\n",
    "train_acc = []\n",
    "test_top1_acc = []\n",
    "test_top5_acc = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "times = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(splits.split(total_set)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('Fold : {}'.format(fold))\n",
    "    \n",
    "    # Train and val samplers\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    print(\"Samples in training:\", len(train_sampler))\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    print(\"Samples in test:\", len(valid_sampler))\n",
    "    \n",
    "    # Train and val loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=train_batch_size, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=1, sampler=valid_sampler)\n",
    "    \n",
    "    device = get_device()\n",
    "    \n",
    "    criterion, model, optimizer = create_optimizer(load_model())\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(h_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        trunning_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += (preds == labels).sum()\n",
    "            trunning_corrects += preds.size(0)\n",
    "            \n",
    "\n",
    "        epoch_loss = running_loss / trunning_corrects\n",
    "        epoch_acc = (running_corrects.double()*100) / trunning_corrects\n",
    "        train_acc.append(epoch_acc.item())\n",
    "        \n",
    "        print('\\t\\t Training: Epoch({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        model.eval()  \n",
    "        \n",
    "        vrunning_loss = 0.0\n",
    "        vrunning_corrects = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for data, labels in valid_loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(data)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            vrunning_loss += loss.item() * data.size(0)\n",
    "            vrunning_corrects += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            \n",
    "        vepoch_loss = vrunning_loss/num_samples\n",
    "        vepoch_acc = (vrunning_corrects.double() * 100)/num_samples\n",
    "        \n",
    "        print('\\t\\t Validation({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, vepoch_loss, vepoch_acc))\n",
    "    \n",
    "    # Calculating and appending scores to this fold\n",
    "    scores = get_scores(model, valid_loader, total_set)\n",
    "    \n",
    "    test_top1_acc.append(scores[0])\n",
    "    test_top5_acc.append(scores[1])\n",
    "    test_precision.append(scores[2])\n",
    "    test_recall.append(scores[3])\n",
    "    test_f1.append(scores[4])\n",
    "    \n",
    "    time_fold = time.time() - start_time\n",
    "    times.append(time_fold)\n",
    "    print(\"Total time per fold: %s seconds.\" %(time_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy average:  0.9050825606188326\n",
      "Top-1 test accuracy average:  0.8668928086838534\n",
      "Top-5 test accuracy average:  0.9940298507462687\n",
      "Weighted Precision test accuracy average:  0.8837668798862829\n",
      "Weighted Recall test accuracy average:  0.8668928086838534\n",
      "Weighted F1 test accuracy average:  0.8558060264776683\n",
      "Average time per fold (seconds): 268.0622427940369\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy average: \", np.mean(train_acc) / 100)\n",
    "print(\"Top-1 test accuracy average: \", np.mean(test_top1_acc))\n",
    "print(\"Top-5 test accuracy average: \", np.mean(test_top5_acc))\n",
    "print(\"Weighted Precision test accuracy average: \", np.mean(test_precision))\n",
    "print(\"Weighted Recall test accuracy average: \", np.mean(test_recall))\n",
    "print(\"Weighted F1 test accuracy average: \", np.mean(test_f1))\n",
    "print(\"Average time per fold (seconds):\", np.mean(times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
