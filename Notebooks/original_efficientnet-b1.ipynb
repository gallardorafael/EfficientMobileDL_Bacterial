{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the paper: \n",
    "\n",
    "## Efficient and Mobile Deep Learning Architectures for Fast Identification of BacterialStrains in Resource-Constrained Devices\n",
    "\n",
    "### Architecture: EfficientNet B1\n",
    "### Data: Original + Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, models\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "# Archs not in Pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# External functions\n",
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "3.8.5 (default, Jul 28 2020, 12:59:40) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data paths and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters and dataset details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset details\n",
    "dataset_version = 'original' # original or augmented\n",
    "img_shape = (224,224)\n",
    "img_size = str(img_shape[0])+\"x\"+str(img_shape[1])\n",
    "\n",
    "# Root directory of dataset\n",
    "data_dir = '/home/yibbtstll/venvs/pytorch_gpu/CySDeepBacterial/Dataset/DIBaS/'\n",
    "\n",
    "train_batch_size = 32\n",
    "val_test_batch_size = 32\n",
    "feature_extract = False\n",
    "pretrained = True\n",
    "h_epochs = 10\n",
    "kfolds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining transforms and creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for input data\n",
    "training_transforms = transforms.Compose([transforms.Resize((224,224), Image.LANCZOS),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "total_set = datasets.ImageFolder(data_dir, transform=training_transforms)\n",
    "\n",
    "# Defining folds\n",
    "splits = KFold(n_splits = kfolds, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the target classes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "{0: 'Acinetobacter.baumanii', 1: 'Actinomyces.israeli', 2: 'Bacteroides.fragilis', 3: 'Bifidobacterium.spp', 4: 'Clostridium.perfringens', 5: 'Enterococcus.faecalis', 6: 'Enterococcus.faecium', 7: 'Escherichia.coli', 8: 'Fusobacterium', 9: 'Lactobacillus.casei', 10: 'Lactobacillus.crispatus', 11: 'Lactobacillus.delbrueckii', 12: 'Lactobacillus.gasseri', 13: 'Lactobacillus.jehnsenii', 14: 'Lactobacillus.johnsonii', 15: 'Lactobacillus.paracasei', 16: 'Lactobacillus.plantarum', 17: 'Lactobacillus.reuteri', 18: 'Lactobacillus.rhamnosus', 19: 'Lactobacillus.salivarius', 20: 'Listeria.monocytogenes', 21: 'Micrococcus.spp', 22: 'Neisseria.gonorrhoeae', 23: 'Porfyromonas.gingivalis', 24: 'Propionibacterium.acnes', 25: 'Proteus', 26: 'Pseudomonas.aeruginosa', 27: 'Staphylococcus.aureus', 28: 'Staphylococcus.epidermidis', 29: 'Staphylococcus.saprophiticus', 30: 'Streptococcus.agalactiae', 31: 'Veionella'}\n"
     ]
    }
   ],
   "source": [
    "train_labels = {value : key for (key, value) in total_set.class_to_idx.items()}\n",
    "    \n",
    "print(len(train_labels)) \n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and inicialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing pre-trained parameters, finetunning the classifier to output 32 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained model parameters to avoid backpropogating through them\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        print(\"Setting grad to false.\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_device():\n",
    "    # Model and criterion to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Transfer Learning\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    \n",
    "    # Mode\n",
    "    model = set_parameter_requires_grad(model, feature_extract)\n",
    "    \n",
    "    # Fine tuning\n",
    "    # Build custom classifier\n",
    "    model._fc = nn.Linear(in_features=1280,\n",
    "                        out_features=32)\n",
    "    return model\n",
    "\n",
    "def create_optimizer(model):\n",
    "    # Parameters to update\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "\n",
    "    else:\n",
    "        n_params = 0\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                n_params += 1\n",
    "\n",
    "\n",
    "    # Loss function and gradient descent\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(params_to_update, \n",
    "                          lr=0.001, \n",
    "                          weight_decay=0.000004)\n",
    "    \n",
    "    return criterion.to(get_device()), model.to(get_device()), optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.1498, Acc: 51.4950\n",
      "\t\t Validation(0) - Loss: 3.0938, Acc: 13.4328\n",
      "\t\t Training: Epoch(1) - Loss: 0.5076, Acc: 85.2159\n",
      "\t\t Validation(1) - Loss: 2.6250, Acc: 20.8955\n",
      "\t\t Training: Epoch(2) - Loss: 0.2966, Acc: 90.0332\n",
      "\t\t Validation(2) - Loss: 2.1584, Acc: 43.2836\n",
      "\t\t Training: Epoch(3) - Loss: 0.2140, Acc: 93.5216\n",
      "\t\t Validation(3) - Loss: 1.2374, Acc: 61.1940\n",
      "\t\t Training: Epoch(4) - Loss: 0.1406, Acc: 96.1794\n",
      "\t\t Validation(4) - Loss: 1.1396, Acc: 71.6418\n",
      "\t\t Training: Epoch(5) - Loss: 0.1360, Acc: 97.0100\n",
      "\t\t Validation(5) - Loss: 1.0834, Acc: 76.1194\n",
      "\t\t Training: Epoch(6) - Loss: 0.1451, Acc: 95.6811\n",
      "\t\t Validation(6) - Loss: 0.9365, Acc: 77.6119\n",
      "\t\t Training: Epoch(7) - Loss: 0.1319, Acc: 96.3455\n",
      "\t\t Validation(7) - Loss: 0.8497, Acc: 80.5970\n",
      "\t\t Training: Epoch(8) - Loss: 0.1598, Acc: 95.3488\n",
      "\t\t Validation(8) - Loss: 0.7088, Acc: 85.0746\n",
      "\t\t Training: Epoch(9) - Loss: 0.1516, Acc: 97.3422\n",
      "\t\t Validation(9) - Loss: 0.7815, Acc: 79.1045\n",
      "Finished.\n",
      "Total time per fold: 254.39333271980286 seconds.\n",
      "Fold : 1\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.0342, Acc: 57.1429\n",
      "\t\t Validation(0) - Loss: 2.7096, Acc: 20.8955\n",
      "\t\t Training: Epoch(1) - Loss: 0.5019, Acc: 85.5482\n",
      "\t\t Validation(1) - Loss: 2.1731, Acc: 35.8209\n",
      "\t\t Training: Epoch(2) - Loss: 0.3765, Acc: 89.7010\n",
      "\t\t Validation(2) - Loss: 1.5600, Acc: 64.1791\n",
      "\t\t Training: Epoch(3) - Loss: 0.2153, Acc: 93.3555\n",
      "\t\t Validation(3) - Loss: 1.2464, Acc: 62.6866\n",
      "\t\t Training: Epoch(4) - Loss: 0.2125, Acc: 94.3522\n",
      "\t\t Validation(4) - Loss: 0.5522, Acc: 82.0896\n",
      "\t\t Training: Epoch(5) - Loss: 0.1141, Acc: 96.5116\n",
      "\t\t Validation(5) - Loss: 0.9501, Acc: 85.0746\n",
      "\t\t Training: Epoch(6) - Loss: 0.0654, Acc: 98.5050\n",
      "\t\t Validation(6) - Loss: 0.4048, Acc: 86.5672\n",
      "\t\t Training: Epoch(7) - Loss: 0.1222, Acc: 96.8439\n",
      "\t\t Validation(7) - Loss: 0.3476, Acc: 85.0746\n",
      "\t\t Training: Epoch(8) - Loss: 0.1797, Acc: 96.0133\n",
      "\t\t Validation(8) - Loss: 0.3633, Acc: 89.5522\n",
      "\t\t Training: Epoch(9) - Loss: 0.0737, Acc: 98.1728\n",
      "\t\t Validation(9) - Loss: 0.4069, Acc: 83.5821\n",
      "Finished.\n",
      "Total time per fold: 260.15892910957336 seconds.\n",
      "Fold : 2\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.1419, Acc: 53.8206\n",
      "\t\t Validation(0) - Loss: 3.1304, Acc: 11.9403\n",
      "\t\t Training: Epoch(1) - Loss: 0.5044, Acc: 86.0465\n",
      "\t\t Validation(1) - Loss: 2.0704, Acc: 43.2836\n",
      "\t\t Training: Epoch(2) - Loss: 0.2597, Acc: 92.5249\n",
      "\t\t Validation(2) - Loss: 1.5462, Acc: 50.7463\n",
      "\t\t Training: Epoch(3) - Loss: 0.3339, Acc: 90.0332\n",
      "\t\t Validation(3) - Loss: 1.7036, Acc: 55.2239\n",
      "\t\t Training: Epoch(4) - Loss: 0.1805, Acc: 95.0166\n",
      "\t\t Validation(4) - Loss: 1.4885, Acc: 61.1940\n",
      "\t\t Training: Epoch(5) - Loss: 0.1653, Acc: 96.0133\n",
      "\t\t Validation(5) - Loss: 0.6303, Acc: 82.0896\n",
      "\t\t Training: Epoch(6) - Loss: 0.1325, Acc: 96.6777\n",
      "\t\t Validation(6) - Loss: 0.7096, Acc: 82.0896\n",
      "\t\t Training: Epoch(7) - Loss: 0.1561, Acc: 94.8505\n",
      "\t\t Validation(7) - Loss: 0.5404, Acc: 82.0896\n",
      "\t\t Training: Epoch(8) - Loss: 0.0813, Acc: 98.5050\n",
      "\t\t Validation(8) - Loss: 1.4331, Acc: 68.6567\n",
      "\t\t Training: Epoch(9) - Loss: 0.0878, Acc: 96.5116\n",
      "\t\t Validation(9) - Loss: 1.1698, Acc: 73.1343\n",
      "Finished.\n",
      "Total time per fold: 262.26750349998474 seconds.\n",
      "Fold : 3\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.0431, Acc: 53.9867\n",
      "\t\t Validation(0) - Loss: 2.4909, Acc: 25.3731\n",
      "\t\t Training: Epoch(1) - Loss: 0.4755, Acc: 86.7110\n",
      "\t\t Validation(1) - Loss: 2.5033, Acc: 35.8209\n",
      "\t\t Training: Epoch(2) - Loss: 0.2221, Acc: 94.3522\n",
      "\t\t Validation(2) - Loss: 2.9384, Acc: 31.3433\n",
      "\t\t Training: Epoch(3) - Loss: 0.2121, Acc: 93.5216\n",
      "\t\t Validation(3) - Loss: 2.6353, Acc: 38.8060\n",
      "\t\t Training: Epoch(4) - Loss: 0.1917, Acc: 94.6844\n",
      "\t\t Validation(4) - Loss: 2.9343, Acc: 44.7761\n",
      "\t\t Training: Epoch(5) - Loss: 0.1531, Acc: 96.1794\n",
      "\t\t Validation(5) - Loss: 1.2926, Acc: 71.6418\n",
      "\t\t Training: Epoch(6) - Loss: 0.1918, Acc: 95.6811\n",
      "\t\t Validation(6) - Loss: 1.4196, Acc: 62.6866\n",
      "\t\t Training: Epoch(7) - Loss: 0.1424, Acc: 95.3488\n",
      "\t\t Validation(7) - Loss: 0.7567, Acc: 79.1045\n",
      "\t\t Training: Epoch(8) - Loss: 0.1114, Acc: 96.5116\n",
      "\t\t Validation(8) - Loss: 0.6480, Acc: 80.5970\n",
      "\t\t Training: Epoch(9) - Loss: 0.1298, Acc: 95.3488\n",
      "\t\t Validation(9) - Loss: 0.3493, Acc: 91.0448\n",
      "Finished.\n",
      "Total time per fold: 258.68224930763245 seconds.\n",
      "Fold : 4\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.0365, Acc: 60.9635\n",
      "\t\t Validation(0) - Loss: 2.3223, Acc: 35.8209\n",
      "\t\t Training: Epoch(1) - Loss: 0.4442, Acc: 89.7010\n",
      "\t\t Validation(1) - Loss: 1.9377, Acc: 43.2836\n",
      "\t\t Training: Epoch(2) - Loss: 0.2910, Acc: 89.3688\n",
      "\t\t Validation(2) - Loss: 1.8887, Acc: 52.2388\n",
      "\t\t Training: Epoch(3) - Loss: 0.2424, Acc: 93.6877\n",
      "\t\t Validation(3) - Loss: 2.2412, Acc: 40.2985\n",
      "\t\t Training: Epoch(4) - Loss: 0.1891, Acc: 95.1827\n",
      "\t\t Validation(4) - Loss: 2.0012, Acc: 50.7463\n",
      "\t\t Training: Epoch(5) - Loss: 0.1478, Acc: 96.5116\n",
      "\t\t Validation(5) - Loss: 2.9129, Acc: 41.7910\n",
      "\t\t Training: Epoch(6) - Loss: 0.1350, Acc: 96.6777\n",
      "\t\t Validation(6) - Loss: 2.2395, Acc: 47.7612\n",
      "\t\t Training: Epoch(7) - Loss: 0.1142, Acc: 96.3455\n",
      "\t\t Validation(7) - Loss: 1.0795, Acc: 74.6269\n",
      "\t\t Training: Epoch(8) - Loss: 0.1742, Acc: 94.6844\n",
      "\t\t Validation(8) - Loss: 0.5557, Acc: 86.5672\n",
      "\t\t Training: Epoch(9) - Loss: 0.0747, Acc: 98.3389\n",
      "\t\t Validation(9) - Loss: 0.4700, Acc: 89.5522\n",
      "Finished.\n",
      "Total time per fold: 259.8636140823364 seconds.\n",
      "Fold : 5\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 1.9479, Acc: 57.9734\n",
      "\t\t Validation(0) - Loss: 2.9311, Acc: 25.3731\n",
      "\t\t Training: Epoch(1) - Loss: 0.4756, Acc: 87.2093\n",
      "\t\t Validation(1) - Loss: 3.1639, Acc: 25.3731\n",
      "\t\t Training: Epoch(2) - Loss: 0.2918, Acc: 90.5316\n",
      "\t\t Validation(2) - Loss: 2.1553, Acc: 38.8060\n",
      "\t\t Training: Epoch(3) - Loss: 0.2215, Acc: 94.1860\n",
      "\t\t Validation(3) - Loss: 2.1971, Acc: 53.7313\n",
      "\t\t Training: Epoch(4) - Loss: 0.1582, Acc: 95.0166\n",
      "\t\t Validation(4) - Loss: 2.0378, Acc: 64.1791\n",
      "\t\t Training: Epoch(5) - Loss: 0.1834, Acc: 94.0199\n",
      "\t\t Validation(5) - Loss: 2.0098, Acc: 73.1343\n",
      "\t\t Training: Epoch(6) - Loss: 0.1393, Acc: 95.6811\n",
      "\t\t Validation(6) - Loss: 0.8066, Acc: 79.1045\n",
      "\t\t Training: Epoch(7) - Loss: 0.1739, Acc: 95.5150\n",
      "\t\t Validation(7) - Loss: 1.8352, Acc: 71.6418\n",
      "\t\t Training: Epoch(8) - Loss: 0.0801, Acc: 97.6744\n",
      "\t\t Validation(8) - Loss: 0.4230, Acc: 91.0448\n",
      "\t\t Training: Epoch(9) - Loss: 0.0630, Acc: 98.5050\n",
      "\t\t Validation(9) - Loss: 0.7277, Acc: 82.0896\n",
      "Finished.\n",
      "Total time per fold: 258.4626486301422 seconds.\n",
      "Fold : 6\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.1230, Acc: 54.1528\n",
      "\t\t Validation(0) - Loss: 3.1465, Acc: 7.4627\n",
      "\t\t Training: Epoch(1) - Loss: 0.5189, Acc: 87.0432\n",
      "\t\t Validation(1) - Loss: 2.2705, Acc: 40.2985\n",
      "\t\t Training: Epoch(2) - Loss: 0.2554, Acc: 93.0233\n",
      "\t\t Validation(2) - Loss: 1.5603, Acc: 49.2537\n",
      "\t\t Training: Epoch(3) - Loss: 0.2082, Acc: 94.1860\n",
      "\t\t Validation(3) - Loss: 1.2617, Acc: 61.1940\n",
      "\t\t Training: Epoch(4) - Loss: 0.2603, Acc: 92.1927\n",
      "\t\t Validation(4) - Loss: 0.8072, Acc: 73.1343\n",
      "\t\t Training: Epoch(5) - Loss: 0.2008, Acc: 93.8538\n",
      "\t\t Validation(5) - Loss: 1.1055, Acc: 71.6418\n",
      "\t\t Training: Epoch(6) - Loss: 0.0857, Acc: 97.8405\n",
      "\t\t Validation(6) - Loss: 0.5440, Acc: 88.0597\n",
      "\t\t Training: Epoch(7) - Loss: 0.0848, Acc: 97.3422\n",
      "\t\t Validation(7) - Loss: 0.4570, Acc: 86.5672\n",
      "\t\t Training: Epoch(8) - Loss: 0.0822, Acc: 97.5083\n",
      "\t\t Validation(8) - Loss: 0.4861, Acc: 80.5970\n",
      "\t\t Training: Epoch(9) - Loss: 0.0707, Acc: 98.5050\n",
      "\t\t Validation(9) - Loss: 0.2624, Acc: 95.5224\n",
      "Finished.\n",
      "Total time per fold: 256.3321042060852 seconds.\n",
      "Fold : 7\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.0875, Acc: 57.6412\n",
      "\t\t Validation(0) - Loss: 2.8479, Acc: 11.9403\n",
      "\t\t Training: Epoch(1) - Loss: 0.5696, Acc: 85.2159\n",
      "\t\t Validation(1) - Loss: 2.4407, Acc: 34.3284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Training: Epoch(2) - Loss: 0.3038, Acc: 90.8638\n",
      "\t\t Validation(2) - Loss: 2.0904, Acc: 35.8209\n",
      "\t\t Training: Epoch(3) - Loss: 0.2360, Acc: 93.6877\n",
      "\t\t Validation(3) - Loss: 1.3587, Acc: 59.7015\n",
      "\t\t Training: Epoch(4) - Loss: 0.1466, Acc: 96.6777\n",
      "\t\t Validation(4) - Loss: 1.3587, Acc: 52.2388\n",
      "\t\t Training: Epoch(5) - Loss: 0.0929, Acc: 97.6744\n",
      "\t\t Validation(5) - Loss: 0.9650, Acc: 70.1493\n",
      "\t\t Training: Epoch(6) - Loss: 0.1277, Acc: 96.6777\n",
      "\t\t Validation(6) - Loss: 0.4304, Acc: 85.0746\n",
      "\t\t Training: Epoch(7) - Loss: 0.0739, Acc: 98.1728\n",
      "\t\t Validation(7) - Loss: 0.3289, Acc: 91.0448\n",
      "\t\t Training: Epoch(8) - Loss: 0.1200, Acc: 96.1794\n",
      "\t\t Validation(8) - Loss: 0.6816, Acc: 80.5970\n",
      "\t\t Training: Epoch(9) - Loss: 0.0638, Acc: 98.0066\n",
      "\t\t Validation(9) - Loss: 0.6873, Acc: 83.5821\n",
      "Finished.\n",
      "Total time per fold: 260.42923974990845 seconds.\n",
      "Fold : 8\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.1396, Acc: 54.8173\n",
      "\t\t Validation(0) - Loss: 2.7543, Acc: 22.3881\n",
      "\t\t Training: Epoch(1) - Loss: 0.4786, Acc: 89.2027\n",
      "\t\t Validation(1) - Loss: 2.7150, Acc: 28.3582\n",
      "\t\t Training: Epoch(2) - Loss: 0.3254, Acc: 88.8704\n",
      "\t\t Validation(2) - Loss: 1.9821, Acc: 47.7612\n",
      "\t\t Training: Epoch(3) - Loss: 0.2343, Acc: 92.8571\n",
      "\t\t Validation(3) - Loss: 1.4958, Acc: 47.7612\n",
      "\t\t Training: Epoch(4) - Loss: 0.1675, Acc: 94.1860\n",
      "\t\t Validation(4) - Loss: 1.7793, Acc: 56.7164\n",
      "\t\t Training: Epoch(5) - Loss: 0.1514, Acc: 96.1794\n",
      "\t\t Validation(5) - Loss: 1.9019, Acc: 61.1940\n",
      "\t\t Training: Epoch(6) - Loss: 0.1467, Acc: 96.3455\n",
      "\t\t Validation(6) - Loss: 1.3941, Acc: 73.1343\n",
      "\t\t Training: Epoch(7) - Loss: 0.1509, Acc: 95.8472\n",
      "\t\t Validation(7) - Loss: 1.4297, Acc: 73.1343\n",
      "\t\t Training: Epoch(8) - Loss: 0.1490, Acc: 95.5150\n",
      "\t\t Validation(8) - Loss: 1.0491, Acc: 73.1343\n",
      "\t\t Training: Epoch(9) - Loss: 0.1141, Acc: 96.8439\n",
      "\t\t Validation(9) - Loss: 0.8783, Acc: 80.5970\n",
      "Finished.\n",
      "Total time per fold: 256.6826777458191 seconds.\n",
      "Fold : 9\n",
      "Samples in training: 603\n",
      "Samples in test: 66\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\t\t Training: Epoch(0) - Loss: 2.0672, Acc: 54.8922\n",
      "\t\t Validation(0) - Loss: 3.0596, Acc: 9.0909\n",
      "\t\t Training: Epoch(1) - Loss: 0.4355, Acc: 88.8889\n",
      "\t\t Validation(1) - Loss: 3.3215, Acc: 21.2121\n",
      "\t\t Training: Epoch(2) - Loss: 0.2617, Acc: 92.7032\n",
      "\t\t Validation(2) - Loss: 2.8972, Acc: 27.2727\n",
      "\t\t Training: Epoch(3) - Loss: 0.1905, Acc: 93.8640\n",
      "\t\t Validation(3) - Loss: 3.0821, Acc: 36.3636\n",
      "\t\t Training: Epoch(4) - Loss: 0.1686, Acc: 94.0299\n",
      "\t\t Validation(4) - Loss: 2.0552, Acc: 48.4848\n",
      "\t\t Training: Epoch(5) - Loss: 0.2387, Acc: 92.8690\n",
      "\t\t Validation(5) - Loss: 1.3380, Acc: 63.6364\n",
      "\t\t Training: Epoch(6) - Loss: 0.1879, Acc: 94.5274\n",
      "\t\t Validation(6) - Loss: 1.2024, Acc: 71.2121\n",
      "\t\t Training: Epoch(7) - Loss: 0.1007, Acc: 97.6783\n",
      "\t\t Validation(7) - Loss: 0.7864, Acc: 77.2727\n",
      "\t\t Training: Epoch(8) - Loss: 0.0984, Acc: 96.6833\n",
      "\t\t Validation(8) - Loss: 1.0281, Acc: 74.2424\n",
      "\t\t Training: Epoch(9) - Loss: 0.1022, Acc: 97.1808\n",
      "\t\t Validation(9) - Loss: 1.0999, Acc: 77.2727\n",
      "Finished.\n",
      "Total time per fold: 259.532418012619 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Variables to store fold scores\n",
    "train_acc = []\n",
    "test_top1_acc = []\n",
    "test_top5_acc = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "times = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(splits.split(total_set)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('Fold : {}'.format(fold))\n",
    "    \n",
    "    # Train and val samplers\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    print(\"Samples in training:\", len(train_sampler))\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    print(\"Samples in test:\", len(valid_sampler))\n",
    "    \n",
    "    # Train and val loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=train_batch_size, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=1, sampler=valid_sampler)\n",
    "    \n",
    "    device = get_device()\n",
    "    \n",
    "    criterion, model, optimizer = create_optimizer(load_model())\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(h_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        trunning_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += (preds == labels).sum()\n",
    "            trunning_corrects += preds.size(0)\n",
    "            \n",
    "\n",
    "        epoch_loss = running_loss / trunning_corrects\n",
    "        epoch_acc = (running_corrects.double()*100) / trunning_corrects\n",
    "        train_acc.append(epoch_acc.item())\n",
    "        \n",
    "        print('\\t\\t Training: Epoch({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        model.eval()  \n",
    "        \n",
    "        vrunning_loss = 0.0\n",
    "        vrunning_corrects = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for data, labels in valid_loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(data)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            vrunning_loss += loss.item() * data.size(0)\n",
    "            vrunning_corrects += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            \n",
    "        vepoch_loss = vrunning_loss/num_samples\n",
    "        vepoch_acc = (vrunning_corrects.double() * 100)/num_samples\n",
    "        \n",
    "        print('\\t\\t Validation({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, vepoch_loss, vepoch_acc))\n",
    "    \n",
    "    # Calculating and appending scores to this fold\n",
    "    scores = get_scores(model, valid_loader, total_set)\n",
    "    \n",
    "    test_top1_acc.append(scores[0])\n",
    "    test_top5_acc.append(scores[1])\n",
    "    test_precision.append(scores[2])\n",
    "    test_recall.append(scores[3])\n",
    "    test_f1.append(scores[4])\n",
    "    \n",
    "    time_fold = time.time() - start_time\n",
    "    times.append(time_fold)\n",
    "    print(\"Total time per fold: %s seconds.\" %(time_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy average:  0.9044844988788063\n",
      "Top-1 test accuracy average:  0.8354816824966079\n",
      "Top-5 test accuracy average:  0.98355947535052\n",
      "Weighted Precision test accuracy average:  0.8579975222275085\n",
      "Weighted Recall test accuracy average:  0.8354816824966079\n",
      "Weighted F1 test accuracy average:  0.8224756172517367\n",
      "Average time per fold (seconds): 258.6804717063904\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy average: \", np.mean(train_acc) / 100)\n",
    "print(\"Top-1 test accuracy average: \", np.mean(test_top1_acc))\n",
    "print(\"Top-5 test accuracy average: \", np.mean(test_top5_acc))\n",
    "print(\"Weighted Precision test accuracy average: \", np.mean(test_precision))\n",
    "print(\"Weighted Recall test accuracy average: \", np.mean(test_recall))\n",
    "print(\"Weighted F1 test accuracy average: \", np.mean(test_f1))\n",
    "print(\"Average time per fold (seconds):\", np.mean(times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
