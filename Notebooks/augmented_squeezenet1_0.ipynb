{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the paper: \n",
    "\n",
    "## Efficient and Mobile Deep Learning Architectures for Fast Identification of BacterialStrains in Resource-Constrained Devices\n",
    "\n",
    "### Architecture: SqueezeNet 1.0\n",
    "### Data: Augmented + Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, models\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "# Archs not in Pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# External functions\n",
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "3.8.5 (default, Jul 28 2020, 12:59:40) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data paths and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters and dataset details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset details\n",
    "dataset_version = 'original' # original or augmented\n",
    "img_shape = (224,224)\n",
    "img_size = str(img_shape[0])+\"x\"+str(img_shape[1])\n",
    "\n",
    "# Root directory of dataset\n",
    "data_dir = '/home/yibbtstll/venvs/pytorch_gpu/CySDeepBacterial/Dataset/DIBaS_augmented/'\n",
    "\n",
    "train_batch_size = 64\n",
    "val_test_batch_size = 32\n",
    "feature_extract = False\n",
    "pretrained = True\n",
    "h_epochs = 15\n",
    "kfolds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining transforms and creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for input data\n",
    "training_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "total_set = datasets.ImageFolder(data_dir, transform=training_transforms)\n",
    "\n",
    "# Defining folds\n",
    "splits = KFold(n_splits = kfolds, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the target classes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "{0: 'Acinetobacter.baumanii', 1: 'Actinomyces.israeli', 2: 'Bacteroides.fragilis', 3: 'Bifidobacterium.spp', 4: 'Clostridium.perfringens', 5: 'Enterococcus.faecalis', 6: 'Enterococcus.faecium', 7: 'Escherichia.coli', 8: 'Fusobacterium', 9: 'Lactobacillus.casei', 10: 'Lactobacillus.crispatus', 11: 'Lactobacillus.delbrueckii', 12: 'Lactobacillus.gasseri', 13: 'Lactobacillus.jehnsenii', 14: 'Lactobacillus.johnsonii', 15: 'Lactobacillus.paracasei', 16: 'Lactobacillus.plantarum', 17: 'Lactobacillus.reuteri', 18: 'Lactobacillus.rhamnosus', 19: 'Lactobacillus.salivarius', 20: 'Listeria.monocytogenes', 21: 'Micrococcus.spp', 22: 'Neisseria.gonorrhoeae', 23: 'Porfyromonas.gingivalis', 24: 'Propionibacterium.acnes', 25: 'Proteus', 26: 'Pseudomonas.aeruginosa', 27: 'Staphylococcus.aureus', 28: 'Staphylococcus.epidermidis', 29: 'Staphylococcus.saprophiticus', 30: 'Streptococcus.agalactiae', 31: 'Veionella'}\n"
     ]
    }
   ],
   "source": [
    "train_labels = {value : key for (key, value) in total_set.class_to_idx.items()}\n",
    "    \n",
    "print(len(train_labels)) \n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and inicialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing pre-trained parameters, finetunning the classifier to output 32 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained model parameters to avoid backpropogating through them\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        print(\"Setting grad to false.\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_device():\n",
    "    # Model and criterion to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.squeezenet1_0(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Transfer Learning\n",
    "    model = models.squeezenet1_0(pretrained=pretrained)\n",
    "    \n",
    "    # Mode\n",
    "    model = set_parameter_requires_grad(model, feature_extract)\n",
    "    \n",
    "    # Fine tuning\n",
    "    # Build custom classifier\n",
    "    # Build custom classifier\n",
    "    model.classifier[1] = nn.Conv2d(512, 32, \n",
    "                                kernel_size=(1,1), stride=(1,1))\n",
    "    return model\n",
    "\n",
    "def create_optimizer(model):\n",
    "    # Parameters to update\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "\n",
    "    else:\n",
    "        n_params = 0\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                n_params += 1\n",
    "\n",
    "\n",
    "    # Loss function and gradient descent\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(params_to_update, \n",
    "                          lr=0.001, \n",
    "                          weight_decay=0.000004)\n",
    "    \n",
    "    return criterion.to(get_device()), model.to(get_device()), optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Samples in training: 21665\n",
      "Samples in test: 2408\n",
      "\t\t Training: Epoch(0) - Loss: 2.1265, Acc: 30.7454\n",
      "\t\t Validation(0) - Loss: 1.1799, Acc: 56.6445\n",
      "\t\t Training: Epoch(1) - Loss: 1.0063, Acc: 63.9188\n",
      "\t\t Validation(1) - Loss: 0.7842, Acc: 70.0997\n",
      "\t\t Training: Epoch(2) - Loss: 0.7485, Acc: 72.9241\n",
      "\t\t Validation(2) - Loss: 0.6509, Acc: 74.7924\n",
      "\t\t Training: Epoch(3) - Loss: 0.6268, Acc: 77.6229\n",
      "\t\t Validation(3) - Loss: 0.5159, Acc: 81.7691\n",
      "\t\t Training: Epoch(4) - Loss: 0.5262, Acc: 80.8678\n",
      "\t\t Validation(4) - Loss: 0.4991, Acc: 82.1844\n",
      "\t\t Training: Epoch(5) - Loss: 0.4942, Acc: 82.4140\n",
      "\t\t Validation(5) - Loss: 0.4248, Acc: 86.0050\n",
      "\t\t Training: Epoch(6) - Loss: 0.4331, Acc: 84.2834\n",
      "\t\t Validation(6) - Loss: 0.4323, Acc: 85.0914\n",
      "\t\t Training: Epoch(7) - Loss: 0.4003, Acc: 85.2758\n",
      "\t\t Validation(7) - Loss: 0.5676, Acc: 79.8588\n",
      "\t\t Training: Epoch(8) - Loss: 0.3831, Acc: 86.3143\n",
      "\t\t Validation(8) - Loss: 0.4847, Acc: 82.8904\n",
      "\t\t Training: Epoch(9) - Loss: 0.3579, Acc: 87.2329\n",
      "\t\t Validation(9) - Loss: 0.3103, Acc: 89.2857\n",
      "\t\t Training: Epoch(10) - Loss: 0.3462, Acc: 87.7914\n",
      "\t\t Validation(10) - Loss: 0.3292, Acc: 88.2475\n",
      "\t\t Training: Epoch(11) - Loss: 0.3136, Acc: 88.7930\n",
      "\t\t Validation(11) - Loss: 0.3218, Acc: 88.6628\n",
      "\t\t Training: Epoch(12) - Loss: 0.3213, Acc: 88.5345\n",
      "\t\t Validation(12) - Loss: 0.4119, Acc: 86.5864\n",
      "\t\t Training: Epoch(13) - Loss: 0.3236, Acc: 88.4191\n",
      "\t\t Validation(13) - Loss: 0.4296, Acc: 86.7110\n",
      "\t\t Training: Epoch(14) - Loss: 0.2908, Acc: 89.4669\n",
      "\t\t Validation(14) - Loss: 0.5913, Acc: 81.8937\n",
      "Finished.\n",
      "Total time per fold: 1769.808528661728 seconds.\n",
      "Fold : 1\n",
      "Samples in training: 21665\n",
      "Samples in test: 2408\n",
      "\t\t Training: Epoch(0) - Loss: 2.3421, Acc: 26.1666\n",
      "\t\t Validation(0) - Loss: 1.3643, Acc: 51.3289\n",
      "\t\t Training: Epoch(1) - Loss: 1.0866, Acc: 61.5140\n",
      "\t\t Validation(1) - Loss: 0.8989, Acc: 68.4801\n",
      "\t\t Training: Epoch(2) - Loss: 0.7618, Acc: 72.6702\n",
      "\t\t Validation(2) - Loss: 0.9735, Acc: 66.7359\n",
      "\t\t Training: Epoch(3) - Loss: 0.6230, Acc: 77.8168\n",
      "\t\t Validation(3) - Loss: 0.4832, Acc: 81.5615\n",
      "\t\t Training: Epoch(4) - Loss: 0.5419, Acc: 80.6970\n",
      "\t\t Validation(4) - Loss: 0.5589, Acc: 79.3605\n",
      "\t\t Training: Epoch(5) - Loss: 0.4635, Acc: 83.1664\n",
      "\t\t Validation(5) - Loss: 0.4471, Acc: 84.0532\n",
      "\t\t Training: Epoch(6) - Loss: 0.4285, Acc: 84.4265\n",
      "\t\t Validation(6) - Loss: 0.5190, Acc: 81.1047\n",
      "\t\t Training: Epoch(7) - Loss: 0.4051, Acc: 85.3496\n",
      "\t\t Validation(7) - Loss: 0.4947, Acc: 82.3090\n",
      "\t\t Training: Epoch(8) - Loss: 0.3840, Acc: 86.0143\n",
      "\t\t Validation(8) - Loss: 0.3770, Acc: 86.6279\n",
      "\t\t Training: Epoch(9) - Loss: 0.3788, Acc: 86.1620\n",
      "\t\t Validation(9) - Loss: 0.3994, Acc: 85.7973\n",
      "\t\t Training: Epoch(10) - Loss: 0.3333, Acc: 87.7729\n",
      "\t\t Validation(10) - Loss: 0.3077, Acc: 88.3721\n",
      "\t\t Training: Epoch(11) - Loss: 0.3509, Acc: 87.1498\n",
      "\t\t Validation(11) - Loss: 0.3151, Acc: 88.9120\n",
      "\t\t Training: Epoch(12) - Loss: 0.3229, Acc: 88.4930\n",
      "\t\t Validation(12) - Loss: 0.4380, Acc: 84.5930\n",
      "\t\t Training: Epoch(13) - Loss: 0.3187, Acc: 88.4976\n",
      "\t\t Validation(13) - Loss: 0.2716, Acc: 89.7841\n",
      "\t\t Training: Epoch(14) - Loss: 0.3065, Acc: 89.0192\n",
      "\t\t Validation(14) - Loss: 0.2816, Acc: 89.7425\n",
      "Finished.\n",
      "Total time per fold: 1305.0585153102875 seconds.\n",
      "Fold : 2\n",
      "Samples in training: 21665\n",
      "Samples in test: 2408\n",
      "\t\t Training: Epoch(0) - Loss: 1.7621, Acc: 42.3910\n",
      "\t\t Validation(0) - Loss: 0.9258, Acc: 66.9850\n",
      "\t\t Training: Epoch(1) - Loss: 0.8209, Acc: 71.3963\n",
      "\t\t Validation(1) - Loss: 0.9125, Acc: 67.2342\n",
      "\t\t Training: Epoch(2) - Loss: 0.6798, Acc: 76.1274\n",
      "\t\t Validation(2) - Loss: 0.5928, Acc: 78.9867\n",
      "\t\t Training: Epoch(3) - Loss: 0.5726, Acc: 79.8015\n",
      "\t\t Validation(3) - Loss: 0.5673, Acc: 77.4086\n",
      "\t\t Training: Epoch(4) - Loss: 0.5079, Acc: 82.3171\n",
      "\t\t Validation(4) - Loss: 0.4084, Acc: 85.3821\n",
      "\t\t Training: Epoch(5) - Loss: 0.4402, Acc: 84.2973\n",
      "\t\t Validation(5) - Loss: 0.4942, Acc: 82.5166\n",
      "\t\t Training: Epoch(6) - Loss: 0.3931, Acc: 86.1528\n",
      "\t\t Validation(6) - Loss: 0.4525, Acc: 84.5930\n",
      "\t\t Training: Epoch(7) - Loss: 0.3894, Acc: 86.4297\n",
      "\t\t Validation(7) - Loss: 0.3773, Acc: 86.3787\n",
      "\t\t Training: Epoch(8) - Loss: 0.3516, Acc: 87.6806\n",
      "\t\t Validation(8) - Loss: 0.5538, Acc: 82.1013\n",
      "\t\t Training: Epoch(9) - Loss: 0.3403, Acc: 88.0822\n",
      "\t\t Validation(9) - Loss: 0.4992, Acc: 83.8455\n",
      "\t\t Training: Epoch(10) - Loss: 0.3185, Acc: 88.9684\n",
      "\t\t Validation(10) - Loss: 0.3281, Acc: 88.8289\n",
      "\t\t Training: Epoch(11) - Loss: 0.2861, Acc: 90.1639\n",
      "\t\t Validation(11) - Loss: 0.3220, Acc: 88.8704\n",
      "\t\t Training: Epoch(12) - Loss: 0.2859, Acc: 90.0808\n",
      "\t\t Validation(12) - Loss: 0.2664, Acc: 91.1960\n",
      "\t\t Training: Epoch(13) - Loss: 0.3053, Acc: 89.5223\n",
      "\t\t Validation(13) - Loss: 0.3282, Acc: 89.7425\n",
      "\t\t Training: Epoch(14) - Loss: 0.2680, Acc: 90.6254\n",
      "\t\t Validation(14) - Loss: 0.2982, Acc: 89.9502\n",
      "Finished.\n",
      "Total time per fold: 1311.3086173534393 seconds.\n",
      "Fold : 3\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 2.0343, Acc: 31.7779\n",
      "\t\t Validation(0) - Loss: 1.2544, Acc: 54.7570\n",
      "\t\t Training: Epoch(1) - Loss: 1.0504, Acc: 62.5265\n",
      "\t\t Validation(1) - Loss: 1.0621, Acc: 60.6980\n",
      "\t\t Training: Epoch(2) - Loss: 0.7870, Acc: 72.3207\n",
      "\t\t Validation(2) - Loss: 0.8454, Acc: 70.5027\n",
      "\t\t Training: Epoch(3) - Loss: 0.6577, Acc: 76.3408\n",
      "\t\t Validation(3) - Loss: 0.5077, Acc: 81.6369\n",
      "\t\t Training: Epoch(4) - Loss: 0.6100, Acc: 78.3347\n",
      "\t\t Validation(4) - Loss: 0.5768, Acc: 80.3490\n",
      "\t\t Training: Epoch(5) - Loss: 0.5296, Acc: 80.7717\n",
      "\t\t Validation(5) - Loss: 0.4744, Acc: 83.4649\n",
      "\t\t Training: Epoch(6) - Loss: 0.4706, Acc: 83.3518\n",
      "\t\t Validation(6) - Loss: 0.3402, Acc: 88.0349\n",
      "\t\t Training: Epoch(7) - Loss: 0.4499, Acc: 83.8364\n",
      "\t\t Validation(7) - Loss: 0.6510, Acc: 79.2688\n",
      "\t\t Training: Epoch(8) - Loss: 0.4754, Acc: 83.2779\n",
      "\t\t Validation(8) - Loss: 0.4625, Acc: 83.2987\n",
      "\t\t Training: Epoch(9) - Loss: 0.4066, Acc: 85.2442\n",
      "\t\t Validation(9) - Loss: 0.5614, Acc: 81.3876\n",
      "\t\t Training: Epoch(10) - Loss: 0.3712, Acc: 86.7765\n",
      "\t\t Validation(10) - Loss: 0.6992, Acc: 78.6456\n",
      "\t\t Training: Epoch(11) - Loss: 0.3709, Acc: 86.8042\n",
      "\t\t Validation(11) - Loss: 0.4383, Acc: 86.4146\n",
      "\t\t Training: Epoch(12) - Loss: 0.3578, Acc: 87.2011\n",
      "\t\t Validation(12) - Loss: 0.2958, Acc: 90.3614\n",
      "\t\t Training: Epoch(13) - Loss: 0.3419, Acc: 87.7458\n",
      "\t\t Validation(13) - Loss: 0.3352, Acc: 89.1982\n",
      "\t\t Training: Epoch(14) - Loss: 0.3243, Acc: 88.3643\n",
      "\t\t Validation(14) - Loss: 0.3096, Acc: 89.8214\n",
      "Finished.\n",
      "Total time per fold: 1305.4960508346558 seconds.\n",
      "Fold : 4\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 2.0290, Acc: 34.2564\n",
      "\t\t Validation(0) - Loss: 1.2111, Acc: 58.1637\n",
      "\t\t Training: Epoch(1) - Loss: 0.9735, Acc: 65.3282\n",
      "\t\t Validation(1) - Loss: 1.1706, Acc: 62.8999\n",
      "\t\t Training: Epoch(2) - Loss: 0.7205, Acc: 74.2361\n",
      "\t\t Validation(2) - Loss: 0.7091, Acc: 75.2804\n",
      "\t\t Training: Epoch(3) - Loss: 0.6138, Acc: 77.9424\n",
      "\t\t Validation(3) - Loss: 0.5893, Acc: 79.1442\n",
      "\t\t Training: Epoch(4) - Loss: 0.5819, Acc: 79.2763\n",
      "\t\t Validation(4) - Loss: 0.4883, Acc: 82.4678\n",
      "\t\t Training: Epoch(5) - Loss: 0.4991, Acc: 81.9948\n",
      "\t\t Validation(5) - Loss: 0.4971, Acc: 81.9693\n",
      "\t\t Training: Epoch(6) - Loss: 0.4466, Acc: 84.0257\n",
      "\t\t Validation(6) - Loss: 0.4450, Acc: 84.2543\n",
      "\t\t Training: Epoch(7) - Loss: 0.4147, Acc: 85.0918\n",
      "\t\t Validation(7) - Loss: 0.3772, Acc: 85.7084\n",
      "\t\t Training: Epoch(8) - Loss: 0.4196, Acc: 85.2626\n",
      "\t\t Validation(8) - Loss: 0.4661, Acc: 83.9219\n",
      "\t\t Training: Epoch(9) - Loss: 0.3901, Acc: 86.2227\n",
      "\t\t Validation(9) - Loss: 0.3398, Acc: 87.1209\n",
      "\t\t Training: Epoch(10) - Loss: 0.3527, Acc: 87.4319\n",
      "\t\t Validation(10) - Loss: 0.5174, Acc: 81.8031\n",
      "\t\t Training: Epoch(11) - Loss: 0.3467, Acc: 87.9258\n",
      "\t\t Validation(11) - Loss: 0.3478, Acc: 87.7856\n",
      "\t\t Training: Epoch(12) - Loss: 0.3512, Acc: 87.6996\n",
      "\t\t Validation(12) - Loss: 0.3503, Acc: 87.0794\n",
      "\t\t Training: Epoch(13) - Loss: 0.3149, Acc: 88.6920\n",
      "\t\t Validation(13) - Loss: 0.2993, Acc: 89.2397\n",
      "\t\t Training: Epoch(14) - Loss: 0.2634, Acc: 90.5797\n",
      "\t\t Validation(14) - Loss: 0.2872, Acc: 89.1566\n",
      "Finished.\n",
      "Total time per fold: 1305.3401663303375 seconds.\n",
      "Fold : 5\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 2.1126, Acc: 31.8564\n",
      "\t\t Validation(0) - Loss: 1.2803, Acc: 53.5521\n",
      "\t\t Training: Epoch(1) - Loss: 0.9936, Acc: 65.3882\n",
      "\t\t Validation(1) - Loss: 0.7263, Acc: 74.0341\n",
      "\t\t Training: Epoch(2) - Loss: 0.6892, Acc: 75.4685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Validation(2) - Loss: 0.5219, Acc: 80.7229\n",
      "\t\t Training: Epoch(3) - Loss: 0.6115, Acc: 78.2839\n",
      "\t\t Validation(3) - Loss: 0.5602, Acc: 80.4736\n",
      "\t\t Training: Epoch(4) - Loss: 0.5225, Acc: 81.2333\n",
      "\t\t Validation(4) - Loss: 0.6011, Acc: 78.8118\n",
      "\t\t Training: Epoch(5) - Loss: 0.5125, Acc: 81.8841\n",
      "\t\t Validation(5) - Loss: 0.4831, Acc: 83.0079\n",
      "\t\t Training: Epoch(6) - Loss: 0.4344, Acc: 84.2703\n",
      "\t\t Validation(6) - Loss: 0.6275, Acc: 78.2302\n",
      "\t\t Training: Epoch(7) - Loss: 0.4274, Acc: 84.9811\n",
      "\t\t Validation(7) - Loss: 0.4236, Acc: 84.7943\n",
      "\t\t Training: Epoch(8) - Loss: 0.3856, Acc: 86.1719\n",
      "\t\t Validation(8) - Loss: 0.3986, Acc: 86.2069\n",
      "\t\t Training: Epoch(9) - Loss: 0.3865, Acc: 86.4534\n",
      "\t\t Validation(9) - Loss: 0.3530, Acc: 87.7856\n",
      "\t\t Training: Epoch(10) - Loss: 0.3418, Acc: 88.0089\n",
      "\t\t Validation(10) - Loss: 0.3319, Acc: 88.2842\n",
      "\t\t Training: Epoch(11) - Loss: 0.3243, Acc: 88.4243\n",
      "\t\t Validation(11) - Loss: 0.3146, Acc: 89.4474\n",
      "\t\t Training: Epoch(12) - Loss: 0.3321, Acc: 88.5627\n",
      "\t\t Validation(12) - Loss: 0.3264, Acc: 88.6581\n",
      "\t\t Training: Epoch(13) - Loss: 0.3152, Acc: 89.0197\n",
      "\t\t Validation(13) - Loss: 0.3876, Acc: 86.6639\n",
      "\t\t Training: Epoch(14) - Loss: 0.2960, Acc: 89.5920\n",
      "\t\t Validation(14) - Loss: 0.3812, Acc: 86.2484\n",
      "Finished.\n",
      "Total time per fold: 1302.7708232402802 seconds.\n",
      "Fold : 6\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 2.2088, Acc: 28.7040\n",
      "\t\t Validation(0) - Loss: 1.3060, Acc: 50.0623\n",
      "\t\t Training: Epoch(1) - Loss: 1.1569, Acc: 57.9802\n",
      "\t\t Validation(1) - Loss: 0.8422, Acc: 68.6332\n",
      "\t\t Training: Epoch(2) - Loss: 0.8592, Acc: 69.0067\n",
      "\t\t Validation(2) - Loss: 0.7533, Acc: 73.7848\n",
      "\t\t Training: Epoch(3) - Loss: 0.7275, Acc: 74.6146\n",
      "\t\t Validation(3) - Loss: 0.6026, Acc: 77.6070\n",
      "\t\t Training: Epoch(4) - Loss: 0.6020, Acc: 78.4593\n",
      "\t\t Validation(4) - Loss: 0.5285, Acc: 80.3074\n",
      "\t\t Training: Epoch(5) - Loss: 0.5510, Acc: 80.1763\n",
      "\t\t Validation(5) - Loss: 0.5024, Acc: 82.2185\n",
      "\t\t Training: Epoch(6) - Loss: 0.4847, Acc: 82.4056\n",
      "\t\t Validation(6) - Loss: 0.4186, Acc: 84.1296\n",
      "\t\t Training: Epoch(7) - Loss: 0.4594, Acc: 83.4210\n",
      "\t\t Validation(7) - Loss: 0.4151, Acc: 84.8774\n",
      "\t\t Training: Epoch(8) - Loss: 0.4467, Acc: 84.2703\n",
      "\t\t Validation(8) - Loss: 0.3171, Acc: 89.4890\n",
      "\t\t Training: Epoch(9) - Loss: 0.4045, Acc: 85.5765\n",
      "\t\t Validation(9) - Loss: 0.3356, Acc: 88.8243\n",
      "\t\t Training: Epoch(10) - Loss: 0.3657, Acc: 86.9750\n",
      "\t\t Validation(10) - Loss: 0.7329, Acc: 79.6427\n",
      "\t\t Training: Epoch(11) - Loss: 0.3792, Acc: 86.5734\n",
      "\t\t Validation(11) - Loss: 0.2888, Acc: 90.1537\n",
      "\t\t Training: Epoch(12) - Loss: 0.3430, Acc: 87.9812\n",
      "\t\t Validation(12) - Loss: 0.3199, Acc: 89.3644\n",
      "\t\t Training: Epoch(13) - Loss: 0.3126, Acc: 89.0889\n",
      "\t\t Validation(13) - Loss: 0.2925, Acc: 89.7383\n",
      "\t\t Training: Epoch(14) - Loss: 0.3070, Acc: 89.3012\n",
      "\t\t Validation(14) - Loss: 0.3448, Acc: 87.4533\n",
      "Finished.\n",
      "Total time per fold: 1303.6002113819122 seconds.\n",
      "Fold : 7\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 2.1958, Acc: 30.3748\n",
      "\t\t Validation(0) - Loss: 1.3998, Acc: 51.2671\n",
      "\t\t Training: Epoch(1) - Loss: 0.9736, Acc: 66.0482\n",
      "\t\t Validation(1) - Loss: 0.8132, Acc: 71.2921\n",
      "\t\t Training: Epoch(2) - Loss: 0.6985, Acc: 75.0300\n",
      "\t\t Validation(2) - Loss: 0.7263, Acc: 76.2775\n",
      "\t\t Training: Epoch(3) - Loss: 0.6074, Acc: 78.1501\n",
      "\t\t Validation(3) - Loss: 0.5084, Acc: 81.1799\n",
      "\t\t Training: Epoch(4) - Loss: 0.5040, Acc: 81.9764\n",
      "\t\t Validation(4) - Loss: 0.5269, Acc: 81.4707\n",
      "\t\t Training: Epoch(5) - Loss: 0.4542, Acc: 83.4072\n",
      "\t\t Validation(5) - Loss: 0.4666, Acc: 83.0910\n",
      "\t\t Training: Epoch(6) - Loss: 0.4336, Acc: 84.5472\n",
      "\t\t Validation(6) - Loss: 0.4991, Acc: 81.3045\n",
      "\t\t Training: Epoch(7) - Loss: 0.4296, Acc: 84.8057\n",
      "\t\t Validation(7) - Loss: 0.4539, Acc: 83.4649\n",
      "\t\t Training: Epoch(8) - Loss: 0.3714, Acc: 86.7119\n",
      "\t\t Validation(8) - Loss: 0.3923, Acc: 86.9963\n",
      "\t\t Training: Epoch(9) - Loss: 0.3383, Acc: 88.1566\n",
      "\t\t Validation(9) - Loss: 0.3644, Acc: 86.4146\n",
      "\t\t Training: Epoch(10) - Loss: 0.3348, Acc: 88.1104\n",
      "\t\t Validation(10) - Loss: 0.3168, Acc: 89.5305\n",
      "\t\t Training: Epoch(11) - Loss: 0.3243, Acc: 88.5812\n",
      "\t\t Validation(11) - Loss: 0.2906, Acc: 89.8629\n",
      "\t\t Training: Epoch(12) - Loss: 0.3089, Acc: 88.8673\n",
      "\t\t Validation(12) - Loss: 0.3874, Acc: 87.8687\n",
      "\t\t Training: Epoch(13) - Loss: 0.2831, Acc: 89.8458\n",
      "\t\t Validation(13) - Loss: 0.3454, Acc: 88.0764\n",
      "\t\t Training: Epoch(14) - Loss: 0.2944, Acc: 89.7212\n",
      "\t\t Validation(14) - Loss: 0.2387, Acc: 91.1508\n",
      "Finished.\n",
      "Total time per fold: 1307.3497023582458 seconds.\n",
      "Fold : 8\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 1.9464, Acc: 35.2580\n",
      "\t\t Validation(0) - Loss: 1.1986, Acc: 59.4931\n",
      "\t\t Training: Epoch(1) - Loss: 0.9584, Acc: 66.0251\n",
      "\t\t Validation(1) - Loss: 1.0567, Acc: 66.3482\n",
      "\t\t Training: Epoch(2) - Loss: 0.6847, Acc: 75.2377\n",
      "\t\t Validation(2) - Loss: 0.7417, Acc: 73.9925\n",
      "\t\t Training: Epoch(3) - Loss: 0.5951, Acc: 78.4270\n",
      "\t\t Validation(3) - Loss: 0.5037, Acc: 80.5152\n",
      "\t\t Training: Epoch(4) - Loss: 0.5027, Acc: 81.6902\n",
      "\t\t Validation(4) - Loss: 0.4269, Acc: 83.7973\n",
      "\t\t Training: Epoch(5) - Loss: 0.4614, Acc: 83.0610\n",
      "\t\t Validation(5) - Loss: 0.5862, Acc: 80.0166\n",
      "\t\t Training: Epoch(6) - Loss: 0.4189, Acc: 84.6626\n",
      "\t\t Validation(6) - Loss: 0.4695, Acc: 82.7171\n",
      "\t\t Training: Epoch(7) - Loss: 0.4032, Acc: 85.4196\n",
      "\t\t Validation(7) - Loss: 0.3254, Acc: 87.3286\n",
      "\t\t Training: Epoch(8) - Loss: 0.3725, Acc: 86.8227\n",
      "\t\t Validation(8) - Loss: 0.3625, Acc: 87.1624\n",
      "\t\t Training: Epoch(9) - Loss: 0.3490, Acc: 87.3119\n",
      "\t\t Validation(9) - Loss: 0.3357, Acc: 88.1595\n",
      "\t\t Training: Epoch(10) - Loss: 0.3424, Acc: 87.8935\n",
      "\t\t Validation(10) - Loss: 0.2752, Acc: 90.7354\n",
      "\t\t Training: Epoch(11) - Loss: 0.3024, Acc: 89.1997\n",
      "\t\t Validation(11) - Loss: 0.4145, Acc: 85.4175\n",
      "\t\t Training: Epoch(12) - Loss: 0.2845, Acc: 89.8874\n",
      "\t\t Validation(12) - Loss: 0.2777, Acc: 90.7354\n",
      "\t\t Training: Epoch(13) - Loss: 0.2885, Acc: 89.8920\n",
      "\t\t Validation(13) - Loss: 0.3169, Acc: 88.1595\n",
      "\t\t Training: Epoch(14) - Loss: 0.2852, Acc: 90.0166\n",
      "\t\t Validation(14) - Loss: 0.2623, Acc: 91.2754\n",
      "Finished.\n",
      "Total time per fold: 1304.1950392723083 seconds.\n",
      "Fold : 9\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 1.9745, Acc: 35.5903\n",
      "\t\t Validation(0) - Loss: 1.2008, Acc: 54.2584\n",
      "\t\t Training: Epoch(1) - Loss: 0.9415, Acc: 66.6713\n",
      "\t\t Validation(1) - Loss: 0.7311, Acc: 75.4466\n",
      "\t\t Training: Epoch(2) - Loss: 0.7182, Acc: 74.8500\n",
      "\t\t Validation(2) - Loss: 0.6724, Acc: 75.8621\n",
      "\t\t Training: Epoch(3) - Loss: 0.5688, Acc: 79.8209\n",
      "\t\t Validation(3) - Loss: 0.6448, Acc: 77.9809\n",
      "\t\t Training: Epoch(4) - Loss: 0.5377, Acc: 80.8686\n",
      "\t\t Validation(4) - Loss: 0.8550, Acc: 71.9152\n",
      "\t\t Training: Epoch(5) - Loss: 0.5033, Acc: 81.5564\n",
      "\t\t Validation(5) - Loss: 0.4877, Acc: 81.2214\n",
      "\t\t Training: Epoch(6) - Loss: 0.4498, Acc: 83.8641\n",
      "\t\t Validation(6) - Loss: 0.5501, Acc: 80.1828\n",
      "\t\t Training: Epoch(7) - Loss: 0.4199, Acc: 85.0826\n",
      "\t\t Validation(7) - Loss: 0.3975, Acc: 85.7914\n",
      "\t\t Training: Epoch(8) - Loss: 0.3859, Acc: 86.0473\n",
      "\t\t Validation(8) - Loss: 0.3172, Acc: 87.9103\n",
      "\t\t Training: Epoch(9) - Loss: 0.3606, Acc: 87.1042\n",
      "\t\t Validation(9) - Loss: 0.5086, Acc: 83.3818\n",
      "\t\t Training: Epoch(10) - Loss: 0.3571, Acc: 87.3165\n",
      "\t\t Validation(10) - Loss: 0.4780, Acc: 84.5035\n",
      "\t\t Training: Epoch(11) - Loss: 0.3271, Acc: 88.2304\n",
      "\t\t Validation(11) - Loss: 0.3498, Acc: 88.1180\n",
      "\t\t Training: Epoch(12) - Loss: 0.3326, Acc: 88.1658\n",
      "\t\t Validation(12) - Loss: 0.2345, Acc: 91.4416\n",
      "\t\t Training: Epoch(13) - Loss: 0.3006, Acc: 89.2366\n",
      "\t\t Validation(13) - Loss: 0.3427, Acc: 87.7441\n",
      "\t\t Training: Epoch(14) - Loss: 0.3079, Acc: 89.0612\n",
      "\t\t Validation(14) - Loss: 0.2450, Acc: 91.5247\n",
      "Finished.\n",
      "Total time per fold: 1345.5973465442657 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Variables to store fold scores\n",
    "train_acc = []\n",
    "test_top1_acc = []\n",
    "test_top5_acc = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "times = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(splits.split(total_set)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('Fold : {}'.format(fold))\n",
    "    \n",
    "    # Train and val samplers\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    print(\"Samples in training:\", len(train_sampler))\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    print(\"Samples in test:\", len(valid_sampler))\n",
    "    \n",
    "    # Train and val loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=train_batch_size, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=1, sampler=valid_sampler)\n",
    "    \n",
    "    device = get_device()\n",
    "    \n",
    "    criterion, model, optimizer = create_optimizer(load_model())\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(h_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        trunning_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += (preds == labels).sum()\n",
    "            trunning_corrects += preds.size(0)\n",
    "            \n",
    "\n",
    "        epoch_loss = running_loss / trunning_corrects\n",
    "        epoch_acc = (running_corrects.double()*100) / trunning_corrects\n",
    "        train_acc.append(epoch_acc.item())\n",
    "        \n",
    "        print('\\t\\t Training: Epoch({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        model.eval()  \n",
    "        \n",
    "        vrunning_loss = 0.0\n",
    "        vrunning_corrects = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for data, labels in valid_loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(data)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            vrunning_loss += loss.item() * data.size(0)\n",
    "            vrunning_corrects += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            \n",
    "        vepoch_loss = vrunning_loss/num_samples\n",
    "        vepoch_acc = (vrunning_corrects.double() * 100)/num_samples\n",
    "        \n",
    "        print('\\t\\t Validation({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, vepoch_loss, vepoch_acc))\n",
    "    \n",
    "    # Calculating and appending scores to this fold\n",
    "    model.class_to_idx = total_set.class_to_idx\n",
    "    scores = get_scores(model, valid_loader)\n",
    "    \n",
    "    test_top1_acc.append(scores[0])\n",
    "    test_top5_acc.append(scores[1])\n",
    "    test_precision.append(scores[2])\n",
    "    test_recall.append(scores[3])\n",
    "    test_f1.append(scores[4])\n",
    "    \n",
    "    time_fold = time.time() - start_time\n",
    "    times.append(time_fold)\n",
    "    print(\"Total time per fold: %s seconds.\" %(time_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy average:  0.7977956661941211\n",
      "Top-1 test accuracy average:  0.8882170393108693\n",
      "Top-5 test accuracy average:  0.9937275623285904\n",
      "Weighted Precision test accuracy average:  0.9007732803668924\n",
      "Weighted Recall test accuracy average:  0.8882170393108693\n",
      "Weighted F1 test accuracy average:  0.8866458748355202\n",
      "Average time per fold (seconds): 1356.052500128746\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy average: \", np.mean(train_acc) / 100)\n",
    "print(\"Top-1 test accuracy average: \", np.mean(test_top1_acc))\n",
    "print(\"Top-5 test accuracy average: \", np.mean(test_top5_acc))\n",
    "print(\"Weighted Precision test accuracy average: \", np.mean(test_precision))\n",
    "print(\"Weighted Recall test accuracy average: \", np.mean(test_recall))\n",
    "print(\"Weighted F1 test accuracy average: \", np.mean(test_f1))\n",
    "print(\"Average time per fold (seconds):\", np.mean(times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
