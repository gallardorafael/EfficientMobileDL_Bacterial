{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the paper: \n",
    "\n",
    "## Efficient and Mobile Deep Learning Architectures for Fast Identification of BacterialStrains in Resource-Constrained Devices\n",
    "\n",
    "### Architecture: ShuffleNet v2x2.0\n",
    "### Data: Original + Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, models\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "# Archs not in Pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# External functions\n",
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "3.8.5 (default, Jul 28 2020, 12:59:40) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data paths and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters and dataset details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset details\n",
    "dataset_version = 'original' # original or augmented\n",
    "img_shape = (224,224)\n",
    "img_size = str(img_shape[0])+\"x\"+str(img_shape[1])\n",
    "\n",
    "# Root directory of dataset\n",
    "data_dir = '/home/yibbtstll/venvs/pytorch_gpu/CySDeepBacterial/Dataset/DIBaS/'\n",
    "\n",
    "train_batch_size = 32\n",
    "val_test_batch_size = 32\n",
    "feature_extract = False\n",
    "pretrained = False\n",
    "h_epochs = 20\n",
    "kfolds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining transforms and creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for input data\n",
    "training_transforms = transforms.Compose([transforms.Resize((224,224), Image.LANCZOS),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "total_set = datasets.ImageFolder(data_dir, transform=training_transforms)\n",
    "\n",
    "# Defining folds\n",
    "splits = KFold(n_splits = kfolds, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the target classes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "{0: 'Acinetobacter.baumanii', 1: 'Actinomyces.israeli', 2: 'Bacteroides.fragilis', 3: 'Bifidobacterium.spp', 4: 'Clostridium.perfringens', 5: 'Enterococcus.faecalis', 6: 'Enterococcus.faecium', 7: 'Escherichia.coli', 8: 'Fusobacterium', 9: 'Lactobacillus.casei', 10: 'Lactobacillus.crispatus', 11: 'Lactobacillus.delbrueckii', 12: 'Lactobacillus.gasseri', 13: 'Lactobacillus.jehnsenii', 14: 'Lactobacillus.johnsonii', 15: 'Lactobacillus.paracasei', 16: 'Lactobacillus.plantarum', 17: 'Lactobacillus.reuteri', 18: 'Lactobacillus.rhamnosus', 19: 'Lactobacillus.salivarius', 20: 'Listeria.monocytogenes', 21: 'Micrococcus.spp', 22: 'Neisseria.gonorrhoeae', 23: 'Porfyromonas.gingivalis', 24: 'Propionibacterium.acnes', 25: 'Proteus', 26: 'Pseudomonas.aeruginosa', 27: 'Staphylococcus.aureus', 28: 'Staphylococcus.epidermidis', 29: 'Staphylococcus.saprophiticus', 30: 'Streptococcus.agalactiae', 31: 'Veionella'}\n"
     ]
    }
   ],
   "source": [
    "train_labels = {value : key for (key, value) in total_set.class_to_idx.items()}\n",
    "    \n",
    "print(len(train_labels)) \n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and inicialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing pre-trained parameters, finetunning the classifier to output 32 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained model parameters to avoid backpropogating through them\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        print(\"Setting grad to false.\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_device():\n",
    "    # Model and criterion to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShuffleNetV2(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (stage2): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(24, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(122, 122, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=122, bias=False)\n",
       "        (4): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(122, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=122, bias=False)\n",
       "        (4): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(122, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=122, bias=False)\n",
       "        (4): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(122, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=122, bias=False)\n",
       "        (4): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(122, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage3): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=244, bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(244, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=244, bias=False)\n",
       "        (4): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(244, 244, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage4): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(488, 488, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=488, bias=False)\n",
       "        (1): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(488, 488, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=488, bias=False)\n",
       "        (4): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(488, 488, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=488, bias=False)\n",
       "        (4): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(488, 488, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=488, bias=False)\n",
       "        (4): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(488, 488, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=488, bias=False)\n",
       "        (4): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(488, 488, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(976, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.shufflenet_v2_x2_0(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Transfer Learning\n",
    "    model = models.shufflenet_v2_x2_0(pretrained=pretrained)\n",
    "    \n",
    "    # Mode\n",
    "    model = set_parameter_requires_grad(model, feature_extract)\n",
    "    \n",
    "    # Fine tuning\n",
    "    # Build custom classifier\n",
    "    model.fc = nn.Linear(in_features=2048,\n",
    "                        out_features=32)\n",
    "    return model\n",
    "\n",
    "def create_optimizer(model):\n",
    "    # Parameters to update\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "\n",
    "    else:\n",
    "        n_params = 0\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                n_params += 1\n",
    "\n",
    "\n",
    "    # Loss function and gradient descent\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(params_to_update, \n",
    "                          lr=0.001, \n",
    "                          weight_decay=0.000004)\n",
    "    \n",
    "    return criterion.to(get_device()), model.to(get_device()), optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 2.9682, Acc: 13.1229\n",
      "\t\t Validation(0) - Loss: 6.3180, Acc: 0.0000\n",
      "\t\t Training: Epoch(1) - Loss: 1.9329, Acc: 33.2226\n",
      "\t\t Validation(1) - Loss: 6.4761, Acc: 7.4627\n",
      "\t\t Training: Epoch(2) - Loss: 1.5138, Acc: 45.8472\n",
      "\t\t Validation(2) - Loss: 2.8015, Acc: 28.3582\n",
      "\t\t Training: Epoch(3) - Loss: 1.2017, Acc: 57.9734\n",
      "\t\t Validation(3) - Loss: 2.0443, Acc: 43.2836\n",
      "\t\t Training: Epoch(4) - Loss: 1.0348, Acc: 63.9535\n",
      "\t\t Validation(4) - Loss: 2.4908, Acc: 31.3433\n",
      "\t\t Training: Epoch(5) - Loss: 0.9762, Acc: 66.4452\n",
      "\t\t Validation(5) - Loss: 2.3048, Acc: 46.2687\n",
      "\t\t Training: Epoch(6) - Loss: 0.8292, Acc: 69.6013\n",
      "\t\t Validation(6) - Loss: 1.3679, Acc: 55.2239\n",
      "\t\t Training: Epoch(7) - Loss: 0.5910, Acc: 78.2392\n",
      "\t\t Validation(7) - Loss: 0.9541, Acc: 70.1493\n",
      "\t\t Training: Epoch(8) - Loss: 0.5966, Acc: 78.4053\n",
      "\t\t Validation(8) - Loss: 1.7719, Acc: 53.7313\n",
      "\t\t Training: Epoch(9) - Loss: 0.5049, Acc: 82.8904\n",
      "\t\t Validation(9) - Loss: 1.8910, Acc: 56.7164\n",
      "\t\t Training: Epoch(10) - Loss: 0.3936, Acc: 86.7110\n",
      "\t\t Validation(10) - Loss: 1.3581, Acc: 67.1642\n",
      "\t\t Training: Epoch(11) - Loss: 0.4106, Acc: 86.5449\n",
      "\t\t Validation(11) - Loss: 1.6914, Acc: 70.1493\n",
      "\t\t Training: Epoch(12) - Loss: 0.3721, Acc: 86.8771\n",
      "\t\t Validation(12) - Loss: 1.6902, Acc: 50.7463\n",
      "\t\t Training: Epoch(13) - Loss: 0.3641, Acc: 88.7043\n",
      "\t\t Validation(13) - Loss: 0.6844, Acc: 73.1343\n",
      "\t\t Training: Epoch(14) - Loss: 0.4962, Acc: 83.2226\n",
      "\t\t Validation(14) - Loss: 1.1707, Acc: 62.6866\n",
      "\t\t Training: Epoch(15) - Loss: 0.2865, Acc: 90.6977\n",
      "\t\t Validation(15) - Loss: 1.7557, Acc: 46.2687\n",
      "\t\t Training: Epoch(16) - Loss: 0.2734, Acc: 91.3621\n",
      "\t\t Validation(16) - Loss: 1.3683, Acc: 52.2388\n",
      "\t\t Training: Epoch(17) - Loss: 0.2974, Acc: 90.0332\n",
      "\t\t Validation(17) - Loss: 2.0745, Acc: 50.7463\n",
      "\t\t Training: Epoch(18) - Loss: 0.3401, Acc: 90.3654\n",
      "\t\t Validation(18) - Loss: 2.0808, Acc: 49.2537\n",
      "\t\t Training: Epoch(19) - Loss: 0.4107, Acc: 86.8771\n",
      "\t\t Validation(19) - Loss: 0.9636, Acc: 62.6866\n",
      "Finished.\n",
      "Total time per fold: 648.1738555431366 seconds.\n",
      "Fold : 1\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.0274, Acc: 14.9502\n",
      "\t\t Validation(0) - Loss: 5.0923, Acc: 8.9552\n",
      "\t\t Training: Epoch(1) - Loss: 1.9135, Acc: 36.3787\n",
      "\t\t Validation(1) - Loss: 9.1011, Acc: 7.4627\n",
      "\t\t Training: Epoch(2) - Loss: 1.6835, Acc: 40.3654\n",
      "\t\t Validation(2) - Loss: 2.1439, Acc: 35.8209\n",
      "\t\t Training: Epoch(3) - Loss: 1.4153, Acc: 49.8339\n",
      "\t\t Validation(3) - Loss: 2.4396, Acc: 32.8358\n",
      "\t\t Training: Epoch(4) - Loss: 1.0078, Acc: 65.4485\n",
      "\t\t Validation(4) - Loss: 1.2095, Acc: 52.2388\n",
      "\t\t Training: Epoch(5) - Loss: 0.8657, Acc: 67.4419\n",
      "\t\t Validation(5) - Loss: 1.2757, Acc: 70.1493\n",
      "\t\t Training: Epoch(6) - Loss: 0.8435, Acc: 68.7708\n",
      "\t\t Validation(6) - Loss: 1.3667, Acc: 56.7164\n",
      "\t\t Training: Epoch(7) - Loss: 0.7692, Acc: 72.2591\n",
      "\t\t Validation(7) - Loss: 1.3570, Acc: 61.1940\n",
      "\t\t Training: Epoch(8) - Loss: 0.5300, Acc: 82.3920\n",
      "\t\t Validation(8) - Loss: 2.1949, Acc: 46.2687\n",
      "\t\t Training: Epoch(9) - Loss: 0.4987, Acc: 82.5581\n",
      "\t\t Validation(9) - Loss: 0.8627, Acc: 71.6418\n",
      "\t\t Training: Epoch(10) - Loss: 0.3533, Acc: 87.2093\n",
      "\t\t Validation(10) - Loss: 0.8854, Acc: 67.1642\n",
      "\t\t Training: Epoch(11) - Loss: 0.3403, Acc: 88.2060\n",
      "\t\t Validation(11) - Loss: 0.4400, Acc: 82.0896\n",
      "\t\t Training: Epoch(12) - Loss: 0.3896, Acc: 85.5482\n",
      "\t\t Validation(12) - Loss: 1.5927, Acc: 59.7015\n",
      "\t\t Training: Epoch(13) - Loss: 0.4535, Acc: 85.2159\n",
      "\t\t Validation(13) - Loss: 3.1205, Acc: 37.3134\n",
      "\t\t Training: Epoch(14) - Loss: 0.4996, Acc: 83.2226\n",
      "\t\t Validation(14) - Loss: 1.1717, Acc: 71.6418\n",
      "\t\t Training: Epoch(15) - Loss: 0.3242, Acc: 89.2027\n",
      "\t\t Validation(15) - Loss: 0.5635, Acc: 77.6119\n",
      "\t\t Training: Epoch(16) - Loss: 0.3272, Acc: 89.0365\n",
      "\t\t Validation(16) - Loss: 0.6254, Acc: 73.1343\n",
      "\t\t Training: Epoch(17) - Loss: 0.2878, Acc: 90.3654\n",
      "\t\t Validation(17) - Loss: 0.5312, Acc: 83.5821\n",
      "\t\t Training: Epoch(18) - Loss: 0.1944, Acc: 94.3522\n",
      "\t\t Validation(18) - Loss: 0.5512, Acc: 82.0896\n",
      "\t\t Training: Epoch(19) - Loss: 0.2094, Acc: 93.8538\n",
      "\t\t Validation(19) - Loss: 0.5464, Acc: 82.0896\n",
      "Finished.\n",
      "Total time per fold: 468.168461561203 seconds.\n",
      "Fold : 2\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 2.9985, Acc: 13.1229\n",
      "\t\t Validation(0) - Loss: 5.9419, Acc: 1.4925\n",
      "\t\t Training: Epoch(1) - Loss: 1.7569, Acc: 37.8738\n",
      "\t\t Validation(1) - Loss: 8.8093, Acc: 5.9701\n",
      "\t\t Training: Epoch(2) - Loss: 1.5571, Acc: 49.1694\n",
      "\t\t Validation(2) - Loss: 2.6673, Acc: 34.3284\n",
      "\t\t Training: Epoch(3) - Loss: 1.3911, Acc: 50.9967\n",
      "\t\t Validation(3) - Loss: 1.6370, Acc: 49.2537\n",
      "\t\t Training: Epoch(4) - Loss: 1.0328, Acc: 61.2957\n",
      "\t\t Validation(4) - Loss: 1.8580, Acc: 44.7761\n",
      "\t\t Training: Epoch(5) - Loss: 0.8616, Acc: 67.1096\n",
      "\t\t Validation(5) - Loss: 1.7140, Acc: 44.7761\n",
      "\t\t Training: Epoch(6) - Loss: 0.8479, Acc: 71.9269\n",
      "\t\t Validation(6) - Loss: 1.2602, Acc: 67.1642\n",
      "\t\t Training: Epoch(7) - Loss: 0.8503, Acc: 68.1063\n",
      "\t\t Validation(7) - Loss: 1.7069, Acc: 52.2388\n",
      "\t\t Training: Epoch(8) - Loss: 0.6868, Acc: 73.5880\n",
      "\t\t Validation(8) - Loss: 1.3124, Acc: 59.7015\n",
      "\t\t Training: Epoch(9) - Loss: 0.5722, Acc: 79.2359\n",
      "\t\t Validation(9) - Loss: 2.9304, Acc: 34.3284\n",
      "\t\t Training: Epoch(10) - Loss: 0.4115, Acc: 87.3754\n",
      "\t\t Validation(10) - Loss: 1.7235, Acc: 50.7463\n",
      "\t\t Training: Epoch(11) - Loss: 0.5204, Acc: 81.2292\n",
      "\t\t Validation(11) - Loss: 2.0672, Acc: 50.7463\n",
      "\t\t Training: Epoch(12) - Loss: 0.4043, Acc: 85.8804\n",
      "\t\t Validation(12) - Loss: 1.3309, Acc: 64.1791\n",
      "\t\t Training: Epoch(13) - Loss: 0.3380, Acc: 89.3688\n",
      "\t\t Validation(13) - Loss: 1.2914, Acc: 68.6567\n",
      "\t\t Training: Epoch(14) - Loss: 0.5446, Acc: 84.3854\n",
      "\t\t Validation(14) - Loss: 1.4708, Acc: 61.1940\n",
      "\t\t Training: Epoch(15) - Loss: 0.3140, Acc: 89.3688\n",
      "\t\t Validation(15) - Loss: 1.5546, Acc: 61.1940\n",
      "\t\t Training: Epoch(16) - Loss: 0.2751, Acc: 91.0299\n",
      "\t\t Validation(16) - Loss: 1.1380, Acc: 70.1493\n",
      "\t\t Training: Epoch(17) - Loss: 0.2802, Acc: 90.8638\n",
      "\t\t Validation(17) - Loss: 1.4431, Acc: 64.1791\n",
      "\t\t Training: Epoch(18) - Loss: 0.2348, Acc: 92.6910\n",
      "\t\t Validation(18) - Loss: 1.5623, Acc: 59.7015\n",
      "\t\t Training: Epoch(19) - Loss: 0.2451, Acc: 92.6910\n",
      "\t\t Validation(19) - Loss: 1.0578, Acc: 70.1493\n",
      "Finished.\n",
      "Total time per fold: 474.27802538871765 seconds.\n",
      "Fold : 3\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.0195, Acc: 9.6346\n",
      "\t\t Validation(0) - Loss: 5.1111, Acc: 2.9851\n",
      "\t\t Training: Epoch(1) - Loss: 2.0133, Acc: 30.5648\n",
      "\t\t Validation(1) - Loss: 6.5286, Acc: 11.9403\n",
      "\t\t Training: Epoch(2) - Loss: 1.6349, Acc: 40.8638\n",
      "\t\t Validation(2) - Loss: 3.1186, Acc: 16.4179\n",
      "\t\t Training: Epoch(3) - Loss: 1.3346, Acc: 51.6611\n",
      "\t\t Validation(3) - Loss: 1.9348, Acc: 43.2836\n",
      "\t\t Training: Epoch(4) - Loss: 1.1036, Acc: 57.4751\n",
      "\t\t Validation(4) - Loss: 1.6674, Acc: 49.2537\n",
      "\t\t Training: Epoch(5) - Loss: 1.0229, Acc: 61.6279\n",
      "\t\t Validation(5) - Loss: 1.5592, Acc: 50.7463\n",
      "\t\t Training: Epoch(6) - Loss: 0.7962, Acc: 70.5980\n",
      "\t\t Validation(6) - Loss: 1.4139, Acc: 52.2388\n",
      "\t\t Training: Epoch(7) - Loss: 0.7574, Acc: 71.0963\n",
      "\t\t Validation(7) - Loss: 1.5193, Acc: 56.7164\n",
      "\t\t Training: Epoch(8) - Loss: 0.8421, Acc: 73.9203\n",
      "\t\t Validation(8) - Loss: 2.2471, Acc: 37.3134\n",
      "\t\t Training: Epoch(9) - Loss: 0.6896, Acc: 76.7442\n",
      "\t\t Validation(9) - Loss: 1.2188, Acc: 64.1791\n",
      "\t\t Training: Epoch(10) - Loss: 0.5432, Acc: 81.2292\n",
      "\t\t Validation(10) - Loss: 1.0744, Acc: 70.1493\n",
      "\t\t Training: Epoch(11) - Loss: 0.4864, Acc: 82.0598\n",
      "\t\t Validation(11) - Loss: 1.1752, Acc: 67.1642\n",
      "\t\t Training: Epoch(12) - Loss: 0.3975, Acc: 85.3821\n",
      "\t\t Validation(12) - Loss: 1.3905, Acc: 58.2090\n",
      "\t\t Training: Epoch(13) - Loss: 0.3485, Acc: 88.8704\n",
      "\t\t Validation(13) - Loss: 1.7897, Acc: 49.2537\n",
      "\t\t Training: Epoch(14) - Loss: 0.3342, Acc: 87.8738\n",
      "\t\t Validation(14) - Loss: 1.3014, Acc: 67.1642\n",
      "\t\t Training: Epoch(15) - Loss: 0.2284, Acc: 91.6944\n",
      "\t\t Validation(15) - Loss: 1.8684, Acc: 49.2537\n",
      "\t\t Training: Epoch(16) - Loss: 0.1855, Acc: 93.8538\n",
      "\t\t Validation(16) - Loss: 1.1428, Acc: 65.6716\n",
      "\t\t Training: Epoch(17) - Loss: 0.2228, Acc: 92.8571\n",
      "\t\t Validation(17) - Loss: 1.0130, Acc: 76.1194\n",
      "\t\t Training: Epoch(18) - Loss: 0.2325, Acc: 91.5282\n",
      "\t\t Validation(18) - Loss: 1.3547, Acc: 67.1642\n",
      "\t\t Training: Epoch(19) - Loss: 0.3226, Acc: 88.2060\n",
      "\t\t Validation(19) - Loss: 2.1410, Acc: 49.2537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Total time per fold: 481.891508102417 seconds.\n",
      "Fold : 4\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.0693, Acc: 12.7907\n",
      "\t\t Validation(0) - Loss: 4.9436, Acc: 2.9851\n",
      "\t\t Training: Epoch(1) - Loss: 2.0505, Acc: 30.0664\n",
      "\t\t Validation(1) - Loss: 5.8426, Acc: 17.9104\n",
      "\t\t Training: Epoch(2) - Loss: 1.6243, Acc: 42.3588\n",
      "\t\t Validation(2) - Loss: 3.1115, Acc: 31.3433\n",
      "\t\t Training: Epoch(3) - Loss: 1.1952, Acc: 57.9734\n",
      "\t\t Validation(3) - Loss: 2.1787, Acc: 35.8209\n",
      "\t\t Training: Epoch(4) - Loss: 1.0281, Acc: 60.6312\n",
      "\t\t Validation(4) - Loss: 1.1900, Acc: 55.2239\n",
      "\t\t Training: Epoch(5) - Loss: 1.0083, Acc: 64.4518\n",
      "\t\t Validation(5) - Loss: 1.5139, Acc: 50.7463\n",
      "\t\t Training: Epoch(6) - Loss: 0.8477, Acc: 69.9336\n",
      "\t\t Validation(6) - Loss: 1.2057, Acc: 55.2239\n",
      "\t\t Training: Epoch(7) - Loss: 0.6889, Acc: 74.5847\n",
      "\t\t Validation(7) - Loss: 2.4596, Acc: 29.8507\n",
      "\t\t Training: Epoch(8) - Loss: 0.5961, Acc: 77.7409\n",
      "\t\t Validation(8) - Loss: 2.1258, Acc: 50.7463\n",
      "\t\t Training: Epoch(9) - Loss: 0.5490, Acc: 78.9037\n",
      "\t\t Validation(9) - Loss: 0.6737, Acc: 74.6269\n",
      "\t\t Training: Epoch(10) - Loss: 0.5569, Acc: 78.2392\n",
      "\t\t Validation(10) - Loss: 1.9576, Acc: 53.7313\n",
      "\t\t Training: Epoch(11) - Loss: 0.7358, Acc: 75.2492\n",
      "\t\t Validation(11) - Loss: 2.6288, Acc: 47.7612\n",
      "\t\t Training: Epoch(12) - Loss: 0.4567, Acc: 84.2193\n",
      "\t\t Validation(12) - Loss: 1.8656, Acc: 47.7612\n",
      "\t\t Training: Epoch(13) - Loss: 0.4813, Acc: 81.8937\n",
      "\t\t Validation(13) - Loss: 1.3427, Acc: 53.7313\n",
      "\t\t Training: Epoch(14) - Loss: 0.3880, Acc: 87.0432\n",
      "\t\t Validation(14) - Loss: 1.6719, Acc: 65.6716\n",
      "\t\t Training: Epoch(15) - Loss: 0.3605, Acc: 87.5415\n",
      "\t\t Validation(15) - Loss: 1.4288, Acc: 61.1940\n",
      "\t\t Training: Epoch(16) - Loss: 0.2813, Acc: 89.8671\n",
      "\t\t Validation(16) - Loss: 1.6817, Acc: 53.7313\n",
      "\t\t Training: Epoch(17) - Loss: 0.2913, Acc: 90.5316\n",
      "\t\t Validation(17) - Loss: 2.1108, Acc: 52.2388\n",
      "\t\t Training: Epoch(18) - Loss: 0.3344, Acc: 89.7010\n",
      "\t\t Validation(18) - Loss: 2.8220, Acc: 46.2687\n",
      "\t\t Training: Epoch(19) - Loss: 0.2305, Acc: 92.6910\n",
      "\t\t Validation(19) - Loss: 1.2658, Acc: 59.7015\n",
      "Finished.\n",
      "Total time per fold: 467.5514235496521 seconds.\n",
      "Fold : 5\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 2.9964, Acc: 14.1196\n",
      "\t\t Validation(0) - Loss: 6.9696, Acc: 5.9701\n",
      "\t\t Training: Epoch(1) - Loss: 1.9535, Acc: 30.8970\n",
      "\t\t Validation(1) - Loss: 7.3717, Acc: 10.4478\n",
      "\t\t Training: Epoch(2) - Loss: 1.4881, Acc: 44.0199\n",
      "\t\t Validation(2) - Loss: 2.1644, Acc: 35.8209\n",
      "\t\t Training: Epoch(3) - Loss: 1.2778, Acc: 52.1595\n",
      "\t\t Validation(3) - Loss: 1.8773, Acc: 52.2388\n",
      "\t\t Training: Epoch(4) - Loss: 1.4361, Acc: 50.8306\n",
      "\t\t Validation(4) - Loss: 1.7891, Acc: 40.2985\n",
      "\t\t Training: Epoch(5) - Loss: 1.2785, Acc: 55.8140\n",
      "\t\t Validation(5) - Loss: 1.6527, Acc: 50.7463\n",
      "\t\t Training: Epoch(6) - Loss: 0.9947, Acc: 64.6179\n",
      "\t\t Validation(6) - Loss: 1.1116, Acc: 64.1791\n",
      "\t\t Training: Epoch(7) - Loss: 0.7724, Acc: 73.0897\n",
      "\t\t Validation(7) - Loss: 1.2299, Acc: 59.7015\n",
      "\t\t Training: Epoch(8) - Loss: 0.5565, Acc: 81.7276\n",
      "\t\t Validation(8) - Loss: 1.6227, Acc: 55.2239\n",
      "\t\t Training: Epoch(9) - Loss: 0.6952, Acc: 76.4120\n",
      "\t\t Validation(9) - Loss: 1.3606, Acc: 61.1940\n",
      "\t\t Training: Epoch(10) - Loss: 0.4590, Acc: 86.0465\n",
      "\t\t Validation(10) - Loss: 0.9641, Acc: 61.1940\n",
      "\t\t Training: Epoch(11) - Loss: 0.5827, Acc: 78.9037\n",
      "\t\t Validation(11) - Loss: 1.1207, Acc: 67.1642\n",
      "\t\t Training: Epoch(12) - Loss: 0.4070, Acc: 85.3821\n",
      "\t\t Validation(12) - Loss: 0.9571, Acc: 68.6567\n",
      "\t\t Training: Epoch(13) - Loss: 0.3140, Acc: 89.7010\n",
      "\t\t Validation(13) - Loss: 1.4479, Acc: 62.6866\n",
      "\t\t Training: Epoch(14) - Loss: 0.3231, Acc: 88.0399\n",
      "\t\t Validation(14) - Loss: 0.7643, Acc: 76.1194\n",
      "\t\t Training: Epoch(15) - Loss: 0.2998, Acc: 89.8671\n",
      "\t\t Validation(15) - Loss: 1.0989, Acc: 67.1642\n",
      "\t\t Training: Epoch(16) - Loss: 0.3919, Acc: 88.5382\n",
      "\t\t Validation(16) - Loss: 2.0345, Acc: 56.7164\n",
      "\t\t Training: Epoch(17) - Loss: 0.4879, Acc: 83.8870\n",
      "\t\t Validation(17) - Loss: 1.1430, Acc: 65.6716\n",
      "\t\t Training: Epoch(18) - Loss: 0.2890, Acc: 90.1993\n",
      "\t\t Validation(18) - Loss: 0.6878, Acc: 76.1194\n",
      "\t\t Training: Epoch(19) - Loss: 0.2404, Acc: 92.3588\n",
      "\t\t Validation(19) - Loss: 1.2799, Acc: 59.7015\n",
      "Finished.\n",
      "Total time per fold: 451.69758772850037 seconds.\n",
      "Fold : 6\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.1103, Acc: 10.7973\n",
      "\t\t Validation(0) - Loss: 6.9399, Acc: 1.4925\n",
      "\t\t Training: Epoch(1) - Loss: 1.8273, Acc: 38.2060\n",
      "\t\t Validation(1) - Loss: 9.2776, Acc: 2.9851\n",
      "\t\t Training: Epoch(2) - Loss: 1.2922, Acc: 52.1595\n",
      "\t\t Validation(2) - Loss: 3.3449, Acc: 32.8358\n",
      "\t\t Training: Epoch(3) - Loss: 1.1415, Acc: 60.7973\n",
      "\t\t Validation(3) - Loss: 2.0530, Acc: 43.2836\n",
      "\t\t Training: Epoch(4) - Loss: 0.9546, Acc: 67.2757\n",
      "\t\t Validation(4) - Loss: 2.6216, Acc: 43.2836\n",
      "\t\t Training: Epoch(5) - Loss: 0.8712, Acc: 69.2691\n",
      "\t\t Validation(5) - Loss: 1.9253, Acc: 53.7313\n",
      "\t\t Training: Epoch(6) - Loss: 0.9170, Acc: 68.4385\n",
      "\t\t Validation(6) - Loss: 1.0334, Acc: 65.6716\n",
      "\t\t Training: Epoch(7) - Loss: 0.6666, Acc: 75.5814\n",
      "\t\t Validation(7) - Loss: 1.1897, Acc: 59.7015\n",
      "\t\t Training: Epoch(8) - Loss: 0.6917, Acc: 77.2425\n",
      "\t\t Validation(8) - Loss: 1.4746, Acc: 61.1940\n",
      "\t\t Training: Epoch(9) - Loss: 0.5244, Acc: 79.9003\n",
      "\t\t Validation(9) - Loss: 1.6932, Acc: 50.7463\n",
      "\t\t Training: Epoch(10) - Loss: 0.5799, Acc: 78.0731\n",
      "\t\t Validation(10) - Loss: 1.1719, Acc: 70.1493\n",
      "\t\t Training: Epoch(11) - Loss: 0.4477, Acc: 84.0532\n",
      "\t\t Validation(11) - Loss: 1.5878, Acc: 61.1940\n",
      "\t\t Training: Epoch(12) - Loss: 0.2691, Acc: 91.6944\n",
      "\t\t Validation(12) - Loss: 0.5752, Acc: 77.6119\n",
      "\t\t Training: Epoch(13) - Loss: 0.3040, Acc: 90.0332\n",
      "\t\t Validation(13) - Loss: 0.6778, Acc: 70.1493\n",
      "\t\t Training: Epoch(14) - Loss: 0.2332, Acc: 91.6944\n",
      "\t\t Validation(14) - Loss: 0.6796, Acc: 74.6269\n",
      "\t\t Training: Epoch(15) - Loss: 0.2617, Acc: 90.8638\n",
      "\t\t Validation(15) - Loss: 0.9422, Acc: 70.1493\n",
      "\t\t Training: Epoch(16) - Loss: 0.2945, Acc: 91.0299\n",
      "\t\t Validation(16) - Loss: 1.4663, Acc: 61.1940\n",
      "\t\t Training: Epoch(17) - Loss: 0.2047, Acc: 93.1894\n",
      "\t\t Validation(17) - Loss: 1.2164, Acc: 71.6418\n",
      "\t\t Training: Epoch(18) - Loss: 0.2203, Acc: 92.1927\n",
      "\t\t Validation(18) - Loss: 0.8618, Acc: 76.1194\n",
      "\t\t Training: Epoch(19) - Loss: 0.2064, Acc: 93.3555\n",
      "\t\t Validation(19) - Loss: 0.8196, Acc: 70.1493\n",
      "Finished.\n",
      "Total time per fold: 457.17223739624023 seconds.\n",
      "Fold : 7\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.0062, Acc: 15.9468\n",
      "\t\t Validation(0) - Loss: 5.8319, Acc: 2.9851\n",
      "\t\t Training: Epoch(1) - Loss: 1.9610, Acc: 33.5548\n",
      "\t\t Validation(1) - Loss: 6.9345, Acc: 7.4627\n",
      "\t\t Training: Epoch(2) - Loss: 1.5518, Acc: 41.8605\n",
      "\t\t Validation(2) - Loss: 2.4560, Acc: 22.3881\n",
      "\t\t Training: Epoch(3) - Loss: 1.2752, Acc: 53.4884\n",
      "\t\t Validation(3) - Loss: 2.9581, Acc: 25.3731\n",
      "\t\t Training: Epoch(4) - Loss: 1.0062, Acc: 65.4485\n",
      "\t\t Validation(4) - Loss: 2.0208, Acc: 38.8060\n",
      "\t\t Training: Epoch(5) - Loss: 0.8744, Acc: 69.9336\n",
      "\t\t Validation(5) - Loss: 2.3179, Acc: 43.2836\n",
      "\t\t Training: Epoch(6) - Loss: 0.6707, Acc: 75.9136\n",
      "\t\t Validation(6) - Loss: 1.9717, Acc: 41.7910\n",
      "\t\t Training: Epoch(7) - Loss: 0.7524, Acc: 75.4153\n",
      "\t\t Validation(7) - Loss: 2.0584, Acc: 52.2388\n",
      "\t\t Training: Epoch(8) - Loss: 0.8037, Acc: 73.2558\n",
      "\t\t Validation(8) - Loss: 2.5036, Acc: 44.7761\n",
      "\t\t Training: Epoch(9) - Loss: 0.6874, Acc: 76.7442\n",
      "\t\t Validation(9) - Loss: 1.7069, Acc: 50.7463\n",
      "\t\t Training: Epoch(10) - Loss: 0.5121, Acc: 81.8937\n",
      "\t\t Validation(10) - Loss: 1.7393, Acc: 52.2388\n",
      "\t\t Training: Epoch(11) - Loss: 0.3841, Acc: 85.8804\n",
      "\t\t Validation(11) - Loss: 1.2756, Acc: 65.6716\n",
      "\t\t Training: Epoch(12) - Loss: 0.3864, Acc: 84.8837\n",
      "\t\t Validation(12) - Loss: 1.1269, Acc: 73.1343\n",
      "\t\t Training: Epoch(13) - Loss: 0.3436, Acc: 89.0365\n",
      "\t\t Validation(13) - Loss: 2.6931, Acc: 44.7761\n",
      "\t\t Training: Epoch(14) - Loss: 0.2848, Acc: 91.0299\n",
      "\t\t Validation(14) - Loss: 1.4312, Acc: 56.7164\n",
      "\t\t Training: Epoch(15) - Loss: 0.2832, Acc: 92.6910\n",
      "\t\t Validation(15) - Loss: 2.1771, Acc: 59.7015\n",
      "\t\t Training: Epoch(16) - Loss: 0.2433, Acc: 90.5316\n",
      "\t\t Validation(16) - Loss: 3.4047, Acc: 37.3134\n",
      "\t\t Training: Epoch(17) - Loss: 0.2618, Acc: 90.5316\n",
      "\t\t Validation(17) - Loss: 1.4185, Acc: 61.1940\n",
      "\t\t Training: Epoch(18) - Loss: 0.1538, Acc: 95.6811\n",
      "\t\t Validation(18) - Loss: 1.7618, Acc: 52.2388\n",
      "\t\t Training: Epoch(19) - Loss: 0.1707, Acc: 94.1860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Validation(19) - Loss: 1.4088, Acc: 62.6866\n",
      "Finished.\n",
      "Total time per fold: 460.94171810150146 seconds.\n",
      "Fold : 8\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.0568, Acc: 12.1262\n",
      "\t\t Validation(0) - Loss: 4.7451, Acc: 1.4925\n",
      "\t\t Training: Epoch(1) - Loss: 2.1239, Acc: 27.9070\n",
      "\t\t Validation(1) - Loss: 5.9616, Acc: 4.4776\n",
      "\t\t Training: Epoch(2) - Loss: 1.6572, Acc: 44.3522\n",
      "\t\t Validation(2) - Loss: 2.0962, Acc: 40.2985\n",
      "\t\t Training: Epoch(3) - Loss: 1.4739, Acc: 48.0066\n",
      "\t\t Validation(3) - Loss: 2.5934, Acc: 31.3433\n",
      "\t\t Training: Epoch(4) - Loss: 1.2059, Acc: 55.3156\n",
      "\t\t Validation(4) - Loss: 2.1672, Acc: 50.7463\n",
      "\t\t Training: Epoch(5) - Loss: 1.1519, Acc: 58.8040\n",
      "\t\t Validation(5) - Loss: 1.4314, Acc: 53.7313\n",
      "\t\t Training: Epoch(6) - Loss: 0.9505, Acc: 66.7774\n",
      "\t\t Validation(6) - Loss: 1.4777, Acc: 53.7313\n",
      "\t\t Training: Epoch(7) - Loss: 0.8634, Acc: 67.2757\n",
      "\t\t Validation(7) - Loss: 2.2720, Acc: 47.7612\n",
      "\t\t Training: Epoch(8) - Loss: 0.7264, Acc: 75.4153\n",
      "\t\t Validation(8) - Loss: 1.7537, Acc: 50.7463\n",
      "\t\t Training: Epoch(9) - Loss: 0.6177, Acc: 78.5714\n",
      "\t\t Validation(9) - Loss: 1.3395, Acc: 50.7463\n",
      "\t\t Training: Epoch(10) - Loss: 0.7254, Acc: 74.4186\n",
      "\t\t Validation(10) - Loss: 1.4647, Acc: 67.1642\n",
      "\t\t Training: Epoch(11) - Loss: 0.6954, Acc: 76.2458\n",
      "\t\t Validation(11) - Loss: 0.9644, Acc: 71.6418\n",
      "\t\t Training: Epoch(12) - Loss: 0.4781, Acc: 81.7276\n",
      "\t\t Validation(12) - Loss: 1.3010, Acc: 59.7015\n",
      "\t\t Training: Epoch(13) - Loss: 0.4432, Acc: 85.5482\n",
      "\t\t Validation(13) - Loss: 1.2215, Acc: 58.2090\n",
      "\t\t Training: Epoch(14) - Loss: 0.2906, Acc: 91.5282\n",
      "\t\t Validation(14) - Loss: 1.4275, Acc: 55.2239\n",
      "\t\t Training: Epoch(15) - Loss: 0.2401, Acc: 92.1927\n",
      "\t\t Validation(15) - Loss: 1.2689, Acc: 68.6567\n",
      "\t\t Training: Epoch(16) - Loss: 0.2090, Acc: 93.3555\n",
      "\t\t Validation(16) - Loss: 0.9815, Acc: 73.1343\n",
      "\t\t Training: Epoch(17) - Loss: 0.1976, Acc: 92.3588\n",
      "\t\t Validation(17) - Loss: 1.3923, Acc: 61.1940\n",
      "\t\t Training: Epoch(18) - Loss: 0.2608, Acc: 91.1960\n",
      "\t\t Validation(18) - Loss: 2.0203, Acc: 47.7612\n",
      "\t\t Training: Epoch(19) - Loss: 0.2457, Acc: 92.0266\n",
      "\t\t Validation(19) - Loss: 1.0751, Acc: 73.1343\n",
      "Finished.\n",
      "Total time per fold: 462.52066373825073 seconds.\n",
      "Fold : 9\n",
      "Samples in training: 603\n",
      "Samples in test: 66\n",
      "\t\t Training: Epoch(0) - Loss: 2.9246, Acc: 16.4179\n",
      "\t\t Validation(0) - Loss: 5.6539, Acc: 1.5152\n",
      "\t\t Training: Epoch(1) - Loss: 1.9501, Acc: 31.1774\n",
      "\t\t Validation(1) - Loss: 6.6390, Acc: 12.1212\n",
      "\t\t Training: Epoch(2) - Loss: 1.6103, Acc: 44.4444\n",
      "\t\t Validation(2) - Loss: 1.9483, Acc: 33.3333\n",
      "\t\t Training: Epoch(3) - Loss: 1.3109, Acc: 55.8872\n",
      "\t\t Validation(3) - Loss: 1.6332, Acc: 51.5152\n",
      "\t\t Training: Epoch(4) - Loss: 1.1400, Acc: 61.5257\n",
      "\t\t Validation(4) - Loss: 2.2006, Acc: 39.3939\n",
      "\t\t Training: Epoch(5) - Loss: 0.8137, Acc: 71.6418\n",
      "\t\t Validation(5) - Loss: 2.4979, Acc: 54.5455\n",
      "\t\t Training: Epoch(6) - Loss: 0.8567, Acc: 70.9784\n",
      "\t\t Validation(6) - Loss: 1.6054, Acc: 45.4545\n",
      "\t\t Training: Epoch(7) - Loss: 0.5420, Acc: 79.2703\n",
      "\t\t Validation(7) - Loss: 1.3574, Acc: 57.5758\n",
      "\t\t Training: Epoch(8) - Loss: 0.6106, Acc: 79.2703\n",
      "\t\t Validation(8) - Loss: 1.1908, Acc: 62.1212\n",
      "\t\t Training: Epoch(9) - Loss: 0.6673, Acc: 77.2803\n",
      "\t\t Validation(9) - Loss: 2.4802, Acc: 46.9697\n",
      "\t\t Training: Epoch(10) - Loss: 0.5778, Acc: 80.7629\n",
      "\t\t Validation(10) - Loss: 1.6025, Acc: 53.0303\n",
      "\t\t Training: Epoch(11) - Loss: 0.5001, Acc: 83.2504\n",
      "\t\t Validation(11) - Loss: 0.9334, Acc: 63.6364\n",
      "\t\t Training: Epoch(12) - Loss: 0.3729, Acc: 87.8939\n",
      "\t\t Validation(12) - Loss: 0.7403, Acc: 71.2121\n",
      "\t\t Training: Epoch(13) - Loss: 0.3170, Acc: 90.0498\n",
      "\t\t Validation(13) - Loss: 1.7845, Acc: 46.9697\n",
      "\t\t Training: Epoch(14) - Loss: 0.2125, Acc: 93.5323\n",
      "\t\t Validation(14) - Loss: 1.2622, Acc: 60.6061\n",
      "\t\t Training: Epoch(15) - Loss: 0.1438, Acc: 95.5224\n",
      "\t\t Validation(15) - Loss: 0.9961, Acc: 75.7576\n",
      "\t\t Training: Epoch(16) - Loss: 0.1579, Acc: 94.6932\n",
      "\t\t Validation(16) - Loss: 0.8404, Acc: 66.6667\n",
      "\t\t Training: Epoch(17) - Loss: 0.2166, Acc: 92.2056\n",
      "\t\t Validation(17) - Loss: 1.2022, Acc: 62.1212\n",
      "\t\t Training: Epoch(18) - Loss: 0.2000, Acc: 94.3615\n",
      "\t\t Validation(18) - Loss: 1.8730, Acc: 60.6061\n",
      "\t\t Training: Epoch(19) - Loss: 0.2039, Acc: 94.0299\n",
      "\t\t Validation(19) - Loss: 1.5048, Acc: 59.0909\n",
      "Finished.\n",
      "Total time per fold: 458.9987428188324 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Variables to store fold scores\n",
    "train_acc = []\n",
    "test_top1_acc = []\n",
    "test_top5_acc = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "times = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(splits.split(total_set)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('Fold : {}'.format(fold))\n",
    "    \n",
    "    # Train and val samplers\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    print(\"Samples in training:\", len(train_sampler))\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    print(\"Samples in test:\", len(valid_sampler))\n",
    "    \n",
    "    # Train and val loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=train_batch_size, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=1, sampler=valid_sampler)\n",
    "    \n",
    "    device = get_device()\n",
    "    \n",
    "    criterion, model, optimizer = create_optimizer(load_model())\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(h_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        trunning_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += (preds == labels).sum()\n",
    "            trunning_corrects += preds.size(0)\n",
    "            \n",
    "\n",
    "        epoch_loss = running_loss / trunning_corrects\n",
    "        epoch_acc = (running_corrects.double()*100) / trunning_corrects\n",
    "        train_acc.append(epoch_acc.item())\n",
    "        \n",
    "        print('\\t\\t Training: Epoch({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        model.eval()  \n",
    "        \n",
    "        vrunning_loss = 0.0\n",
    "        vrunning_corrects = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for data, labels in valid_loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(data)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            vrunning_loss += loss.item() * data.size(0)\n",
    "            vrunning_corrects += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            \n",
    "        vepoch_loss = vrunning_loss/num_samples\n",
    "        vepoch_acc = (vrunning_corrects.double() * 100)/num_samples\n",
    "        \n",
    "        print('\\t\\t Validation({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, vepoch_loss, vepoch_acc))\n",
    "    \n",
    "    # Calculating and appending scores to this fold\n",
    "    model.class_to_idx = total_set.class_to_idx\n",
    "    scores = get_scores(model, valid_loader)\n",
    "    \n",
    "    test_top1_acc.append(scores[0])\n",
    "    test_top5_acc.append(scores[1])\n",
    "    test_precision.append(scores[2])\n",
    "    test_recall.append(scores[3])\n",
    "    test_f1.append(scores[4])\n",
    "    \n",
    "    time_fold = time.time() - start_time\n",
    "    times.append(time_fold)\n",
    "    print(\"Total time per fold: %s seconds.\" %(time_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy average:  0.72694400368038\n",
      "Top-1 test accuracy average:  0.6486431478968792\n",
      "Top-5 test accuracy average:  0.9655585707824514\n",
      "Weighted Precision test accuracy average:  0.6827980228726498\n",
      "Weighted Recall test accuracy average:  0.6486431478968792\n",
      "Weighted F1 test accuracy average:  0.6266244534901253\n",
      "Average time per fold (seconds): 483.1394223928452\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy average: \", np.mean(train_acc) / 100)\n",
    "print(\"Top-1 test accuracy average: \", np.mean(test_top1_acc))\n",
    "print(\"Top-5 test accuracy average: \", np.mean(test_top5_acc))\n",
    "print(\"Weighted Precision test accuracy average: \", np.mean(test_precision))\n",
    "print(\"Weighted Recall test accuracy average: \", np.mean(test_recall))\n",
    "print(\"Weighted F1 test accuracy average: \", np.mean(test_f1))\n",
    "print(\"Average time per fold (seconds):\", np.mean(times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
