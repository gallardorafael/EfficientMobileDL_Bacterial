{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the paper: \n",
    "\n",
    "## Efficient and Mobile Deep Learning Architectures for Fast Identification of BacterialStrains in Resource-Constrained Devices\n",
    "\n",
    "### Architecture: MobileNet v3 Large\n",
    "### Data: Augmented + Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, models\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "# Archs not in Pytorch\n",
    "from scripts.mobilenetv3 import mobilenetv3_large\n",
    "\n",
    "# External functions\n",
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "3.8.5 (default, Jul 28 2020, 12:59:40) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data paths and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters and dataset details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset details\n",
    "dataset_version = 'original' # original or augmented\n",
    "img_shape = (224,224)\n",
    "img_size = str(img_shape[0])+\"x\"+str(img_shape[1])\n",
    "\n",
    "# Root directory of dataset\n",
    "data_dir = '/home/yibbtstll/venvs/pytorch_gpu/CySDeepBacterial/Dataset/DIBaS_augmented/'\n",
    "\n",
    "train_batch_size = 64\n",
    "val_test_batch_size = 32\n",
    "feature_extract = False\n",
    "pretrained = True\n",
    "h_epochs = 15\n",
    "kfolds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining transforms and creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for input data\n",
    "training_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "total_set = datasets.ImageFolder(data_dir, transform=training_transforms)\n",
    "\n",
    "# Defining folds\n",
    "splits = KFold(n_splits = kfolds, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the target classes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "{0: 'Acinetobacter.baumanii', 1: 'Actinomyces.israeli', 2: 'Bacteroides.fragilis', 3: 'Bifidobacterium.spp', 4: 'Clostridium.perfringens', 5: 'Enterococcus.faecalis', 6: 'Enterococcus.faecium', 7: 'Escherichia.coli', 8: 'Fusobacterium', 9: 'Lactobacillus.casei', 10: 'Lactobacillus.crispatus', 11: 'Lactobacillus.delbrueckii', 12: 'Lactobacillus.gasseri', 13: 'Lactobacillus.jehnsenii', 14: 'Lactobacillus.johnsonii', 15: 'Lactobacillus.paracasei', 16: 'Lactobacillus.plantarum', 17: 'Lactobacillus.reuteri', 18: 'Lactobacillus.rhamnosus', 19: 'Lactobacillus.salivarius', 20: 'Listeria.monocytogenes', 21: 'Micrococcus.spp', 22: 'Neisseria.gonorrhoeae', 23: 'Porfyromonas.gingivalis', 24: 'Propionibacterium.acnes', 25: 'Proteus', 26: 'Pseudomonas.aeruginosa', 27: 'Staphylococcus.aureus', 28: 'Staphylococcus.epidermidis', 29: 'Staphylococcus.saprophiticus', 30: 'Streptococcus.agalactiae', 31: 'Veionella'}\n"
     ]
    }
   ],
   "source": [
    "train_labels = {value : key for (key, value) in total_set.class_to_idx.items()}\n",
    "    \n",
    "print(len(train_labels)) \n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and inicialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing pre-trained parameters, finetunning the classifier to output 32 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained model parameters to avoid backpropogating through them\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        print(\"Setting grad to false.\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_device():\n",
    "    # Model and criterion to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): h_swish(\n",
       "        (sigmoid): h_sigmoid(\n",
       "          (relu): ReLU6(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Identity()\n",
       "        (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Identity()\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Identity()\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=72, out_features=24, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=24, out_features=72, bias=True)\n",
       "            (3): h_sigmoid(\n",
       "              (relu): ReLU6(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "        (4): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=120, out_features=32, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=32, out_features=120, bias=True)\n",
       "            (3): h_sigmoid(\n",
       "              (relu): ReLU6(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "        (4): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=120, out_features=32, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=32, out_features=120, bias=True)\n",
       "            (3): h_sigmoid(\n",
       "              (relu): ReLU6(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "        (4): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Identity()\n",
       "        (6): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "        (4): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Identity()\n",
       "        (6): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "        (4): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Identity()\n",
       "        (6): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "        (4): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Identity()\n",
       "        (6): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "        (4): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=480, out_features=120, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=120, out_features=480, bias=True)\n",
       "            (3): h_sigmoid(\n",
       "              (relu): ReLU6(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "        (4): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=672, out_features=168, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=168, out_features=672, bias=True)\n",
       "            (3): h_sigmoid(\n",
       "              (relu): ReLU6(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "        (4): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=672, out_features=168, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=168, out_features=672, bias=True)\n",
       "            (3): h_sigmoid(\n",
       "              (relu): ReLU6(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=960, out_features=240, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=240, out_features=960, bias=True)\n",
       "            (3): h_sigmoid(\n",
       "              (relu): ReLU6(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=960, out_features=240, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=240, out_features=960, bias=True)\n",
       "            (3): h_sigmoid(\n",
       "              (relu): ReLU6(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): h_swish(\n",
       "          (sigmoid): h_sigmoid(\n",
       "            (relu): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=960, out_features=1280, bias=True)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenetv3_large()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Transfer Learning\n",
    "    model = mobilenetv3_large()\n",
    "    model.load_state_dict(torch.load('scripts/pretrained/mobilenetv3-large-1cd25616.pth'))\n",
    "    \n",
    "    # Mode\n",
    "    model = set_parameter_requires_grad(model, feature_extract)\n",
    "    \n",
    "    # Fine tuning\n",
    "    # Build custom classifier\n",
    "    model.classifier[3] = nn.Linear(in_features=1280,\n",
    "                                out_features=32)\n",
    "    return model\n",
    "\n",
    "def create_optimizer(model):\n",
    "    # Parameters to update\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "\n",
    "    else:\n",
    "        n_params = 0\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                n_params += 1\n",
    "\n",
    "\n",
    "    # Loss function and gradient descent\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(params_to_update, \n",
    "                          lr=0.0001, \n",
    "                          weight_decay=0.00004)\n",
    "    \n",
    "    return criterion.to(get_device()), model.to(get_device()), optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Samples in training: 21665\n",
      "Samples in test: 2408\n",
      "\t\t Training: Epoch(0) - Loss: 0.7530, Acc: 80.6739\n",
      "\t\t Validation(0) - Loss: 0.1976, Acc: 93.8538\n",
      "\t\t Training: Epoch(1) - Loss: 0.1592, Acc: 94.9365\n",
      "\t\t Validation(1) - Loss: 0.1159, Acc: 96.0963\n",
      "\t\t Training: Epoch(2) - Loss: 0.1005, Acc: 96.7413\n",
      "\t\t Validation(2) - Loss: 0.1094, Acc: 96.2625\n",
      "\t\t Training: Epoch(3) - Loss: 0.0723, Acc: 97.6044\n",
      "\t\t Validation(3) - Loss: 0.0916, Acc: 96.9269\n",
      "\t\t Training: Epoch(4) - Loss: 0.0616, Acc: 97.9137\n",
      "\t\t Validation(4) - Loss: 0.1047, Acc: 96.6362\n",
      "\t\t Training: Epoch(5) - Loss: 0.0416, Acc: 98.6522\n",
      "\t\t Validation(5) - Loss: 0.0938, Acc: 97.3007\n",
      "\t\t Training: Epoch(6) - Loss: 0.0408, Acc: 98.7261\n",
      "\t\t Validation(6) - Loss: 0.0980, Acc: 97.2176\n",
      "\t\t Training: Epoch(7) - Loss: 0.0307, Acc: 99.0030\n",
      "\t\t Validation(7) - Loss: 0.0993, Acc: 97.0930\n",
      "\t\t Training: Epoch(8) - Loss: 0.0300, Acc: 99.0815\n",
      "\t\t Validation(8) - Loss: 0.1017, Acc: 97.0515\n",
      "\t\t Training: Epoch(9) - Loss: 0.0237, Acc: 99.2199\n",
      "\t\t Validation(9) - Loss: 0.0851, Acc: 97.3422\n",
      "\t\t Training: Epoch(10) - Loss: 0.0291, Acc: 99.0492\n",
      "\t\t Validation(10) - Loss: 0.1012, Acc: 97.2176\n",
      "\t\t Training: Epoch(11) - Loss: 0.0236, Acc: 99.2199\n",
      "\t\t Validation(11) - Loss: 0.1101, Acc: 97.2176\n",
      "\t\t Training: Epoch(12) - Loss: 0.0165, Acc: 99.4553\n",
      "\t\t Validation(12) - Loss: 0.1146, Acc: 97.3837\n",
      "\t\t Training: Epoch(13) - Loss: 0.0184, Acc: 99.4184\n",
      "\t\t Validation(13) - Loss: 0.1125, Acc: 96.9684\n",
      "\t\t Training: Epoch(14) - Loss: 0.0171, Acc: 99.4369\n",
      "\t\t Validation(14) - Loss: 0.1082, Acc: 97.0515\n",
      "Finished.\n",
      "Total time per fold: 2373.130252122879 seconds.\n",
      "Fold : 1\n",
      "Samples in training: 21665\n",
      "Samples in test: 2408\n",
      "\t\t Training: Epoch(0) - Loss: 0.7355, Acc: 81.4355\n",
      "\t\t Validation(0) - Loss: 0.1671, Acc: 94.4352\n",
      "\t\t Training: Epoch(1) - Loss: 0.1526, Acc: 95.0704\n",
      "\t\t Validation(1) - Loss: 0.1129, Acc: 96.3870\n",
      "\t\t Training: Epoch(2) - Loss: 0.0974, Acc: 96.7921\n",
      "\t\t Validation(2) - Loss: 0.0998, Acc: 96.8439\n",
      "\t\t Training: Epoch(3) - Loss: 0.0720, Acc: 97.6321\n",
      "\t\t Validation(3) - Loss: 0.0925, Acc: 96.5947\n",
      "\t\t Training: Epoch(4) - Loss: 0.0550, Acc: 98.1537\n",
      "\t\t Validation(4) - Loss: 0.0827, Acc: 97.3007\n",
      "\t\t Training: Epoch(5) - Loss: 0.0419, Acc: 98.6476\n",
      "\t\t Validation(5) - Loss: 0.0878, Acc: 96.8854\n",
      "\t\t Training: Epoch(6) - Loss: 0.0367, Acc: 98.8091\n",
      "\t\t Validation(6) - Loss: 0.1155, Acc: 96.7608\n",
      "\t\t Training: Epoch(7) - Loss: 0.0310, Acc: 98.9984\n",
      "\t\t Validation(7) - Loss: 0.0849, Acc: 97.3007\n",
      "\t\t Training: Epoch(8) - Loss: 0.0303, Acc: 99.0538\n",
      "\t\t Validation(8) - Loss: 0.0859, Acc: 97.4252\n",
      "\t\t Training: Epoch(9) - Loss: 0.0255, Acc: 99.1922\n",
      "\t\t Validation(9) - Loss: 0.1156, Acc: 96.5116\n",
      "\t\t Training: Epoch(10) - Loss: 0.0197, Acc: 99.3676\n",
      "\t\t Validation(10) - Loss: 0.0921, Acc: 97.6744\n",
      "\t\t Training: Epoch(11) - Loss: 0.0202, Acc: 99.4461\n",
      "\t\t Validation(11) - Loss: 0.1024, Acc: 97.4668\n",
      "\t\t Training: Epoch(12) - Loss: 0.0214, Acc: 99.3123\n",
      "\t\t Validation(12) - Loss: 0.1040, Acc: 97.4252\n",
      "\t\t Training: Epoch(13) - Loss: 0.0183, Acc: 99.3676\n",
      "\t\t Validation(13) - Loss: 0.0932, Acc: 97.4668\n",
      "\t\t Training: Epoch(14) - Loss: 0.0229, Acc: 99.2799\n",
      "\t\t Validation(14) - Loss: 0.1090, Acc: 97.3007\n",
      "Finished.\n",
      "Total time per fold: 1760.7540769577026 seconds.\n",
      "Fold : 2\n",
      "Samples in training: 21665\n",
      "Samples in test: 2408\n",
      "\t\t Training: Epoch(0) - Loss: 0.7479, Acc: 80.8585\n",
      "\t\t Validation(0) - Loss: 0.1634, Acc: 95.1827\n",
      "\t\t Training: Epoch(1) - Loss: 0.1590, Acc: 94.9227\n",
      "\t\t Validation(1) - Loss: 0.1165, Acc: 96.0963\n",
      "\t\t Training: Epoch(2) - Loss: 0.1009, Acc: 96.6721\n",
      "\t\t Validation(2) - Loss: 0.0994, Acc: 97.1346\n",
      "\t\t Training: Epoch(3) - Loss: 0.0726, Acc: 97.5398\n",
      "\t\t Validation(3) - Loss: 0.0993, Acc: 97.2176\n",
      "\t\t Training: Epoch(4) - Loss: 0.0573, Acc: 98.1676\n",
      "\t\t Validation(4) - Loss: 0.0959, Acc: 97.0100\n",
      "\t\t Training: Epoch(5) - Loss: 0.0432, Acc: 98.5784\n",
      "\t\t Validation(5) - Loss: 0.1027, Acc: 96.8023\n",
      "\t\t Training: Epoch(6) - Loss: 0.0376, Acc: 98.8230\n",
      "\t\t Validation(6) - Loss: 0.0990, Acc: 97.1346\n",
      "\t\t Training: Epoch(7) - Loss: 0.0315, Acc: 99.0538\n",
      "\t\t Validation(7) - Loss: 0.0938, Acc: 97.3422\n",
      "\t\t Training: Epoch(8) - Loss: 0.0293, Acc: 99.1230\n",
      "\t\t Validation(8) - Loss: 0.1221, Acc: 96.7193\n",
      "\t\t Training: Epoch(9) - Loss: 0.0271, Acc: 99.1045\n",
      "\t\t Validation(9) - Loss: 0.1047, Acc: 97.1346\n",
      "\t\t Training: Epoch(10) - Loss: 0.0211, Acc: 99.3538\n",
      "\t\t Validation(10) - Loss: 0.1049, Acc: 97.3837\n",
      "\t\t Training: Epoch(11) - Loss: 0.0296, Acc: 99.1322\n",
      "\t\t Validation(11) - Loss: 0.0984, Acc: 97.0100\n",
      "\t\t Training: Epoch(12) - Loss: 0.0184, Acc: 99.4230\n",
      "\t\t Validation(12) - Loss: 0.0971, Acc: 97.4252\n",
      "\t\t Training: Epoch(13) - Loss: 0.0208, Acc: 99.3861\n",
      "\t\t Validation(13) - Loss: 0.1068, Acc: 97.3837\n",
      "\t\t Training: Epoch(14) - Loss: 0.0193, Acc: 99.4369\n",
      "\t\t Validation(14) - Loss: 0.1001, Acc: 97.3007\n",
      "Finished.\n",
      "Total time per fold: 1748.4476056098938 seconds.\n",
      "Fold : 3\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 0.7352, Acc: 81.5056\n",
      "\t\t Validation(0) - Loss: 0.1607, Acc: 94.9730\n",
      "\t\t Training: Epoch(1) - Loss: 0.1562, Acc: 94.8168\n",
      "\t\t Validation(1) - Loss: 0.1121, Acc: 96.2609\n",
      "\t\t Training: Epoch(2) - Loss: 0.0948, Acc: 96.8014\n",
      "\t\t Validation(2) - Loss: 0.0866, Acc: 97.0918\n",
      "\t\t Training: Epoch(3) - Loss: 0.0683, Acc: 97.7061\n",
      "\t\t Validation(3) - Loss: 0.0971, Acc: 96.7179\n",
      "\t\t Training: Epoch(4) - Loss: 0.0580, Acc: 98.2046\n",
      "\t\t Validation(4) - Loss: 0.0992, Acc: 96.6764\n",
      "\t\t Training: Epoch(5) - Loss: 0.0457, Acc: 98.4353\n",
      "\t\t Validation(5) - Loss: 0.1052, Acc: 96.9672\n",
      "\t\t Training: Epoch(6) - Loss: 0.0427, Acc: 98.5507\n",
      "\t\t Validation(6) - Loss: 0.0992, Acc: 96.8425\n",
      "\t\t Training: Epoch(7) - Loss: 0.0374, Acc: 98.8092\n",
      "\t\t Validation(7) - Loss: 0.0927, Acc: 96.9672\n",
      "\t\t Training: Epoch(8) - Loss: 0.0290, Acc: 99.0077\n",
      "\t\t Validation(8) - Loss: 0.1161, Acc: 96.8425\n",
      "\t\t Training: Epoch(9) - Loss: 0.0207, Acc: 99.3769\n",
      "\t\t Validation(9) - Loss: 0.1029, Acc: 97.1749\n",
      "\t\t Training: Epoch(10) - Loss: 0.0247, Acc: 99.2061\n",
      "\t\t Validation(10) - Loss: 0.1056, Acc: 96.9256\n",
      "\t\t Training: Epoch(11) - Loss: 0.0283, Acc: 99.0446\n",
      "\t\t Validation(11) - Loss: 0.0793, Acc: 97.6319\n",
      "\t\t Training: Epoch(12) - Loss: 0.0209, Acc: 99.3031\n",
      "\t\t Validation(12) - Loss: 0.1111, Acc: 97.2165\n",
      "\t\t Training: Epoch(13) - Loss: 0.0168, Acc: 99.4738\n",
      "\t\t Validation(13) - Loss: 0.0943, Acc: 97.4242\n",
      "\t\t Training: Epoch(14) - Loss: 0.0175, Acc: 99.4784\n",
      "\t\t Validation(14) - Loss: 0.0994, Acc: 97.0918\n",
      "Finished.\n",
      "Total time per fold: 1751.226223230362 seconds.\n",
      "Fold : 4\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 0.7335, Acc: 81.2794\n",
      "\t\t Validation(0) - Loss: 0.1683, Acc: 94.8899\n",
      "\t\t Training: Epoch(1) - Loss: 0.1464, Acc: 95.1445\n",
      "\t\t Validation(1) - Loss: 0.1155, Acc: 96.5517\n",
      "\t\t Training: Epoch(2) - Loss: 0.0989, Acc: 96.7230\n",
      "\t\t Validation(2) - Loss: 0.1027, Acc: 96.6348\n",
      "\t\t Training: Epoch(3) - Loss: 0.0693, Acc: 97.7984\n",
      "\t\t Validation(3) - Loss: 0.0968, Acc: 97.0918\n",
      "\t\t Training: Epoch(4) - Loss: 0.0558, Acc: 98.1353\n",
      "\t\t Validation(4) - Loss: 0.1158, Acc: 96.5933\n",
      "\t\t Training: Epoch(5) - Loss: 0.0424, Acc: 98.6061\n",
      "\t\t Validation(5) - Loss: 0.1184, Acc: 97.0918\n",
      "\t\t Training: Epoch(6) - Loss: 0.0377, Acc: 98.8230\n",
      "\t\t Validation(6) - Loss: 0.1015, Acc: 96.8425\n",
      "\t\t Training: Epoch(7) - Loss: 0.0328, Acc: 98.8923\n",
      "\t\t Validation(7) - Loss: 0.0960, Acc: 97.0087\n",
      "\t\t Training: Epoch(8) - Loss: 0.0305, Acc: 99.0769\n",
      "\t\t Validation(8) - Loss: 0.0913, Acc: 97.2165\n",
      "\t\t Training: Epoch(9) - Loss: 0.0263, Acc: 99.1969\n",
      "\t\t Validation(9) - Loss: 0.1262, Acc: 96.7179\n",
      "\t\t Training: Epoch(10) - Loss: 0.0248, Acc: 99.1877\n",
      "\t\t Validation(10) - Loss: 0.1165, Acc: 96.6764\n",
      "\t\t Training: Epoch(11) - Loss: 0.0185, Acc: 99.4738\n",
      "\t\t Validation(11) - Loss: 0.0958, Acc: 97.4657\n",
      "\t\t Training: Epoch(12) - Loss: 0.0197, Acc: 99.3861\n",
      "\t\t Validation(12) - Loss: 0.1057, Acc: 97.4657\n",
      "\t\t Training: Epoch(13) - Loss: 0.0169, Acc: 99.5154\n",
      "\t\t Validation(13) - Loss: 0.1103, Acc: 96.9672\n",
      "\t\t Training: Epoch(14) - Loss: 0.0206, Acc: 99.3538\n",
      "\t\t Validation(14) - Loss: 0.0959, Acc: 97.5904\n",
      "Finished.\n",
      "Total time per fold: 1750.2483603954315 seconds.\n",
      "Fold : 5\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 0.7319, Acc: 81.5425\n",
      "\t\t Validation(0) - Loss: 0.1656, Acc: 94.7237\n",
      "\t\t Training: Epoch(1) - Loss: 0.1479, Acc: 95.1491\n",
      "\t\t Validation(1) - Loss: 0.1446, Acc: 95.1807\n",
      "\t\t Training: Epoch(2) - Loss: 0.0987, Acc: 96.7461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Validation(2) - Loss: 0.1105, Acc: 96.6348\n",
      "\t\t Training: Epoch(3) - Loss: 0.0702, Acc: 97.8399\n",
      "\t\t Validation(3) - Loss: 0.1060, Acc: 96.5102\n",
      "\t\t Training: Epoch(4) - Loss: 0.0569, Acc: 98.0892\n",
      "\t\t Validation(4) - Loss: 0.0951, Acc: 96.9256\n",
      "\t\t Training: Epoch(5) - Loss: 0.0436, Acc: 98.5646\n",
      "\t\t Validation(5) - Loss: 0.0864, Acc: 97.1749\n",
      "\t\t Training: Epoch(6) - Loss: 0.0387, Acc: 98.7953\n",
      "\t\t Validation(6) - Loss: 0.0980, Acc: 96.9256\n",
      "\t\t Training: Epoch(7) - Loss: 0.0337, Acc: 98.8969\n",
      "\t\t Validation(7) - Loss: 0.1292, Acc: 96.3855\n",
      "\t\t Training: Epoch(8) - Loss: 0.0262, Acc: 99.1369\n",
      "\t\t Validation(8) - Loss: 0.1033, Acc: 97.1334\n",
      "\t\t Training: Epoch(9) - Loss: 0.0250, Acc: 99.1600\n",
      "\t\t Validation(9) - Loss: 0.1069, Acc: 96.8841\n",
      "\t\t Training: Epoch(10) - Loss: 0.0238, Acc: 99.2846\n",
      "\t\t Validation(10) - Loss: 0.0954, Acc: 97.3411\n",
      "\t\t Training: Epoch(11) - Loss: 0.0218, Acc: 99.3400\n",
      "\t\t Validation(11) - Loss: 0.1038, Acc: 96.8841\n",
      "\t\t Training: Epoch(12) - Loss: 0.0180, Acc: 99.4461\n",
      "\t\t Validation(12) - Loss: 0.1178, Acc: 97.1749\n",
      "\t\t Training: Epoch(13) - Loss: 0.0214, Acc: 99.1969\n",
      "\t\t Validation(13) - Loss: 0.1136, Acc: 97.1749\n",
      "\t\t Training: Epoch(14) - Loss: 0.0197, Acc: 99.3446\n",
      "\t\t Validation(14) - Loss: 0.0960, Acc: 97.3411\n",
      "Finished.\n",
      "Total time per fold: 1751.0159530639648 seconds.\n",
      "Fold : 6\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 0.7336, Acc: 81.4364\n",
      "\t\t Validation(0) - Loss: 0.1765, Acc: 94.1836\n",
      "\t\t Training: Epoch(1) - Loss: 0.1604, Acc: 94.6829\n",
      "\t\t Validation(1) - Loss: 0.1085, Acc: 96.4271\n",
      "\t\t Training: Epoch(2) - Loss: 0.0962, Acc: 96.7414\n",
      "\t\t Validation(2) - Loss: 0.1103, Acc: 96.5517\n",
      "\t\t Training: Epoch(3) - Loss: 0.0788, Acc: 97.4384\n",
      "\t\t Validation(3) - Loss: 0.0892, Acc: 97.1334\n",
      "\t\t Training: Epoch(4) - Loss: 0.0537, Acc: 98.1815\n",
      "\t\t Validation(4) - Loss: 0.0848, Acc: 97.1334\n",
      "\t\t Training: Epoch(5) - Loss: 0.0464, Acc: 98.5000\n",
      "\t\t Validation(5) - Loss: 0.0963, Acc: 96.6764\n",
      "\t\t Training: Epoch(6) - Loss: 0.0360, Acc: 98.8046\n",
      "\t\t Validation(6) - Loss: 0.0742, Acc: 97.6319\n",
      "\t\t Training: Epoch(7) - Loss: 0.0371, Acc: 98.7769\n",
      "\t\t Validation(7) - Loss: 0.0951, Acc: 96.9672\n",
      "\t\t Training: Epoch(8) - Loss: 0.0256, Acc: 99.1554\n",
      "\t\t Validation(8) - Loss: 0.0772, Acc: 97.5904\n",
      "\t\t Training: Epoch(9) - Loss: 0.0256, Acc: 99.1969\n",
      "\t\t Validation(9) - Loss: 0.1051, Acc: 97.3826\n",
      "\t\t Training: Epoch(10) - Loss: 0.0253, Acc: 99.1646\n",
      "\t\t Validation(10) - Loss: 0.0976, Acc: 97.3826\n",
      "\t\t Training: Epoch(11) - Loss: 0.0190, Acc: 99.4138\n",
      "\t\t Validation(11) - Loss: 0.1214, Acc: 96.9256\n",
      "\t\t Training: Epoch(12) - Loss: 0.0241, Acc: 99.2984\n",
      "\t\t Validation(12) - Loss: 0.1051, Acc: 97.2580\n",
      "\t\t Training: Epoch(13) - Loss: 0.0146, Acc: 99.5477\n",
      "\t\t Validation(13) - Loss: 0.1158, Acc: 96.8841\n",
      "\t\t Training: Epoch(14) - Loss: 0.0196, Acc: 99.3584\n",
      "\t\t Validation(14) - Loss: 0.1092, Acc: 97.3826\n",
      "Finished.\n",
      "Total time per fold: 1762.4569623470306 seconds.\n",
      "Fold : 7\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 0.7415, Acc: 81.0948\n",
      "\t\t Validation(0) - Loss: 0.1731, Acc: 94.3083\n",
      "\t\t Training: Epoch(1) - Loss: 0.1563, Acc: 94.9737\n",
      "\t\t Validation(1) - Loss: 0.1208, Acc: 95.9285\n",
      "\t\t Training: Epoch(2) - Loss: 0.1000, Acc: 96.7691\n",
      "\t\t Validation(2) - Loss: 0.1073, Acc: 96.3025\n",
      "\t\t Training: Epoch(3) - Loss: 0.0750, Acc: 97.5861\n",
      "\t\t Validation(3) - Loss: 0.1049, Acc: 96.6348\n",
      "\t\t Training: Epoch(4) - Loss: 0.0573, Acc: 98.0107\n",
      "\t\t Validation(4) - Loss: 0.0860, Acc: 97.6319\n",
      "\t\t Training: Epoch(5) - Loss: 0.0427, Acc: 98.5876\n",
      "\t\t Validation(5) - Loss: 0.0970, Acc: 97.2165\n",
      "\t\t Training: Epoch(6) - Loss: 0.0386, Acc: 98.6800\n",
      "\t\t Validation(6) - Loss: 0.1126, Acc: 96.9672\n",
      "\t\t Training: Epoch(7) - Loss: 0.0300, Acc: 99.0169\n",
      "\t\t Validation(7) - Loss: 0.1309, Acc: 96.5102\n",
      "\t\t Training: Epoch(8) - Loss: 0.0337, Acc: 98.9107\n",
      "\t\t Validation(8) - Loss: 0.1069, Acc: 97.0918\n",
      "\t\t Training: Epoch(9) - Loss: 0.0264, Acc: 99.1692\n",
      "\t\t Validation(9) - Loss: 0.1359, Acc: 96.6348\n",
      "\t\t Training: Epoch(10) - Loss: 0.0212, Acc: 99.3123\n",
      "\t\t Validation(10) - Loss: 0.1391, Acc: 96.5517\n",
      "\t\t Training: Epoch(11) - Loss: 0.0244, Acc: 99.2246\n",
      "\t\t Validation(11) - Loss: 0.1086, Acc: 96.9672\n",
      "\t\t Training: Epoch(12) - Loss: 0.0187, Acc: 99.3723\n",
      "\t\t Validation(12) - Loss: 0.1144, Acc: 97.2995\n",
      "\t\t Training: Epoch(13) - Loss: 0.0150, Acc: 99.5754\n",
      "\t\t Validation(13) - Loss: 0.1128, Acc: 97.3826\n",
      "\t\t Training: Epoch(14) - Loss: 0.0167, Acc: 99.5015\n",
      "\t\t Validation(14) - Loss: 0.1128, Acc: 97.2165\n",
      "Finished.\n",
      "Total time per fold: 1747.348824262619 seconds.\n",
      "Fold : 8\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 0.7334, Acc: 81.7410\n",
      "\t\t Validation(0) - Loss: 0.1544, Acc: 95.2223\n",
      "\t\t Training: Epoch(1) - Loss: 0.1554, Acc: 95.0337\n",
      "\t\t Validation(1) - Loss: 0.1120, Acc: 96.7179\n",
      "\t\t Training: Epoch(2) - Loss: 0.1018, Acc: 96.6168\n",
      "\t\t Validation(2) - Loss: 0.1046, Acc: 96.1778\n",
      "\t\t Training: Epoch(3) - Loss: 0.0709, Acc: 97.5445\n",
      "\t\t Validation(3) - Loss: 0.0801, Acc: 97.3826\n",
      "\t\t Training: Epoch(4) - Loss: 0.0513, Acc: 98.3569\n",
      "\t\t Validation(4) - Loss: 0.0996, Acc: 96.9256\n",
      "\t\t Training: Epoch(5) - Loss: 0.0476, Acc: 98.4769\n",
      "\t\t Validation(5) - Loss: 0.0820, Acc: 97.5488\n",
      "\t\t Training: Epoch(6) - Loss: 0.0416, Acc: 98.7584\n",
      "\t\t Validation(6) - Loss: 0.0797, Acc: 97.5073\n",
      "\t\t Training: Epoch(7) - Loss: 0.0308, Acc: 99.0584\n",
      "\t\t Validation(7) - Loss: 0.0754, Acc: 97.7150\n",
      "\t\t Training: Epoch(8) - Loss: 0.0242, Acc: 99.2754\n",
      "\t\t Validation(8) - Loss: 0.0795, Acc: 97.4242\n",
      "\t\t Training: Epoch(9) - Loss: 0.0307, Acc: 99.0169\n",
      "\t\t Validation(9) - Loss: 0.0811, Acc: 97.0087\n",
      "\t\t Training: Epoch(10) - Loss: 0.0234, Acc: 99.2200\n",
      "\t\t Validation(10) - Loss: 0.1064, Acc: 97.4242\n",
      "\t\t Training: Epoch(11) - Loss: 0.0215, Acc: 99.3261\n",
      "\t\t Validation(11) - Loss: 0.1040, Acc: 97.0503\n",
      "\t\t Training: Epoch(12) - Loss: 0.0155, Acc: 99.4923\n",
      "\t\t Validation(12) - Loss: 0.1005, Acc: 97.7981\n",
      "\t\t Training: Epoch(13) - Loss: 0.0181, Acc: 99.4415\n",
      "\t\t Validation(13) - Loss: 0.0843, Acc: 97.5904\n",
      "\t\t Training: Epoch(14) - Loss: 0.0161, Acc: 99.5200\n",
      "\t\t Validation(14) - Loss: 0.0981, Acc: 97.5073\n",
      "Finished.\n",
      "Total time per fold: 1748.8589351177216 seconds.\n",
      "Fold : 9\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 0.7379, Acc: 81.2840\n",
      "\t\t Validation(0) - Loss: 0.1570, Acc: 94.8484\n",
      "\t\t Training: Epoch(1) - Loss: 0.1558, Acc: 94.8906\n",
      "\t\t Validation(1) - Loss: 0.1223, Acc: 96.0532\n",
      "\t\t Training: Epoch(2) - Loss: 0.0996, Acc: 96.8014\n",
      "\t\t Validation(2) - Loss: 0.0821, Acc: 97.3411\n",
      "\t\t Training: Epoch(3) - Loss: 0.0734, Acc: 97.5168\n",
      "\t\t Validation(3) - Loss: 0.0874, Acc: 97.2580\n",
      "\t\t Training: Epoch(4) - Loss: 0.0564, Acc: 98.1399\n",
      "\t\t Validation(4) - Loss: 0.0696, Acc: 97.8396\n",
      "\t\t Training: Epoch(5) - Loss: 0.0480, Acc: 98.4492\n",
      "\t\t Validation(5) - Loss: 0.0686, Acc: 97.8812\n",
      "\t\t Training: Epoch(6) - Loss: 0.0369, Acc: 98.8046\n",
      "\t\t Validation(6) - Loss: 0.0792, Acc: 97.7981\n",
      "\t\t Training: Epoch(7) - Loss: 0.0321, Acc: 98.9938\n",
      "\t\t Validation(7) - Loss: 0.0760, Acc: 97.6735\n",
      "\t\t Training: Epoch(8) - Loss: 0.0284, Acc: 99.1138\n",
      "\t\t Validation(8) - Loss: 0.0673, Acc: 98.1720\n",
      "\t\t Training: Epoch(9) - Loss: 0.0215, Acc: 99.3031\n",
      "\t\t Validation(9) - Loss: 0.0737, Acc: 97.7565\n",
      "\t\t Training: Epoch(10) - Loss: 0.0225, Acc: 99.2846\n",
      "\t\t Validation(10) - Loss: 0.0976, Acc: 97.5488\n",
      "\t\t Training: Epoch(11) - Loss: 0.0252, Acc: 99.1877\n",
      "\t\t Validation(11) - Loss: 0.0695, Acc: 98.1720\n",
      "\t\t Training: Epoch(12) - Loss: 0.0214, Acc: 99.3861\n",
      "\t\t Validation(12) - Loss: 0.0861, Acc: 97.8812\n",
      "\t\t Training: Epoch(13) - Loss: 0.0217, Acc: 99.3123\n",
      "\t\t Validation(13) - Loss: 0.0849, Acc: 97.7565\n",
      "\t\t Training: Epoch(14) - Loss: 0.0175, Acc: 99.4784\n",
      "\t\t Validation(14) - Loss: 0.0804, Acc: 98.0474\n",
      "Finished.\n",
      "Total time per fold: 1767.0854337215424 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Variables to store fold scores\n",
    "train_acc = []\n",
    "test_top1_acc = []\n",
    "test_top5_acc = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "times = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(splits.split(total_set)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('Fold : {}'.format(fold))\n",
    "    \n",
    "    # Train and val samplers\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    print(\"Samples in training:\", len(train_sampler))\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    print(\"Samples in test:\", len(valid_sampler))\n",
    "    \n",
    "    # Train and val loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=train_batch_size, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=1, sampler=valid_sampler)\n",
    "    \n",
    "    device = get_device()\n",
    "    \n",
    "    criterion, model, optimizer = create_optimizer(load_model())\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(h_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        trunning_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += (preds == labels).sum()\n",
    "            trunning_corrects += preds.size(0)\n",
    "            \n",
    "\n",
    "        epoch_loss = running_loss / trunning_corrects\n",
    "        epoch_acc = (running_corrects.double()*100) / trunning_corrects\n",
    "        train_acc.append(epoch_acc.item())\n",
    "        \n",
    "        print('\\t\\t Training: Epoch({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        model.eval()  \n",
    "        \n",
    "        vrunning_loss = 0.0\n",
    "        vrunning_corrects = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for data, labels in valid_loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(data)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            vrunning_loss += loss.item() * data.size(0)\n",
    "            vrunning_corrects += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            \n",
    "        vepoch_loss = vrunning_loss/num_samples\n",
    "        vepoch_acc = (vrunning_corrects.double() * 100)/num_samples\n",
    "        \n",
    "        print('\\t\\t Validation({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, vepoch_loss, vepoch_acc))\n",
    "    \n",
    "    # Calculating and appending scores to this fold\n",
    "    model.class_to_idx = total_set.class_to_idx\n",
    "    scores = get_scores(model, valid_loader)\n",
    "    \n",
    "    test_top1_acc.append(scores[0])\n",
    "    test_top5_acc.append(scores[1])\n",
    "    test_precision.append(scores[2])\n",
    "    test_recall.append(scores[3])\n",
    "    test_f1.append(scores[4])\n",
    "    \n",
    "    time_fold = time.time() - start_time\n",
    "    times.append(time_fold)\n",
    "    print(\"Total time per fold: %s seconds.\" %(time_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy average:  0.9733612703963118\n",
      "Top-1 test accuracy average:  0.9738298077175239\n",
      "Top-5 test accuracy average:  0.9977983304509136\n",
      "Weighted Precision test accuracy average:  0.974598242942422\n",
      "Weighted Recall test accuracy average:  0.9738298077175239\n",
      "Weighted F1 test accuracy average:  0.9738621522654076\n",
      "Average time per fold (seconds): 1816.0572626829148\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy average: \", np.mean(train_acc) / 100)\n",
    "print(\"Top-1 test accuracy average: \", np.mean(test_top1_acc))\n",
    "print(\"Top-5 test accuracy average: \", np.mean(test_top5_acc))\n",
    "print(\"Weighted Precision test accuracy average: \", np.mean(test_precision))\n",
    "print(\"Weighted Recall test accuracy average: \", np.mean(test_recall))\n",
    "print(\"Weighted F1 test accuracy average: \", np.mean(test_f1))\n",
    "print(\"Average time per fold (seconds):\", np.mean(times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
