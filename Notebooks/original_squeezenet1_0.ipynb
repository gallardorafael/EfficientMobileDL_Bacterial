{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the paper: \n",
    "\n",
    "## Efficient and Mobile Deep Learning Architectures for Fast Identification of BacterialStrains in Resource-Constrained Devices\n",
    "\n",
    "### Architecture: SqueezeNet 1.0\n",
    "### Data: Original + Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, models\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "# Archs not in Pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# External functions\n",
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "3.8.5 (default, Jul 28 2020, 12:59:40) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data paths and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters and dataset details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset details\n",
    "dataset_version = 'original' # original or augmented\n",
    "img_shape = (224,224)\n",
    "img_size = str(img_shape[0])+\"x\"+str(img_shape[1])\n",
    "\n",
    "# Root directory of dataset\n",
    "data_dir = '/home/yibbtstll/venvs/pytorch_gpu/CySDeepBacterial/Dataset/DIBaS/'\n",
    "\n",
    "train_batch_size = 32\n",
    "val_test_batch_size = 32\n",
    "feature_extract = False\n",
    "pretrained = True\n",
    "h_epochs = 15\n",
    "kfolds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining transforms and creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for input data\n",
    "training_transforms = transforms.Compose([transforms.Resize((224,224), Image.LANCZOS),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "total_set = datasets.ImageFolder(data_dir, transform=training_transforms)\n",
    "\n",
    "# Defining folds\n",
    "splits = KFold(n_splits = kfolds, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the target classes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "{0: 'Acinetobacter.baumanii', 1: 'Actinomyces.israeli', 2: 'Bacteroides.fragilis', 3: 'Bifidobacterium.spp', 4: 'Clostridium.perfringens', 5: 'Enterococcus.faecalis', 6: 'Enterococcus.faecium', 7: 'Escherichia.coli', 8: 'Fusobacterium', 9: 'Lactobacillus.casei', 10: 'Lactobacillus.crispatus', 11: 'Lactobacillus.delbrueckii', 12: 'Lactobacillus.gasseri', 13: 'Lactobacillus.jehnsenii', 14: 'Lactobacillus.johnsonii', 15: 'Lactobacillus.paracasei', 16: 'Lactobacillus.plantarum', 17: 'Lactobacillus.reuteri', 18: 'Lactobacillus.rhamnosus', 19: 'Lactobacillus.salivarius', 20: 'Listeria.monocytogenes', 21: 'Micrococcus.spp', 22: 'Neisseria.gonorrhoeae', 23: 'Porfyromonas.gingivalis', 24: 'Propionibacterium.acnes', 25: 'Proteus', 26: 'Pseudomonas.aeruginosa', 27: 'Staphylococcus.aureus', 28: 'Staphylococcus.epidermidis', 29: 'Staphylococcus.saprophiticus', 30: 'Streptococcus.agalactiae', 31: 'Veionella'}\n"
     ]
    }
   ],
   "source": [
    "train_labels = {value : key for (key, value) in total_set.class_to_idx.items()}\n",
    "    \n",
    "print(len(train_labels)) \n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and inicialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing pre-trained parameters, finetunning the classifier to output 32 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained model parameters to avoid backpropogating through them\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        print(\"Setting grad to false.\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_device():\n",
    "    # Model and criterion to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.squeezenet1_0(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Transfer Learning\n",
    "    model = models.squeezenet1_0(pretrained=pretrained)\n",
    "    \n",
    "    # Mode\n",
    "    model = set_parameter_requires_grad(model, feature_extract)\n",
    "    \n",
    "    # Fine tuning\n",
    "    # Build custom classifier\n",
    "    # Build custom classifier\n",
    "    model.classifier[1] = nn.Conv2d(512, 32, \n",
    "                                kernel_size=(1,1), stride=(1,1))\n",
    "    return model\n",
    "\n",
    "def create_optimizer(model):\n",
    "    # Parameters to update\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "\n",
    "    else:\n",
    "        n_params = 0\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                n_params += 1\n",
    "\n",
    "\n",
    "    # Loss function and gradient descent\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(params_to_update, \n",
    "                          lr=0.001, \n",
    "                          weight_decay=0.000004)\n",
    "    \n",
    "    return criterion.to(get_device()), model.to(get_device()), optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.4847, Acc: 4.6512\n",
      "\t\t Validation(0) - Loss: 3.4039, Acc: 4.4776\n",
      "\t\t Training: Epoch(1) - Loss: 3.4062, Acc: 2.8239\n",
      "\t\t Validation(1) - Loss: 3.4026, Acc: 0.0000\n",
      "\t\t Training: Epoch(2) - Loss: 3.2047, Acc: 7.8073\n",
      "\t\t Validation(2) - Loss: 3.1388, Acc: 13.4328\n",
      "\t\t Training: Epoch(3) - Loss: 3.0573, Acc: 10.4651\n",
      "\t\t Validation(3) - Loss: 3.3264, Acc: 5.9701\n",
      "\t\t Training: Epoch(4) - Loss: 2.8191, Acc: 13.6213\n",
      "\t\t Validation(4) - Loss: 2.4820, Acc: 17.9104\n",
      "\t\t Training: Epoch(5) - Loss: 2.3817, Acc: 20.4319\n",
      "\t\t Validation(5) - Loss: 2.2403, Acc: 28.3582\n",
      "\t\t Training: Epoch(6) - Loss: 2.2306, Acc: 24.2525\n",
      "\t\t Validation(6) - Loss: 2.5579, Acc: 13.4328\n",
      "\t\t Training: Epoch(7) - Loss: 1.9775, Acc: 30.5648\n",
      "\t\t Validation(7) - Loss: 1.8254, Acc: 29.8507\n",
      "\t\t Training: Epoch(8) - Loss: 1.8629, Acc: 32.3920\n",
      "\t\t Validation(8) - Loss: 1.8080, Acc: 28.3582\n",
      "\t\t Training: Epoch(9) - Loss: 1.6189, Acc: 39.0365\n",
      "\t\t Validation(9) - Loss: 1.4058, Acc: 49.2537\n",
      "\t\t Training: Epoch(10) - Loss: 1.6832, Acc: 35.7143\n",
      "\t\t Validation(10) - Loss: 1.7052, Acc: 34.3284\n",
      "\t\t Training: Epoch(11) - Loss: 1.5345, Acc: 39.2027\n",
      "\t\t Validation(11) - Loss: 1.3658, Acc: 47.7612\n",
      "\t\t Training: Epoch(12) - Loss: 1.4226, Acc: 46.6777\n",
      "\t\t Validation(12) - Loss: 1.5309, Acc: 38.8060\n",
      "\t\t Training: Epoch(13) - Loss: 1.4992, Acc: 43.6877\n",
      "\t\t Validation(13) - Loss: 1.5908, Acc: 35.8209\n",
      "\t\t Training: Epoch(14) - Loss: 1.3609, Acc: 45.1827\n",
      "\t\t Validation(14) - Loss: 1.3307, Acc: 38.8060\n",
      "Finished.\n",
      "Total time per fold: 345.3527879714966 seconds.\n",
      "Fold : 1\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.4691, Acc: 3.8206\n",
      "\t\t Validation(0) - Loss: 3.3064, Acc: 5.9701\n",
      "\t\t Training: Epoch(1) - Loss: 3.2403, Acc: 8.1395\n",
      "\t\t Validation(1) - Loss: 3.0443, Acc: 7.4627\n",
      "\t\t Training: Epoch(2) - Loss: 3.0138, Acc: 11.6279\n",
      "\t\t Validation(2) - Loss: 3.0884, Acc: 4.4776\n",
      "\t\t Training: Epoch(3) - Loss: 2.9043, Acc: 11.1296\n",
      "\t\t Validation(3) - Loss: 2.6319, Acc: 19.4030\n",
      "\t\t Training: Epoch(4) - Loss: 2.5719, Acc: 16.2791\n",
      "\t\t Validation(4) - Loss: 2.0918, Acc: 26.8657\n",
      "\t\t Training: Epoch(5) - Loss: 2.2856, Acc: 18.4385\n",
      "\t\t Validation(5) - Loss: 1.9917, Acc: 34.3284\n",
      "\t\t Training: Epoch(6) - Loss: 2.1442, Acc: 24.7508\n",
      "\t\t Validation(6) - Loss: 1.9502, Acc: 31.3433\n",
      "\t\t Training: Epoch(7) - Loss: 2.0092, Acc: 25.7475\n",
      "\t\t Validation(7) - Loss: 1.8132, Acc: 40.2985\n",
      "\t\t Training: Epoch(8) - Loss: 1.9087, Acc: 29.2359\n",
      "\t\t Validation(8) - Loss: 1.9386, Acc: 34.3284\n",
      "\t\t Training: Epoch(9) - Loss: 1.6786, Acc: 32.7243\n",
      "\t\t Validation(9) - Loss: 1.6262, Acc: 37.3134\n",
      "\t\t Training: Epoch(10) - Loss: 1.7954, Acc: 32.2259\n",
      "\t\t Validation(10) - Loss: 1.6734, Acc: 41.7910\n",
      "\t\t Training: Epoch(11) - Loss: 1.5646, Acc: 39.2027\n",
      "\t\t Validation(11) - Loss: 1.4809, Acc: 41.7910\n",
      "\t\t Training: Epoch(12) - Loss: 1.4641, Acc: 41.5282\n",
      "\t\t Validation(12) - Loss: 1.6565, Acc: 31.3433\n",
      "\t\t Training: Epoch(13) - Loss: 1.4247, Acc: 45.1827\n",
      "\t\t Validation(13) - Loss: 1.4788, Acc: 43.2836\n",
      "\t\t Training: Epoch(14) - Loss: 1.4734, Acc: 43.8538\n",
      "\t\t Validation(14) - Loss: 1.4349, Acc: 46.2687\n",
      "Finished.\n",
      "Total time per fold: 342.5947210788727 seconds.\n",
      "Fold : 2\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.4757, Acc: 4.6512\n",
      "\t\t Validation(0) - Loss: 3.3949, Acc: 5.9701\n",
      "\t\t Training: Epoch(1) - Loss: 3.3621, Acc: 6.6445\n",
      "\t\t Validation(1) - Loss: 3.4368, Acc: 8.9552\n",
      "\t\t Training: Epoch(2) - Loss: 3.2580, Acc: 7.1429\n",
      "\t\t Validation(2) - Loss: 3.1654, Acc: 11.9403\n",
      "\t\t Training: Epoch(3) - Loss: 2.8726, Acc: 13.4551\n",
      "\t\t Validation(3) - Loss: 3.0241, Acc: 19.4030\n",
      "\t\t Training: Epoch(4) - Loss: 2.6370, Acc: 15.9468\n",
      "\t\t Validation(4) - Loss: 2.9190, Acc: 10.4478\n",
      "\t\t Training: Epoch(5) - Loss: 2.4720, Acc: 19.1030\n",
      "\t\t Validation(5) - Loss: 2.5782, Acc: 23.8806\n",
      "\t\t Training: Epoch(6) - Loss: 2.1821, Acc: 25.7475\n",
      "\t\t Validation(6) - Loss: 2.2722, Acc: 26.8657\n",
      "\t\t Training: Epoch(7) - Loss: 2.0576, Acc: 26.7442\n",
      "\t\t Validation(7) - Loss: 2.2608, Acc: 22.3881\n",
      "\t\t Training: Epoch(8) - Loss: 1.7414, Acc: 35.7143\n",
      "\t\t Validation(8) - Loss: 2.1334, Acc: 34.3284\n",
      "\t\t Training: Epoch(9) - Loss: 1.6616, Acc: 37.0432\n",
      "\t\t Validation(9) - Loss: 1.8010, Acc: 34.3284\n",
      "\t\t Training: Epoch(10) - Loss: 1.5819, Acc: 40.0332\n",
      "\t\t Validation(10) - Loss: 2.5010, Acc: 23.8806\n",
      "\t\t Training: Epoch(11) - Loss: 1.8414, Acc: 35.5482\n",
      "\t\t Validation(11) - Loss: 2.4141, Acc: 28.3582\n",
      "\t\t Training: Epoch(12) - Loss: 1.6784, Acc: 39.8671\n",
      "\t\t Validation(12) - Loss: 1.8279, Acc: 34.3284\n",
      "\t\t Training: Epoch(13) - Loss: 1.4702, Acc: 46.1794\n",
      "\t\t Validation(13) - Loss: 1.4530, Acc: 43.2836\n",
      "\t\t Training: Epoch(14) - Loss: 1.3339, Acc: 49.1694\n",
      "\t\t Validation(14) - Loss: 1.6615, Acc: 46.2687\n",
      "Finished.\n",
      "Total time per fold: 337.99619030952454 seconds.\n",
      "Fold : 3\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.5149, Acc: 3.6545\n",
      "\t\t Validation(0) - Loss: 3.4305, Acc: 5.9701\n",
      "\t\t Training: Epoch(1) - Loss: 3.4609, Acc: 3.9867\n",
      "\t\t Validation(1) - Loss: 3.4445, Acc: 2.9851\n",
      "\t\t Training: Epoch(2) - Loss: 3.4152, Acc: 2.9900\n",
      "\t\t Validation(2) - Loss: 3.4039, Acc: 2.9851\n",
      "\t\t Training: Epoch(3) - Loss: 3.2443, Acc: 7.4751\n",
      "\t\t Validation(3) - Loss: 3.2743, Acc: 8.9552\n",
      "\t\t Training: Epoch(4) - Loss: 2.9896, Acc: 12.7907\n",
      "\t\t Validation(4) - Loss: 2.9313, Acc: 10.4478\n",
      "\t\t Training: Epoch(5) - Loss: 2.7659, Acc: 14.1196\n",
      "\t\t Validation(5) - Loss: 2.8733, Acc: 11.9403\n",
      "\t\t Training: Epoch(6) - Loss: 2.5128, Acc: 15.7807\n",
      "\t\t Validation(6) - Loss: 2.4403, Acc: 14.9254\n",
      "\t\t Training: Epoch(7) - Loss: 2.3550, Acc: 19.1030\n",
      "\t\t Validation(7) - Loss: 2.3082, Acc: 22.3881\n",
      "\t\t Training: Epoch(8) - Loss: 2.1915, Acc: 18.6047\n",
      "\t\t Validation(8) - Loss: 2.0287, Acc: 32.8358\n",
      "\t\t Training: Epoch(9) - Loss: 2.0236, Acc: 25.2492\n",
      "\t\t Validation(9) - Loss: 2.2968, Acc: 23.8806\n",
      "\t\t Training: Epoch(10) - Loss: 2.0332, Acc: 25.2492\n",
      "\t\t Validation(10) - Loss: 1.8377, Acc: 32.8358\n",
      "\t\t Training: Epoch(11) - Loss: 1.7286, Acc: 35.3821\n",
      "\t\t Validation(11) - Loss: 1.7994, Acc: 31.3433\n",
      "\t\t Training: Epoch(12) - Loss: 1.6946, Acc: 34.5515\n",
      "\t\t Validation(12) - Loss: 2.0293, Acc: 35.8209\n",
      "\t\t Training: Epoch(13) - Loss: 1.6591, Acc: 36.3787\n",
      "\t\t Validation(13) - Loss: 1.5307, Acc: 34.3284\n",
      "\t\t Training: Epoch(14) - Loss: 1.4988, Acc: 37.7076\n",
      "\t\t Validation(14) - Loss: 1.6601, Acc: 37.3134\n",
      "Finished.\n",
      "Total time per fold: 336.3998317718506 seconds.\n",
      "Fold : 4\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.5057, Acc: 2.8239\n",
      "\t\t Validation(0) - Loss: 3.4761, Acc: 2.9851\n",
      "\t\t Training: Epoch(1) - Loss: 3.4282, Acc: 5.3156\n",
      "\t\t Validation(1) - Loss: 3.5899, Acc: 4.4776\n",
      "\t\t Training: Epoch(2) - Loss: 3.2870, Acc: 7.3090\n",
      "\t\t Validation(2) - Loss: 3.3022, Acc: 4.4776\n",
      "\t\t Training: Epoch(3) - Loss: 3.1566, Acc: 7.3090\n",
      "\t\t Validation(3) - Loss: 3.3464, Acc: 2.9851\n",
      "\t\t Training: Epoch(4) - Loss: 3.3286, Acc: 5.3156\n",
      "\t\t Validation(4) - Loss: 3.1930, Acc: 5.9701\n",
      "\t\t Training: Epoch(5) - Loss: 3.1744, Acc: 6.6445\n",
      "\t\t Validation(5) - Loss: 3.1674, Acc: 4.4776\n",
      "\t\t Training: Epoch(6) - Loss: 3.0675, Acc: 5.4817\n",
      "\t\t Validation(6) - Loss: 3.1547, Acc: 11.9403\n",
      "\t\t Training: Epoch(7) - Loss: 3.0068, Acc: 6.6445\n",
      "\t\t Validation(7) - Loss: 3.1504, Acc: 7.4627\n",
      "\t\t Training: Epoch(8) - Loss: 2.9355, Acc: 6.1462\n",
      "\t\t Validation(8) - Loss: 2.9872, Acc: 7.4627\n",
      "\t\t Training: Epoch(9) - Loss: 2.8605, Acc: 10.6312\n",
      "\t\t Validation(9) - Loss: 3.0316, Acc: 2.9851\n",
      "\t\t Training: Epoch(10) - Loss: 2.8374, Acc: 10.2990\n",
      "\t\t Validation(10) - Loss: 2.9293, Acc: 8.9552\n",
      "\t\t Training: Epoch(11) - Loss: 2.8199, Acc: 12.4585\n",
      "\t\t Validation(11) - Loss: 2.8828, Acc: 10.4478\n",
      "\t\t Training: Epoch(12) - Loss: 2.7459, Acc: 12.1262\n",
      "\t\t Validation(12) - Loss: 2.7934, Acc: 10.4478\n",
      "\t\t Training: Epoch(13) - Loss: 2.6300, Acc: 15.7807\n",
      "\t\t Validation(13) - Loss: 2.7244, Acc: 11.9403\n",
      "\t\t Training: Epoch(14) - Loss: 2.6957, Acc: 13.1229\n",
      "\t\t Validation(14) - Loss: 2.6870, Acc: 13.4328\n",
      "Finished.\n",
      "Total time per fold: 337.48518538475037 seconds.\n",
      "Fold : 5\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.4602, Acc: 4.1528\n",
      "\t\t Validation(0) - Loss: 3.3970, Acc: 7.4627\n",
      "\t\t Training: Epoch(1) - Loss: 3.2383, Acc: 7.6412\n",
      "\t\t Validation(1) - Loss: 3.0148, Acc: 7.4627\n",
      "\t\t Training: Epoch(2) - Loss: 2.9945, Acc: 11.6279\n",
      "\t\t Validation(2) - Loss: 3.0035, Acc: 16.4179\n",
      "\t\t Training: Epoch(3) - Loss: 2.7353, Acc: 17.2757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Validation(3) - Loss: 2.0625, Acc: 28.3582\n",
      "\t\t Training: Epoch(4) - Loss: 2.3510, Acc: 21.9269\n",
      "\t\t Validation(4) - Loss: 1.8915, Acc: 28.3582\n",
      "\t\t Training: Epoch(5) - Loss: 2.0052, Acc: 26.4120\n",
      "\t\t Validation(5) - Loss: 1.9753, Acc: 23.8806\n",
      "\t\t Training: Epoch(6) - Loss: 1.8291, Acc: 32.2259\n",
      "\t\t Validation(6) - Loss: 1.6705, Acc: 41.7910\n",
      "\t\t Training: Epoch(7) - Loss: 1.7368, Acc: 36.3787\n",
      "\t\t Validation(7) - Loss: 1.8372, Acc: 40.2985\n",
      "\t\t Training: Epoch(8) - Loss: 1.7572, Acc: 38.2060\n",
      "\t\t Validation(8) - Loss: 1.8715, Acc: 32.8358\n",
      "\t\t Training: Epoch(9) - Loss: 1.6068, Acc: 39.7010\n",
      "\t\t Validation(9) - Loss: 1.4952, Acc: 38.8060\n",
      "\t\t Training: Epoch(10) - Loss: 1.4238, Acc: 42.8571\n",
      "\t\t Validation(10) - Loss: 1.8066, Acc: 34.3284\n",
      "\t\t Training: Epoch(11) - Loss: 1.6156, Acc: 37.8738\n",
      "\t\t Validation(11) - Loss: 1.6634, Acc: 37.3134\n",
      "\t\t Training: Epoch(12) - Loss: 1.2465, Acc: 51.1628\n",
      "\t\t Validation(12) - Loss: 1.2889, Acc: 47.7612\n",
      "\t\t Training: Epoch(13) - Loss: 1.0880, Acc: 57.8073\n",
      "\t\t Validation(13) - Loss: 1.5094, Acc: 44.7761\n",
      "\t\t Training: Epoch(14) - Loss: 1.1905, Acc: 53.8206\n",
      "\t\t Validation(14) - Loss: 1.5515, Acc: 41.7910\n",
      "Finished.\n",
      "Total time per fold: 343.28931045532227 seconds.\n",
      "Fold : 6\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.4875, Acc: 3.4884\n",
      "\t\t Validation(0) - Loss: 3.3715, Acc: 2.9851\n",
      "\t\t Training: Epoch(1) - Loss: 3.3369, Acc: 6.3123\n",
      "\t\t Validation(1) - Loss: 3.2334, Acc: 4.4776\n",
      "\t\t Training: Epoch(2) - Loss: 3.1842, Acc: 7.3090\n",
      "\t\t Validation(2) - Loss: 2.9365, Acc: 13.4328\n",
      "\t\t Training: Epoch(3) - Loss: 2.9592, Acc: 9.1362\n",
      "\t\t Validation(3) - Loss: 2.9071, Acc: 7.4627\n",
      "\t\t Training: Epoch(4) - Loss: 2.8223, Acc: 11.2957\n",
      "\t\t Validation(4) - Loss: 2.5035, Acc: 22.3881\n",
      "\t\t Training: Epoch(5) - Loss: 2.5337, Acc: 17.6080\n",
      "\t\t Validation(5) - Loss: 2.1561, Acc: 35.8209\n",
      "\t\t Training: Epoch(6) - Loss: 2.0647, Acc: 26.4120\n",
      "\t\t Validation(6) - Loss: 1.6704, Acc: 35.8209\n",
      "\t\t Training: Epoch(7) - Loss: 1.8440, Acc: 31.2292\n",
      "\t\t Validation(7) - Loss: 1.9602, Acc: 34.3284\n",
      "\t\t Training: Epoch(8) - Loss: 1.8802, Acc: 32.0598\n",
      "\t\t Validation(8) - Loss: 1.6617, Acc: 34.3284\n",
      "\t\t Training: Epoch(9) - Loss: 1.6470, Acc: 36.3787\n",
      "\t\t Validation(9) - Loss: 1.5465, Acc: 44.7761\n",
      "\t\t Training: Epoch(10) - Loss: 1.5899, Acc: 39.2027\n",
      "\t\t Validation(10) - Loss: 1.5891, Acc: 38.8060\n",
      "\t\t Training: Epoch(11) - Loss: 1.5791, Acc: 39.8671\n",
      "\t\t Validation(11) - Loss: 1.2916, Acc: 55.2239\n",
      "\t\t Training: Epoch(12) - Loss: 1.4032, Acc: 47.1761\n",
      "\t\t Validation(12) - Loss: 1.5496, Acc: 34.3284\n",
      "\t\t Training: Epoch(13) - Loss: 1.4055, Acc: 44.3522\n",
      "\t\t Validation(13) - Loss: 1.2915, Acc: 46.2687\n",
      "\t\t Training: Epoch(14) - Loss: 1.0984, Acc: 54.6512\n",
      "\t\t Validation(14) - Loss: 1.0859, Acc: 61.1940\n",
      "Finished.\n",
      "Total time per fold: 336.9374041557312 seconds.\n",
      "Fold : 7\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.5194, Acc: 3.6545\n",
      "\t\t Validation(0) - Loss: 3.4574, Acc: 0.0000\n",
      "\t\t Training: Epoch(1) - Loss: 3.4083, Acc: 5.4817\n",
      "\t\t Validation(1) - Loss: 3.3941, Acc: 0.0000\n",
      "\t\t Training: Epoch(2) - Loss: 3.2935, Acc: 5.4817\n",
      "\t\t Validation(2) - Loss: 3.3664, Acc: 2.9851\n",
      "\t\t Training: Epoch(3) - Loss: 3.1812, Acc: 7.4751\n",
      "\t\t Validation(3) - Loss: 3.2895, Acc: 5.9701\n",
      "\t\t Training: Epoch(4) - Loss: 3.0228, Acc: 7.1429\n",
      "\t\t Validation(4) - Loss: 3.2301, Acc: 2.9851\n",
      "\t\t Training: Epoch(5) - Loss: 2.8687, Acc: 9.9668\n",
      "\t\t Validation(5) - Loss: 3.0909, Acc: 4.4776\n",
      "\t\t Training: Epoch(6) - Loss: 2.8187, Acc: 10.7973\n",
      "\t\t Validation(6) - Loss: 2.8966, Acc: 10.4478\n",
      "\t\t Training: Epoch(7) - Loss: 2.6257, Acc: 13.1229\n",
      "\t\t Validation(7) - Loss: 2.8008, Acc: 7.4627\n",
      "\t\t Training: Epoch(8) - Loss: 2.4578, Acc: 16.7774\n",
      "\t\t Validation(8) - Loss: 2.5602, Acc: 17.9104\n",
      "\t\t Training: Epoch(9) - Loss: 2.2586, Acc: 22.7575\n",
      "\t\t Validation(9) - Loss: 2.5504, Acc: 11.9403\n",
      "\t\t Training: Epoch(10) - Loss: 2.2682, Acc: 20.5980\n",
      "\t\t Validation(10) - Loss: 2.3818, Acc: 19.4030\n",
      "\t\t Training: Epoch(11) - Loss: 2.1701, Acc: 22.0930\n",
      "\t\t Validation(11) - Loss: 2.0747, Acc: 23.8806\n",
      "\t\t Training: Epoch(12) - Loss: 1.8719, Acc: 31.0631\n",
      "\t\t Validation(12) - Loss: 1.9298, Acc: 25.3731\n",
      "\t\t Training: Epoch(13) - Loss: 1.7978, Acc: 35.0498\n",
      "\t\t Validation(13) - Loss: 2.3564, Acc: 19.4030\n",
      "\t\t Training: Epoch(14) - Loss: 2.1107, Acc: 25.5814\n",
      "\t\t Validation(14) - Loss: 2.0817, Acc: 25.3731\n",
      "Finished.\n",
      "Total time per fold: 340.3938190937042 seconds.\n",
      "Fold : 8\n",
      "Samples in training: 602\n",
      "Samples in test: 67\n",
      "\t\t Training: Epoch(0) - Loss: 3.4169, Acc: 6.3123\n",
      "\t\t Validation(0) - Loss: 3.2224, Acc: 16.4179\n",
      "\t\t Training: Epoch(1) - Loss: 3.1827, Acc: 10.7973\n",
      "\t\t Validation(1) - Loss: 3.1131, Acc: 10.4478\n",
      "\t\t Training: Epoch(2) - Loss: 2.9165, Acc: 12.6246\n",
      "\t\t Validation(2) - Loss: 2.7641, Acc: 17.9104\n",
      "\t\t Training: Epoch(3) - Loss: 2.5570, Acc: 20.9302\n",
      "\t\t Validation(3) - Loss: 2.4247, Acc: 19.4030\n",
      "\t\t Training: Epoch(4) - Loss: 2.3750, Acc: 26.5781\n",
      "\t\t Validation(4) - Loss: 2.2542, Acc: 19.4030\n",
      "\t\t Training: Epoch(5) - Loss: 1.9580, Acc: 32.0598\n",
      "\t\t Validation(5) - Loss: 1.9093, Acc: 34.3284\n",
      "\t\t Training: Epoch(6) - Loss: 1.7817, Acc: 38.0399\n",
      "\t\t Validation(6) - Loss: 1.7788, Acc: 37.3134\n",
      "\t\t Training: Epoch(7) - Loss: 1.6011, Acc: 43.6877\n",
      "\t\t Validation(7) - Loss: 1.1768, Acc: 56.7164\n",
      "\t\t Training: Epoch(8) - Loss: 1.1038, Acc: 54.6512\n",
      "\t\t Validation(8) - Loss: 1.1826, Acc: 56.7164\n",
      "\t\t Training: Epoch(9) - Loss: 1.3956, Acc: 48.3389\n",
      "\t\t Validation(9) - Loss: 1.4484, Acc: 43.2836\n",
      "\t\t Training: Epoch(10) - Loss: 1.1997, Acc: 53.4884\n",
      "\t\t Validation(10) - Loss: 0.8870, Acc: 70.1493\n",
      "\t\t Training: Epoch(11) - Loss: 0.8225, Acc: 68.2724\n",
      "\t\t Validation(11) - Loss: 0.9623, Acc: 64.1791\n",
      "\t\t Training: Epoch(12) - Loss: 0.7058, Acc: 71.9269\n",
      "\t\t Validation(12) - Loss: 1.0886, Acc: 59.7015\n",
      "\t\t Training: Epoch(13) - Loss: 1.0082, Acc: 63.2890\n",
      "\t\t Validation(13) - Loss: 1.0405, Acc: 64.1791\n",
      "\t\t Training: Epoch(14) - Loss: 0.9262, Acc: 66.2791\n",
      "\t\t Validation(14) - Loss: 0.6886, Acc: 74.6269\n",
      "Finished.\n",
      "Total time per fold: 338.6892876625061 seconds.\n",
      "Fold : 9\n",
      "Samples in training: 603\n",
      "Samples in test: 66\n",
      "\t\t Training: Epoch(0) - Loss: 3.3893, Acc: 6.1360\n",
      "\t\t Validation(0) - Loss: 3.2296, Acc: 6.0606\n",
      "\t\t Training: Epoch(1) - Loss: 3.1666, Acc: 9.7844\n",
      "\t\t Validation(1) - Loss: 3.0832, Acc: 9.0909\n",
      "\t\t Training: Epoch(2) - Loss: 2.8956, Acc: 14.4279\n",
      "\t\t Validation(2) - Loss: 3.0133, Acc: 6.0606\n",
      "\t\t Training: Epoch(3) - Loss: 2.6746, Acc: 15.4229\n",
      "\t\t Validation(3) - Loss: 2.6667, Acc: 13.6364\n",
      "\t\t Training: Epoch(4) - Loss: 2.3725, Acc: 20.2322\n",
      "\t\t Validation(4) - Loss: 2.1933, Acc: 24.2424\n",
      "\t\t Training: Epoch(5) - Loss: 2.0629, Acc: 28.1924\n",
      "\t\t Validation(5) - Loss: 2.0177, Acc: 22.7273\n",
      "\t\t Training: Epoch(6) - Loss: 1.8871, Acc: 33.3333\n",
      "\t\t Validation(6) - Loss: 1.8245, Acc: 39.3939\n",
      "\t\t Training: Epoch(7) - Loss: 1.7533, Acc: 38.4743\n",
      "\t\t Validation(7) - Loss: 1.6560, Acc: 34.8485\n",
      "\t\t Training: Epoch(8) - Loss: 1.5188, Acc: 44.1128\n",
      "\t\t Validation(8) - Loss: 1.3552, Acc: 56.0606\n",
      "\t\t Training: Epoch(9) - Loss: 1.5091, Acc: 43.7811\n",
      "\t\t Validation(9) - Loss: 1.2784, Acc: 51.5152\n",
      "\t\t Training: Epoch(10) - Loss: 1.4549, Acc: 46.4345\n",
      "\t\t Validation(10) - Loss: 1.3035, Acc: 43.9394\n",
      "\t\t Training: Epoch(11) - Loss: 1.2526, Acc: 52.4046\n",
      "\t\t Validation(11) - Loss: 1.7632, Acc: 36.3636\n",
      "\t\t Training: Epoch(12) - Loss: 1.2535, Acc: 52.7363\n",
      "\t\t Validation(12) - Loss: 0.8271, Acc: 66.6667\n",
      "\t\t Training: Epoch(13) - Loss: 1.1914, Acc: 54.7264\n",
      "\t\t Validation(13) - Loss: 1.2284, Acc: 54.5455\n",
      "\t\t Training: Epoch(14) - Loss: 1.2181, Acc: 56.2189\n",
      "\t\t Validation(14) - Loss: 0.8851, Acc: 63.6364\n",
      "Finished.\n",
      "Total time per fold: 342.74838280677795 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Variables to store fold scores\n",
    "train_acc = []\n",
    "test_top1_acc = []\n",
    "test_top5_acc = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "times = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(splits.split(total_set)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('Fold : {}'.format(fold))\n",
    "    \n",
    "    # Train and val samplers\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    print(\"Samples in training:\", len(train_sampler))\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    print(\"Samples in test:\", len(valid_sampler))\n",
    "    \n",
    "    # Train and val loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=train_batch_size, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=1, sampler=valid_sampler)\n",
    "    \n",
    "    device = get_device()\n",
    "    \n",
    "    criterion, model, optimizer = create_optimizer(load_model())\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(h_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        trunning_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += (preds == labels).sum()\n",
    "            trunning_corrects += preds.size(0)\n",
    "            \n",
    "\n",
    "        epoch_loss = running_loss / trunning_corrects\n",
    "        epoch_acc = (running_corrects.double()*100) / trunning_corrects\n",
    "        train_acc.append(epoch_acc.item())\n",
    "        \n",
    "        print('\\t\\t Training: Epoch({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        model.eval()  \n",
    "        \n",
    "        vrunning_loss = 0.0\n",
    "        vrunning_corrects = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for data, labels in valid_loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(data)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            vrunning_loss += loss.item() * data.size(0)\n",
    "            vrunning_corrects += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            \n",
    "        vepoch_loss = vrunning_loss/num_samples\n",
    "        vepoch_acc = (vrunning_corrects.double() * 100)/num_samples\n",
    "        \n",
    "        print('\\t\\t Validation({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, vepoch_loss, vepoch_acc))\n",
    "    \n",
    "    # Calculating and appending scores to this fold\n",
    "    model.class_to_idx = total_set.class_to_idx\n",
    "    scores = get_scores(model, valid_loader)\n",
    "    \n",
    "    test_top1_acc.append(scores[0])\n",
    "    test_top5_acc.append(scores[1])\n",
    "    test_precision.append(scores[2])\n",
    "    test_recall.append(scores[3])\n",
    "    test_f1.append(scores[4])\n",
    "    \n",
    "    time_fold = time.time() - start_time\n",
    "    times.append(time_fold)\n",
    "    print(\"Total time per fold: %s seconds.\" %(time_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy average:  0.25734037453926384\n",
      "Top-1 test accuracy average:  0.44871099050203533\n",
      "Top-5 test accuracy average:  0.8895296246042514\n",
      "Weighted Precision test accuracy average:  0.4366966698309982\n",
      "Weighted Recall test accuracy average:  0.44871099050203533\n",
      "Weighted F1 test accuracy average:  0.40472946208020844\n",
      "Average time per fold (seconds): 340.1886920690537\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy average: \", np.mean(train_acc) / 100)\n",
    "print(\"Top-1 test accuracy average: \", np.mean(test_top1_acc))\n",
    "print(\"Top-5 test accuracy average: \", np.mean(test_top5_acc))\n",
    "print(\"Weighted Precision test accuracy average: \", np.mean(test_precision))\n",
    "print(\"Weighted Recall test accuracy average: \", np.mean(test_recall))\n",
    "print(\"Weighted F1 test accuracy average: \", np.mean(test_f1))\n",
    "print(\"Average time per fold (seconds):\", np.mean(times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
