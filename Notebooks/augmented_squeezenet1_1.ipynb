{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the paper: \n",
    "\n",
    "## Efficient and Mobile Deep Learning Architectures for Fast Identification of BacterialStrains in Resource-Constrained Devices\n",
    "\n",
    "### Architecture: SqueezeNet 1.1\n",
    "### Data: Augmented + Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, models\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "# Archs not in Pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# External functions\n",
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "3.8.5 (default, Jul 28 2020, 12:59:40) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data paths and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters and dataset details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset details\n",
    "dataset_version = 'original' # original or augmented\n",
    "img_shape = (224,224)\n",
    "img_size = str(img_shape[0])+\"x\"+str(img_shape[1])\n",
    "\n",
    "# Root directory of dataset\n",
    "data_dir = '/home/yibbtstll/venvs/pytorch_gpu/CySDeepBacterial/Dataset/DIBaS_augmented/'\n",
    "\n",
    "train_batch_size = 64\n",
    "val_test_batch_size = 32\n",
    "feature_extract = False\n",
    "pretrained = True\n",
    "h_epochs = 15\n",
    "kfolds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining transforms and creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for input data\n",
    "training_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "total_set = datasets.ImageFolder(data_dir, transform=training_transforms)\n",
    "\n",
    "# Defining folds\n",
    "splits = KFold(n_splits = kfolds, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the target classes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "{0: 'Acinetobacter.baumanii', 1: 'Actinomyces.israeli', 2: 'Bacteroides.fragilis', 3: 'Bifidobacterium.spp', 4: 'Clostridium.perfringens', 5: 'Enterococcus.faecalis', 6: 'Enterococcus.faecium', 7: 'Escherichia.coli', 8: 'Fusobacterium', 9: 'Lactobacillus.casei', 10: 'Lactobacillus.crispatus', 11: 'Lactobacillus.delbrueckii', 12: 'Lactobacillus.gasseri', 13: 'Lactobacillus.jehnsenii', 14: 'Lactobacillus.johnsonii', 15: 'Lactobacillus.paracasei', 16: 'Lactobacillus.plantarum', 17: 'Lactobacillus.reuteri', 18: 'Lactobacillus.rhamnosus', 19: 'Lactobacillus.salivarius', 20: 'Listeria.monocytogenes', 21: 'Micrococcus.spp', 22: 'Neisseria.gonorrhoeae', 23: 'Porfyromonas.gingivalis', 24: 'Propionibacterium.acnes', 25: 'Proteus', 26: 'Pseudomonas.aeruginosa', 27: 'Staphylococcus.aureus', 28: 'Staphylococcus.epidermidis', 29: 'Staphylococcus.saprophiticus', 30: 'Streptococcus.agalactiae', 31: 'Veionella'}\n"
     ]
    }
   ],
   "source": [
    "train_labels = {value : key for (key, value) in total_set.class_to_idx.items()}\n",
    "    \n",
    "print(len(train_labels)) \n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and inicialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing pre-trained parameters, finetunning the classifier to output 32 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained model parameters to avoid backpropogating through them\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        print(\"Setting grad to false.\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_device():\n",
    "    # Model and criterion to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (6): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.squeezenet1_1(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Transfer Learning\n",
    "    model = models.squeezenet1_1(pretrained=pretrained)\n",
    "    \n",
    "    # Mode\n",
    "    model = set_parameter_requires_grad(model, feature_extract)\n",
    "    \n",
    "    # Fine tuning\n",
    "    # Build custom classifier\n",
    "    # Build custom classifier\n",
    "    model.classifier[1] = nn.Conv2d(512, 32, \n",
    "                                kernel_size=(1,1), stride=(1,1))\n",
    "    return model\n",
    "\n",
    "def create_optimizer(model):\n",
    "    # Parameters to update\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "\n",
    "    else:\n",
    "        n_params = 0\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad == True:\n",
    "                n_params += 1\n",
    "\n",
    "\n",
    "    # Loss function and gradient descent\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(params_to_update, \n",
    "                          lr=0.001, \n",
    "                          weight_decay=0.000004)\n",
    "    \n",
    "    return criterion.to(get_device()), model.to(get_device()), optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Samples in training: 21665\n",
      "Samples in test: 2408\n",
      "\t\t Training: Epoch(0) - Loss: 2.7865, Acc: 15.2227\n",
      "\t\t Validation(0) - Loss: 1.8262, Acc: 37.1262\n",
      "\t\t Training: Epoch(1) - Loss: 1.0946, Acc: 59.7646\n",
      "\t\t Validation(1) - Loss: 0.8659, Acc: 69.2691\n",
      "\t\t Training: Epoch(2) - Loss: 0.7304, Acc: 73.9534\n",
      "\t\t Validation(2) - Loss: 0.6242, Acc: 77.3671\n",
      "\t\t Training: Epoch(3) - Loss: 0.5893, Acc: 78.9938\n",
      "\t\t Validation(3) - Loss: 0.4551, Acc: 83.3056\n",
      "\t\t Training: Epoch(4) - Loss: 0.4434, Acc: 84.3203\n",
      "\t\t Validation(4) - Loss: 0.3928, Acc: 86.3372\n",
      "\t\t Training: Epoch(5) - Loss: 0.4206, Acc: 85.1881\n",
      "\t\t Validation(5) - Loss: 0.3405, Acc: 88.1645\n",
      "\t\t Training: Epoch(6) - Loss: 0.3755, Acc: 86.4113\n",
      "\t\t Validation(6) - Loss: 0.3065, Acc: 88.6628\n",
      "\t\t Training: Epoch(7) - Loss: 0.3390, Acc: 87.8191\n",
      "\t\t Validation(7) - Loss: 0.3167, Acc: 88.6628\n",
      "\t\t Training: Epoch(8) - Loss: 0.3076, Acc: 88.9545\n",
      "\t\t Validation(8) - Loss: 0.2438, Acc: 91.4452\n",
      "\t\t Training: Epoch(9) - Loss: 0.3024, Acc: 89.3238\n",
      "\t\t Validation(9) - Loss: 0.3406, Acc: 88.2060\n",
      "\t\t Training: Epoch(10) - Loss: 0.2570, Acc: 90.4777\n",
      "\t\t Validation(10) - Loss: 0.2836, Acc: 89.4934\n",
      "\t\t Training: Epoch(11) - Loss: 0.2653, Acc: 90.4962\n",
      "\t\t Validation(11) - Loss: 0.2509, Acc: 90.7392\n",
      "\t\t Training: Epoch(12) - Loss: 0.2637, Acc: 90.5285\n",
      "\t\t Validation(12) - Loss: 0.2528, Acc: 90.7392\n",
      "\t\t Training: Epoch(13) - Loss: 0.2257, Acc: 91.9686\n",
      "\t\t Validation(13) - Loss: 0.2524, Acc: 91.2791\n",
      "\t\t Training: Epoch(14) - Loss: 0.2798, Acc: 90.5008\n",
      "\t\t Validation(14) - Loss: 0.2210, Acc: 92.2757\n",
      "Finished.\n",
      "Total time per fold: 918.7878179550171 seconds.\n",
      "Fold : 1\n",
      "Samples in training: 21665\n",
      "Samples in test: 2408\n",
      "\t\t Training: Epoch(0) - Loss: 1.9642, Acc: 34.4842\n",
      "\t\t Validation(0) - Loss: 1.1364, Acc: 58.2226\n",
      "\t\t Training: Epoch(1) - Loss: 0.8693, Acc: 68.6776\n",
      "\t\t Validation(1) - Loss: 0.7145, Acc: 73.9203\n",
      "\t\t Training: Epoch(2) - Loss: 0.6302, Acc: 77.6414\n",
      "\t\t Validation(2) - Loss: 0.5007, Acc: 83.1395\n",
      "\t\t Training: Epoch(3) - Loss: 0.4859, Acc: 82.7925\n",
      "\t\t Validation(3) - Loss: 0.5770, Acc: 80.8970\n",
      "\t\t Training: Epoch(4) - Loss: 0.4081, Acc: 85.5666\n",
      "\t\t Validation(4) - Loss: 0.4633, Acc: 84.1777\n",
      "\t\t Training: Epoch(5) - Loss: 0.3491, Acc: 87.8929\n",
      "\t\t Validation(5) - Loss: 0.4453, Acc: 84.9668\n",
      "\t\t Training: Epoch(6) - Loss: 0.3211, Acc: 89.0007\n",
      "\t\t Validation(6) - Loss: 0.4341, Acc: 86.4203\n",
      "\t\t Training: Epoch(7) - Loss: 0.3072, Acc: 89.5915\n",
      "\t\t Validation(7) - Loss: 0.3596, Acc: 88.4551\n",
      "\t\t Training: Epoch(8) - Loss: 0.2826, Acc: 90.4039\n",
      "\t\t Validation(8) - Loss: 0.4074, Acc: 87.1678\n",
      "\t\t Training: Epoch(9) - Loss: 0.2740, Acc: 90.8239\n",
      "\t\t Validation(9) - Loss: 0.2560, Acc: 91.0714\n",
      "\t\t Training: Epoch(10) - Loss: 0.2425, Acc: 91.8532\n",
      "\t\t Validation(10) - Loss: 0.2296, Acc: 91.7359\n",
      "\t\t Training: Epoch(11) - Loss: 0.2298, Acc: 92.0932\n",
      "\t\t Validation(11) - Loss: 0.1967, Acc: 93.1894\n",
      "\t\t Training: Epoch(12) - Loss: 0.2091, Acc: 92.9148\n",
      "\t\t Validation(12) - Loss: 0.2538, Acc: 91.6528\n",
      "\t\t Training: Epoch(13) - Loss: 0.2154, Acc: 92.6887\n",
      "\t\t Validation(13) - Loss: 0.2121, Acc: 93.3970\n",
      "\t\t Training: Epoch(14) - Loss: 0.2247, Acc: 92.6240\n",
      "\t\t Validation(14) - Loss: 0.2477, Acc: 92.2757\n",
      "Finished.\n",
      "Total time per fold: 940.7066869735718 seconds.\n",
      "Fold : 2\n",
      "Samples in training: 21665\n",
      "Samples in test: 2408\n",
      "\t\t Training: Epoch(0) - Loss: 2.3337, Acc: 23.9326\n",
      "\t\t Validation(0) - Loss: 1.6985, Acc: 39.2857\n",
      "\t\t Training: Epoch(1) - Loss: 1.1037, Acc: 59.0399\n",
      "\t\t Validation(1) - Loss: 0.9091, Acc: 64.4103\n",
      "\t\t Training: Epoch(2) - Loss: 0.7972, Acc: 70.5700\n",
      "\t\t Validation(2) - Loss: 0.6960, Acc: 75.5814\n",
      "\t\t Training: Epoch(3) - Loss: 0.6308, Acc: 77.2767\n",
      "\t\t Validation(3) - Loss: 0.7169, Acc: 76.3704\n",
      "\t\t Training: Epoch(4) - Loss: 0.5505, Acc: 80.0138\n",
      "\t\t Validation(4) - Loss: 0.4938, Acc: 82.3505\n",
      "\t\t Training: Epoch(5) - Loss: 0.4701, Acc: 82.8987\n",
      "\t\t Validation(5) - Loss: 0.4500, Acc: 83.7209\n",
      "\t\t Training: Epoch(6) - Loss: 0.4274, Acc: 84.5188\n",
      "\t\t Validation(6) - Loss: 0.5245, Acc: 82.4751\n",
      "\t\t Training: Epoch(7) - Loss: 0.4051, Acc: 85.1881\n",
      "\t\t Validation(7) - Loss: 0.4991, Acc: 82.5581\n",
      "\t\t Training: Epoch(8) - Loss: 0.3393, Acc: 87.5190\n",
      "\t\t Validation(8) - Loss: 0.3624, Acc: 87.2093\n",
      "\t\t Training: Epoch(9) - Loss: 0.3725, Acc: 86.8221\n",
      "\t\t Validation(9) - Loss: 0.2812, Acc: 89.6179\n",
      "\t\t Training: Epoch(10) - Loss: 0.3193, Acc: 88.5899\n",
      "\t\t Validation(10) - Loss: 0.4387, Acc: 85.3405\n",
      "\t\t Training: Epoch(11) - Loss: 0.3041, Acc: 89.3330\n",
      "\t\t Validation(11) - Loss: 0.3901, Acc: 85.6312\n",
      "\t\t Training: Epoch(12) - Loss: 0.2698, Acc: 90.2746\n",
      "\t\t Validation(12) - Loss: 0.4130, Acc: 88.9535\n",
      "\t\t Training: Epoch(13) - Loss: 0.2669, Acc: 90.5008\n",
      "\t\t Validation(13) - Loss: 0.4379, Acc: 84.3439\n",
      "\t\t Training: Epoch(14) - Loss: 0.2629, Acc: 90.6347\n",
      "\t\t Validation(14) - Loss: 0.2673, Acc: 90.9053\n",
      "Finished.\n",
      "Total time per fold: 925.3646445274353 seconds.\n",
      "Fold : 3\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 1.8115, Acc: 40.6489\n",
      "\t\t Validation(0) - Loss: 0.9201, Acc: 66.9298\n",
      "\t\t Training: Epoch(1) - Loss: 0.7444, Acc: 73.3453\n",
      "\t\t Validation(1) - Loss: 0.6963, Acc: 75.9867\n",
      "\t\t Training: Epoch(2) - Loss: 0.5610, Acc: 79.7563\n",
      "\t\t Validation(2) - Loss: 0.5876, Acc: 80.9722\n",
      "\t\t Training: Epoch(3) - Loss: 0.4515, Acc: 83.8041\n",
      "\t\t Validation(3) - Loss: 0.4773, Acc: 82.7171\n",
      "\t\t Training: Epoch(4) - Loss: 0.4224, Acc: 84.8426\n",
      "\t\t Validation(4) - Loss: 0.2914, Acc: 89.8214\n",
      "\t\t Training: Epoch(5) - Loss: 0.3845, Acc: 86.5734\n",
      "\t\t Validation(5) - Loss: 0.3514, Acc: 87.8272\n",
      "\t\t Training: Epoch(6) - Loss: 0.3400, Acc: 87.8750\n",
      "\t\t Validation(6) - Loss: 0.3031, Acc: 88.9489\n",
      "\t\t Training: Epoch(7) - Loss: 0.3103, Acc: 88.8997\n",
      "\t\t Validation(7) - Loss: 0.3371, Acc: 88.2011\n",
      "\t\t Training: Epoch(8) - Loss: 0.2985, Acc: 89.1720\n",
      "\t\t Validation(8) - Loss: 0.3970, Acc: 86.6639\n",
      "\t\t Training: Epoch(9) - Loss: 0.2909, Acc: 89.7028\n",
      "\t\t Validation(9) - Loss: 0.2924, Acc: 89.2813\n",
      "\t\t Training: Epoch(10) - Loss: 0.2693, Acc: 90.6536\n",
      "\t\t Validation(10) - Loss: 0.2085, Acc: 92.8126\n",
      "\t\t Training: Epoch(11) - Loss: 0.2502, Acc: 91.1474\n",
      "\t\t Validation(11) - Loss: 0.3141, Acc: 89.1566\n",
      "\t\t Training: Epoch(12) - Loss: 0.2544, Acc: 91.2305\n",
      "\t\t Validation(12) - Loss: 0.2487, Acc: 91.8986\n",
      "\t\t Training: Epoch(13) - Loss: 0.2366, Acc: 92.0382\n",
      "\t\t Validation(13) - Loss: 0.2257, Acc: 92.6464\n",
      "\t\t Training: Epoch(14) - Loss: 0.2135, Acc: 92.5090\n",
      "\t\t Validation(14) - Loss: 0.2598, Acc: 91.2754\n",
      "Finished.\n",
      "Total time per fold: 920.2732892036438 seconds.\n",
      "Fold : 4\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 2.0977, Acc: 30.8548\n",
      "\t\t Validation(0) - Loss: 1.4972, Acc: 49.3145\n",
      "\t\t Training: Epoch(1) - Loss: 0.9463, Acc: 65.8128\n",
      "\t\t Validation(1) - Loss: 0.6479, Acc: 76.8592\n",
      "\t\t Training: Epoch(2) - Loss: 0.6835, Acc: 75.5100\n",
      "\t\t Validation(2) - Loss: 0.5944, Acc: 78.8949\n",
      "\t\t Training: Epoch(3) - Loss: 0.5278, Acc: 80.9702\n",
      "\t\t Validation(3) - Loss: 0.4200, Acc: 85.1683\n",
      "\t\t Training: Epoch(4) - Loss: 0.4588, Acc: 83.7718\n",
      "\t\t Validation(4) - Loss: 0.4978, Acc: 83.0079\n",
      "\t\t Training: Epoch(5) - Loss: 0.3983, Acc: 85.7196\n",
      "\t\t Validation(5) - Loss: 0.4149, Acc: 84.5451\n",
      "\t\t Training: Epoch(6) - Loss: 0.3713, Acc: 86.8365\n",
      "\t\t Validation(6) - Loss: 0.5878, Acc: 80.0582\n",
      "\t\t Training: Epoch(7) - Loss: 0.3551, Acc: 87.5888\n",
      "\t\t Validation(7) - Loss: 0.3113, Acc: 88.8658\n",
      "\t\t Training: Epoch(8) - Loss: 0.3010, Acc: 89.2827\n",
      "\t\t Validation(8) - Loss: 0.5697, Acc: 81.9277\n",
      "\t\t Training: Epoch(9) - Loss: 0.3216, Acc: 88.5073\n",
      "\t\t Validation(9) - Loss: 0.3984, Acc: 87.0378\n",
      "\t\t Training: Epoch(10) - Loss: 0.2727, Acc: 90.4228\n",
      "\t\t Validation(10) - Loss: 0.3199, Acc: 90.1953\n",
      "\t\t Training: Epoch(11) - Loss: 0.2909, Acc: 89.8366\n",
      "\t\t Validation(11) - Loss: 0.2753, Acc: 90.7769\n",
      "\t\t Training: Epoch(12) - Loss: 0.2559, Acc: 91.0597\n",
      "\t\t Validation(12) - Loss: 0.3941, Acc: 86.7054\n",
      "\t\t Training: Epoch(13) - Loss: 0.2445, Acc: 91.5028\n",
      "\t\t Validation(13) - Loss: 0.2433, Acc: 92.0233\n",
      "\t\t Training: Epoch(14) - Loss: 0.2402, Acc: 91.6644\n",
      "\t\t Validation(14) - Loss: 0.2613, Acc: 91.8986\n",
      "Finished.\n",
      "Total time per fold: 929.206273317337 seconds.\n",
      "Fold : 5\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 1.9050, Acc: 37.6165\n",
      "\t\t Validation(0) - Loss: 1.0446, Acc: 61.9028\n",
      "\t\t Training: Epoch(1) - Loss: 0.8081, Acc: 71.2268\n",
      "\t\t Validation(1) - Loss: 0.6628, Acc: 76.1944\n",
      "\t\t Training: Epoch(2) - Loss: 0.5759, Acc: 79.7148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Validation(2) - Loss: 0.5568, Acc: 80.1828\n",
      "\t\t Training: Epoch(3) - Loss: 0.4795, Acc: 83.1672\n",
      "\t\t Validation(3) - Loss: 0.4379, Acc: 84.2543\n",
      "\t\t Training: Epoch(4) - Loss: 0.3910, Acc: 86.0057\n",
      "\t\t Validation(4) - Loss: 0.3410, Acc: 88.2011\n",
      "\t\t Training: Epoch(5) - Loss: 0.3725, Acc: 86.8088\n",
      "\t\t Validation(5) - Loss: 0.2938, Acc: 89.5721\n",
      "\t\t Training: Epoch(6) - Loss: 0.3130, Acc: 88.8212\n",
      "\t\t Validation(6) - Loss: 0.4511, Acc: 84.4204\n",
      "\t\t Training: Epoch(7) - Loss: 0.3414, Acc: 88.4566\n",
      "\t\t Validation(7) - Loss: 0.3817, Acc: 86.5393\n",
      "\t\t Training: Epoch(8) - Loss: 0.2903, Acc: 89.9520\n",
      "\t\t Validation(8) - Loss: 0.3269, Acc: 88.9489\n",
      "\t\t Training: Epoch(9) - Loss: 0.2528, Acc: 90.9905\n",
      "\t\t Validation(9) - Loss: 0.2577, Acc: 92.1894\n",
      "\t\t Training: Epoch(10) - Loss: 0.2346, Acc: 91.8813\n",
      "\t\t Validation(10) - Loss: 0.2703, Acc: 91.1924\n",
      "\t\t Training: Epoch(11) - Loss: 0.2494, Acc: 91.3136\n",
      "\t\t Validation(11) - Loss: 0.1976, Acc: 92.9788\n",
      "\t\t Training: Epoch(12) - Loss: 0.2231, Acc: 92.3105\n",
      "\t\t Validation(12) - Loss: 0.2819, Acc: 90.6938\n",
      "\t\t Training: Epoch(13) - Loss: 0.2224, Acc: 92.4998\n",
      "\t\t Validation(13) - Loss: 0.2192, Acc: 92.8126\n",
      "\t\t Training: Epoch(14) - Loss: 0.1921, Acc: 93.3952\n",
      "\t\t Validation(14) - Loss: 0.2877, Acc: 91.8155\n",
      "Finished.\n",
      "Total time per fold: 924.3694777488708 seconds.\n",
      "Fold : 6\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 2.7852, Acc: 15.1897\n",
      "\t\t Validation(0) - Loss: 1.6560, Acc: 36.7678\n",
      "\t\t Training: Epoch(1) - Loss: 1.2880, Acc: 52.5755\n",
      "\t\t Validation(1) - Loss: 1.1050, Acc: 60.7811\n",
      "\t\t Training: Epoch(2) - Loss: 0.8708, Acc: 68.4667\n",
      "\t\t Validation(2) - Loss: 0.7440, Acc: 73.9510\n",
      "\t\t Training: Epoch(3) - Loss: 0.6878, Acc: 74.9146\n",
      "\t\t Validation(3) - Loss: 0.5950, Acc: 78.7703\n",
      "\t\t Training: Epoch(4) - Loss: 0.5743, Acc: 78.6901\n",
      "\t\t Validation(4) - Loss: 0.7036, Acc: 75.8205\n",
      "\t\t Training: Epoch(5) - Loss: 0.5167, Acc: 81.0763\n",
      "\t\t Validation(5) - Loss: 0.7579, Acc: 74.6572\n",
      "\t\t Training: Epoch(6) - Loss: 0.4544, Acc: 83.3749\n",
      "\t\t Validation(6) - Loss: 0.4430, Acc: 84.7528\n",
      "\t\t Training: Epoch(7) - Loss: 0.4126, Acc: 85.0088\n",
      "\t\t Validation(7) - Loss: 0.4120, Acc: 85.9161\n",
      "\t\t Training: Epoch(8) - Loss: 0.3965, Acc: 85.5303\n",
      "\t\t Validation(8) - Loss: 0.4193, Acc: 85.5422\n",
      "\t\t Training: Epoch(9) - Loss: 0.3812, Acc: 86.2273\n",
      "\t\t Validation(9) - Loss: 0.4161, Acc: 85.0436\n",
      "\t\t Training: Epoch(10) - Loss: 0.3440, Acc: 87.3765\n",
      "\t\t Validation(10) - Loss: 0.4969, Acc: 83.1325\n",
      "\t\t Training: Epoch(11) - Loss: 0.3414, Acc: 87.4550\n",
      "\t\t Validation(11) - Loss: 0.4146, Acc: 85.5422\n",
      "\t\t Training: Epoch(12) - Loss: 0.3211, Acc: 88.3135\n",
      "\t\t Validation(12) - Loss: 0.2873, Acc: 89.6552\n",
      "\t\t Training: Epoch(13) - Loss: 0.3037, Acc: 89.0981\n",
      "\t\t Validation(13) - Loss: 0.3198, Acc: 88.2426\n",
      "\t\t Training: Epoch(14) - Loss: 0.2726, Acc: 90.0074\n",
      "\t\t Validation(14) - Loss: 0.3183, Acc: 88.7412\n",
      "Finished.\n",
      "Total time per fold: 901.7456169128418 seconds.\n",
      "Fold : 7\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 2.1839, Acc: 29.6594\n",
      "\t\t Validation(0) - Loss: 1.0720, Acc: 61.2796\n",
      "\t\t Training: Epoch(1) - Loss: 0.9158, Acc: 66.9713\n",
      "\t\t Validation(1) - Loss: 0.8701, Acc: 68.6747\n",
      "\t\t Training: Epoch(2) - Loss: 0.6673, Acc: 75.8793\n",
      "\t\t Validation(2) - Loss: 0.8830, Acc: 69.9626\n",
      "\t\t Training: Epoch(3) - Loss: 0.5889, Acc: 79.0040\n",
      "\t\t Validation(3) - Loss: 0.4741, Acc: 83.6726\n",
      "\t\t Training: Epoch(4) - Loss: 0.4834, Acc: 82.5395\n",
      "\t\t Validation(4) - Loss: 0.4916, Acc: 82.3847\n",
      "\t\t Training: Epoch(5) - Loss: 0.4186, Acc: 84.8842\n",
      "\t\t Validation(5) - Loss: 0.5553, Acc: 80.7229\n",
      "\t\t Training: Epoch(6) - Loss: 0.4017, Acc: 85.5534\n",
      "\t\t Validation(6) - Loss: 0.4002, Acc: 85.1683\n",
      "\t\t Training: Epoch(7) - Loss: 0.3730, Acc: 86.4904\n",
      "\t\t Validation(7) - Loss: 0.6381, Acc: 82.4678\n",
      "\t\t Training: Epoch(8) - Loss: 0.3484, Acc: 87.5935\n",
      "\t\t Validation(8) - Loss: 0.3828, Acc: 86.7470\n",
      "\t\t Training: Epoch(9) - Loss: 0.3155, Acc: 88.6827\n",
      "\t\t Validation(9) - Loss: 0.3041, Acc: 88.9904\n",
      "\t\t Training: Epoch(10) - Loss: 0.3062, Acc: 89.0150\n",
      "\t\t Validation(10) - Loss: 0.3177, Acc: 88.1595\n",
      "\t\t Training: Epoch(11) - Loss: 0.2998, Acc: 89.1812\n",
      "\t\t Validation(11) - Loss: 0.3831, Acc: 87.9103\n",
      "\t\t Training: Epoch(12) - Loss: 0.2594, Acc: 90.5289\n",
      "\t\t Validation(12) - Loss: 0.2610, Acc: 90.3199\n",
      "\t\t Training: Epoch(13) - Loss: 0.2664, Acc: 90.6166\n",
      "\t\t Validation(13) - Loss: 0.2991, Acc: 89.9044\n",
      "\t\t Training: Epoch(14) - Loss: 0.2671, Acc: 90.7597\n",
      "\t\t Validation(14) - Loss: 0.3154, Acc: 88.4504\n",
      "Finished.\n",
      "Total time per fold: 897.9345779418945 seconds.\n",
      "Fold : 8\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 2.2555, Acc: 25.2239\n",
      "\t\t Validation(0) - Loss: 1.4190, Acc: 49.1483\n",
      "\t\t Training: Epoch(1) - Loss: 1.0731, Acc: 60.0572\n",
      "\t\t Validation(1) - Loss: 0.9045, Acc: 67.5530\n",
      "\t\t Training: Epoch(2) - Loss: 0.7473, Acc: 72.9761\n",
      "\t\t Validation(2) - Loss: 0.6140, Acc: 76.6930\n",
      "\t\t Training: Epoch(3) - Loss: 0.5484, Acc: 79.9409\n",
      "\t\t Validation(3) - Loss: 0.7361, Acc: 75.1558\n",
      "\t\t Training: Epoch(4) - Loss: 0.4897, Acc: 82.3548\n",
      "\t\t Validation(4) - Loss: 0.3709, Acc: 86.9132\n",
      "\t\t Training: Epoch(5) - Loss: 0.4448, Acc: 84.1364\n",
      "\t\t Validation(5) - Loss: 0.4762, Acc: 81.9277\n",
      "\t\t Training: Epoch(6) - Loss: 0.3807, Acc: 86.3750\n",
      "\t\t Validation(6) - Loss: 0.3955, Acc: 86.2900\n",
      "\t\t Training: Epoch(7) - Loss: 0.3676, Acc: 87.1873\n",
      "\t\t Validation(7) - Loss: 0.3737, Acc: 86.4562\n",
      "\t\t Training: Epoch(8) - Loss: 0.3068, Acc: 89.1351\n",
      "\t\t Validation(8) - Loss: 0.3358, Acc: 88.1180\n",
      "\t\t Training: Epoch(9) - Loss: 0.2902, Acc: 89.8181\n",
      "\t\t Validation(9) - Loss: 0.2367, Acc: 91.3170\n",
      "\t\t Training: Epoch(10) - Loss: 0.3063, Acc: 89.1858\n",
      "\t\t Validation(10) - Loss: 0.2654, Acc: 90.2784\n",
      "\t\t Training: Epoch(11) - Loss: 0.2578, Acc: 91.0090\n",
      "\t\t Validation(11) - Loss: 0.2360, Acc: 92.4803\n",
      "\t\t Training: Epoch(12) - Loss: 0.2455, Acc: 91.7013\n",
      "\t\t Validation(12) - Loss: 0.2595, Acc: 92.1064\n",
      "\t\t Training: Epoch(13) - Loss: 0.2668, Acc: 90.9628\n",
      "\t\t Validation(13) - Loss: 0.2755, Acc: 90.1122\n",
      "\t\t Training: Epoch(14) - Loss: 0.2001, Acc: 93.1321\n",
      "\t\t Validation(14) - Loss: 0.2153, Acc: 93.3112\n",
      "Finished.\n",
      "Total time per fold: 921.0427830219269 seconds.\n",
      "Fold : 9\n",
      "Samples in training: 21666\n",
      "Samples in test: 2407\n",
      "\t\t Training: Epoch(0) - Loss: 2.1923, Acc: 29.4148\n",
      "\t\t Validation(0) - Loss: 1.1424, Acc: 58.4545\n",
      "\t\t Training: Epoch(1) - Loss: 0.9494, Acc: 65.7943\n",
      "\t\t Validation(1) - Loss: 0.7092, Acc: 74.4911\n",
      "\t\t Training: Epoch(2) - Loss: 0.6411, Acc: 76.9778\n",
      "\t\t Validation(2) - Loss: 0.5412, Acc: 79.8089\n",
      "\t\t Training: Epoch(3) - Loss: 0.5454, Acc: 80.5179\n",
      "\t\t Validation(3) - Loss: 0.4691, Acc: 83.2156\n",
      "\t\t Training: Epoch(4) - Loss: 0.4769, Acc: 82.7149\n",
      "\t\t Validation(4) - Loss: 0.4641, Acc: 83.7142\n",
      "\t\t Training: Epoch(5) - Loss: 0.4192, Acc: 84.9442\n",
      "\t\t Validation(5) - Loss: 0.4073, Acc: 85.7084\n",
      "\t\t Training: Epoch(6) - Loss: 0.3773, Acc: 86.3427\n",
      "\t\t Validation(6) - Loss: 0.3903, Acc: 86.2069\n",
      "\t\t Training: Epoch(7) - Loss: 0.3341, Acc: 88.1289\n",
      "\t\t Validation(7) - Loss: 0.4461, Acc: 85.4175\n",
      "\t\t Training: Epoch(8) - Loss: 0.3494, Acc: 87.5565\n",
      "\t\t Validation(8) - Loss: 0.3861, Acc: 87.1624\n",
      "\t\t Training: Epoch(9) - Loss: 0.3113, Acc: 89.1074\n",
      "\t\t Validation(9) - Loss: 0.4047, Acc: 86.3315\n",
      "\t\t Training: Epoch(10) - Loss: 0.2827, Acc: 89.9243\n",
      "\t\t Validation(10) - Loss: 0.2959, Acc: 89.6967\n",
      "\t\t Training: Epoch(11) - Loss: 0.2841, Acc: 90.1828\n",
      "\t\t Validation(11) - Loss: 0.2532, Acc: 91.6909\n",
      "\t\t Training: Epoch(12) - Loss: 0.2533, Acc: 91.3228\n",
      "\t\t Validation(12) - Loss: 0.3491, Acc: 88.7827\n",
      "\t\t Training: Epoch(13) - Loss: 0.2459, Acc: 91.5028\n",
      "\t\t Validation(13) - Loss: 0.5037, Acc: 85.8330\n",
      "\t\t Training: Epoch(14) - Loss: 0.2473, Acc: 91.6459\n",
      "\t\t Validation(14) - Loss: 0.2277, Acc: 92.6880\n",
      "Finished.\n",
      "Total time per fold: 1002.7497203350067 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Variables to store fold scores\n",
    "train_acc = []\n",
    "test_top1_acc = []\n",
    "test_top5_acc = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "times = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(splits.split(total_set)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('Fold : {}'.format(fold))\n",
    "    \n",
    "    # Train and val samplers\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    print(\"Samples in training:\", len(train_sampler))\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    print(\"Samples in test:\", len(valid_sampler))\n",
    "    \n",
    "    # Train and val loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=train_batch_size, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "                      total_set, batch_size=1, sampler=valid_sampler)\n",
    "    \n",
    "    device = get_device()\n",
    "    \n",
    "    criterion, model, optimizer = create_optimizer(load_model())\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(h_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        trunning_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += (preds == labels).sum()\n",
    "            trunning_corrects += preds.size(0)\n",
    "            \n",
    "\n",
    "        epoch_loss = running_loss / trunning_corrects\n",
    "        epoch_acc = (running_corrects.double()*100) / trunning_corrects\n",
    "        train_acc.append(epoch_acc.item())\n",
    "        \n",
    "        print('\\t\\t Training: Epoch({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        model.eval()  \n",
    "        \n",
    "        vrunning_loss = 0.0\n",
    "        vrunning_corrects = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for data, labels in valid_loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(data)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            vrunning_loss += loss.item() * data.size(0)\n",
    "            vrunning_corrects += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            \n",
    "        vepoch_loss = vrunning_loss/num_samples\n",
    "        vepoch_acc = (vrunning_corrects.double() * 100)/num_samples\n",
    "        \n",
    "        print('\\t\\t Validation({}) - Loss: {:.4f}, Acc: {:.4f}'.format(epoch, vepoch_loss, vepoch_acc))\n",
    "    \n",
    "    # Calculating and appending scores to this fold\n",
    "    model.class_to_idx = total_set.class_to_idx\n",
    "    scores = get_scores(model, valid_loader)\n",
    "    \n",
    "    test_top1_acc.append(scores[0])\n",
    "    test_top5_acc.append(scores[1])\n",
    "    test_precision.append(scores[2])\n",
    "    test_recall.append(scores[3])\n",
    "    test_f1.append(scores[4])\n",
    "    \n",
    "    time_fold = time.time() - start_time\n",
    "    times.append(time_fold)\n",
    "    print(\"Total time per fold: %s seconds.\" %(time_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy average:  0.8143815196941172\n",
      "Top-1 test accuracy average:  0.9136371180678724\n",
      "Top-5 test accuracy average:  0.9955550636501787\n",
      "Weighted Precision test accuracy average:  0.9210600141944474\n",
      "Weighted Recall test accuracy average:  0.9136371180678724\n",
      "Weighted F1 test accuracy average:  0.9125822249383386\n",
      "Average time per fold (seconds): 928.2180887937545\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy average: \", np.mean(train_acc) / 100)\n",
    "print(\"Top-1 test accuracy average: \", np.mean(test_top1_acc))\n",
    "print(\"Top-5 test accuracy average: \", np.mean(test_top5_acc))\n",
    "print(\"Weighted Precision test accuracy average: \", np.mean(test_precision))\n",
    "print(\"Weighted Recall test accuracy average: \", np.mean(test_recall))\n",
    "print(\"Weighted F1 test accuracy average: \", np.mean(test_f1))\n",
    "print(\"Average time per fold (seconds):\", np.mean(times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
